{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c468fb39",
   "metadata": {},
   "source": [
    "# Full Day RFI Flagging\n",
    "\n",
    "**by Josh Dillon**, last updated June 19, 2023\n",
    "\n",
    "This notebook is designed to figure out a single full-day RFI mask using the best autocorelations, taking individual [file_calibration](https://github.com/HERA-Team/hera_notebook_templates/blob/master/notebooks/file_calibration.ipynb) notebook results as a prior but then potentially undoing flags. \n",
    "\n",
    "Here's a set of links to skip to particular figures and tables:\n",
    "\n",
    "\n",
    "# [• Figure 1: Show All DPSS Residual z-Scores](#Figure-1:-Show-All-DPSS-Residual-z-Scores)\n",
    "# [• Figure 2: z-Score of DPSS-Filtered, Averaged Good Autocorrelation and Initial Flags](#Figure-2:-z-Score-of-DPSS-Filtered,-Averaged-Good-Autocorrelation-and-Initial-Flags)\n",
    "# [• Figure 3: z-Score of DPSS-Filtered, Averaged Good Autocorrelation and Expanded Flags](#Figure-3:-z-Score-of-DPSS-Filtered,-Averaged-Good-Autocorrelation-and-Expanded-Flags)\n",
    "# [• Figure 4: z-Score of DPSS-Filtered, Averaged Good Autocorrelation and Final, Re-Computed Flags](#Figure-4:-z-Score-of-DPSS-Filtered,-Averaged-Good-Autocorrelation-and-Final,-Re-Computed-Flags)\n",
    "# [• Figure 5: Summary of Flags Before and After Recomputing Them](#Figure-5:-Summary-of-Flags-Before-and-After-Recomputing-Them)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c085a84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "tstart = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63080ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HDF5_USE_FILE_LOCKING'] = 'FALSE'\n",
    "import h5py\n",
    "import hdf5plugin  # REQUIRED to have the compression plugins available\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import copy\n",
    "import warnings\n",
    "import textwrap\n",
    "from pyuvdata import UVFlag, UVData, UVCal\n",
    "from hera_cal import io, utils, abscal\n",
    "from hera_cal.smooth_cal import CalibrationSmoother, dpss_filters, solve_2D_DPSS\n",
    "from hera_qm import ant_class, xrfi, metrics_io\n",
    "from hera_filters import dspec\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "%matplotlib inline\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "_ = np.seterr(all='ignore')  # get rid of red warnings\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e434a261",
   "metadata": {},
   "source": [
    "## Parse inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800c80d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get filenames\n",
    "SUM_FILE = os.environ.get(\"SUM_FILE\", None)\n",
    "# SUM_FILE = '/lustre/aoc/projects/hera/h6c-analysis/IDR2/2459866/zen.2459866.25282.sum.uvh5'  # If sum_file is not defined in the environment variables, define it here.\n",
    "SUM_SUFFIX = os.environ.get(\"SUM_SUFFIX\", 'sum.uvh5')\n",
    "SUM_AUTOS_SUFFIX = os.environ.get(\"SUM_AUTOS_SUFFIX\", 'sum.autos.uvh5')\n",
    "DIFF_AUTOS_SUFFIX = os.environ.get(\"DIFF_AUTOS_SUFFIX\", 'diff.autos.uvh5')\n",
    "CAL_SUFFIX = os.environ.get(\"CAL_SUFFIX\", 'sum.omni.calfits')\n",
    "ANT_CLASS_SUFFIX = os.environ.get(\"ANT_CLASS_SUFFIX\", 'sum.ant_class.csv')\n",
    "APRIORI_YAML_PATH = os.environ.get(\"APRIORI_YAML_PATH\", None)\n",
    "OUT_FLAG_SUFFIX = os.environ.get(\"OUT_FLAG_SUFFIX\", 'sum.flag_waterfall.h5')\n",
    "\n",
    "sum_glob = '.'.join(SUM_FILE.split('.')[:-3]) + '.*.' + SUM_SUFFIX\n",
    "auto_sums_glob = sum_glob.replace(SUM_SUFFIX, SUM_AUTOS_SUFFIX)\n",
    "auto_diffs_glob = sum_glob.replace(SUM_SUFFIX, DIFF_AUTOS_SUFFIX)\n",
    "cal_files_glob = sum_glob.replace(SUM_SUFFIX, CAL_SUFFIX)\n",
    "ant_class_csvs_glob = sum_glob.replace(SUM_SUFFIX, ANT_CLASS_SUFFIX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2311b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A priori flag settings\n",
    "FM_LOW_FREQ = float(os.environ.get(\"FM_LOW_FREQ\", 87.5)) # in MHz\n",
    "FM_HIGH_FREQ = float(os.environ.get(\"FM_HIGH_FREQ\", 108.0)) # in MHz\n",
    "FM_freq_range = [FM_LOW_FREQ * 1e6, FM_HIGH_FREQ * 1e6]\n",
    "MAX_SOLAR_ALT = float(os.environ.get(\"MAX_SOLAR_ALT\", 0.0)) # in degrees\n",
    "\n",
    "# DPSS settings\n",
    "FREQ_FILTER_SCALE = float(os.environ.get(\"FREQ_FILTER_SCALE\", 5.0)) # in MHz\n",
    "TIME_FILTER_SCALE = float(os.environ.get(\"TIME_FILTER_SCALE\", 450.0))# in s\n",
    "EIGENVAL_CUTOFF = float(os.environ.get(\"EIGENVAL_CUTOFF\", 1e-12))\n",
    "\n",
    "# Outlier flagging settings\n",
    "MIN_FRAC_OF_AUTOS = float(os.environ.get(\"MIN_FRAC_OF_AUTOS\", .25))\n",
    "MAX_AUTO_L2 = float(os.environ.get(\"MAX_AUTRO_L2\", 1.2))\n",
    "Z_THRESH = float(os.environ.get(\"Z_THRESH\", 5.0))\n",
    "WS_Z_THRESH = float(os.environ.get(\"WS_Z_THRESH\", 4.0))\n",
    "AVG_Z_THRESH = float(os.environ.get(\"AVG_Z_THRESH\", 1.5))\n",
    "REPEAT_FLAG_Z_THRESH = float(os.environ.get(\"REPEAT_FLAG_Z_THESH\", 2.0))\n",
    "MAX_FREQ_FLAG_FRAC = float(os.environ.get(\"MAX_FREQ_FLAG_FRAC\", .25))\n",
    "MAX_TIME_FLAG_FRAC = float(os.environ.get(\"MAX_TIME_FLAG_FRAC\", .1))\n",
    "\n",
    "for setting in ['FM_LOW_FREQ', 'FM_HIGH_FREQ', 'MAX_SOLAR_ALT', 'FREQ_FILTER_SCALE', 'TIME_FILTER_SCALE', \n",
    "                'EIGENVAL_CUTOFF', 'MIN_FRAC_OF_AUTOS', 'MAX_AUTO_L2', 'Z_THRESH', 'WS_Z_THRESH', 'AVG_Z_THRESH', 'REPEAT_FLAG_Z_THRESH', \n",
    "                'MAX_FREQ_FLAG_FRAC ', 'MAX_TIME_FLAG_FRAC ']:\n",
    "        print(f'{setting} = {eval(setting)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bb3c9a",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bbe8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_sums = sorted(glob.glob(auto_sums_glob))\n",
    "print(f'Found {len(auto_sums)} *.{SUM_AUTOS_SUFFIX} files starting with {auto_sums[0]}.')\n",
    "\n",
    "auto_diffs = sorted(glob.glob(auto_diffs_glob))\n",
    "print(f'Found {len(auto_diffs)} *.{DIFF_AUTOS_SUFFIX} files starting with {auto_diffs[0]}.')\n",
    "\n",
    "cal_files = sorted(glob.glob(cal_files_glob))\n",
    "print(f'Found {len(cal_files)} *.{CAL_SUFFIX} files starting with {cal_files[0]}.')\n",
    "\n",
    "ant_class_csvs = sorted(glob.glob(ant_class_csvs_glob))\n",
    "print(f'Found {len(ant_class_csvs)} *.{ANT_CLASS_SUFFIX} files starting with {ant_class_csvs[0]}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79afec39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ant_class csvs\n",
    "tables = [pd.read_csv(f).dropna(axis=0, how='all') for f in ant_class_csvs]\n",
    "table_cols = tables[0].columns[1::2]\n",
    "class_cols = tables[0].columns[2::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e00375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure out antennas that were not flagged when the sun was down, or were only flagged for Even/Odd Zeros or Redcal chi^2 or Bad X-Engine Diffs\n",
    "ap_strs = np.array(tables[0]['Antenna'])\n",
    "ant_flags = np.array([t[class_cols] for t in tables]) == 'bad'\n",
    "sun_low_enough = np.array([t['Solar Alt'] < MAX_SOLAR_ALT for t in tables])\n",
    "ants = sorted(set(int(a[:-1]) for a in ap_strs))\n",
    "candidate_autos = set()\n",
    "for i, ap_str in enumerate(ap_strs):\n",
    "    has_other_flags = np.any([(ant_flags[:, i, cc] & sun_low_enough[:, i]) for cc, colname in enumerate(class_cols) \n",
    "                              if colname not in ['Antenna Class', 'Even/Odd Zeros Class','Redcal chi^2 Class', 'Bad Diff X-Engines Class']])\n",
    "    if not has_other_flags:\n",
    "        ap = int(ap_str[:-1]), utils.comply_pol(ap_str[-1])\n",
    "        candidate_autos.add(utils.join_bl(ap, ap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7387c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sum and diff autos, checking to see whether any of them show packet loss\n",
    "good_data = {}\n",
    "info_dicts = {}\n",
    "for sf, df in list(zip(auto_sums, auto_diffs)):\n",
    "    rv = io.read_hera_hdf5(sf, bls=candidate_autos)\n",
    "    good_data[sf] = rv['data']\n",
    "    info_dicts[sf] = rv['info']\n",
    "    diff = io.read_hera_hdf5(df, bls=candidate_autos)['data']\n",
    "    zeros_class = ant_class.even_odd_zeros_checker(good_data[sf], diff)\n",
    "    for ant in zeros_class.bad_ants:\n",
    "        candidate_autos.remove(utils.join_bl(ant, ant))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bffdadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load calibration solutions\n",
    "cs = CalibrationSmoother(cal_files, load_cspa=False, load_chisq=False, pick_refant=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d86a0fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load a priori flagged times\n",
    "if APRIORI_YAML_PATH is not None:\n",
    "    print(f'Loading a priori flagged times from {APRIORI_YAML_PATH}')\n",
    "    apriori_flags = np.zeros(len(cs.time_grid), dtype=bool)\n",
    "    apriori_flags[metrics_io.read_a_priori_int_flags(APRIORI_YAML_PATH, times=cs.time_grid).astype(int)] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb72e88",
   "metadata": {},
   "source": [
    "## Figure out a subset of most-stable antennas to filter and flag on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfd0ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_cal_flags = np.all([f for f in cs.flag_grids.values()], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b024969",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_autos(per_file_autos, bls_to_use, auto_sums, cs):\n",
    "    '''Averages autos over baselines, matching the time_grid in CalibrationSmoother cs.'''\n",
    "    avg_per_file_autos = {sf: np.mean([per_file_autos[sf][bl] for bl in bls_to_use], axis=0) for sf in auto_sums}\n",
    "    avg_autos = np.zeros((len(cs.time_grid), len(cs.freqs)), dtype=float)\n",
    "    for sf, cf in zip(auto_sums, cs.cals):\n",
    "        avg_autos[cs.time_indices[cf], :] = np.abs(avg_per_file_autos[sf])\n",
    "    return avg_autos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84aa3acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_candidate_auto = average_autos(good_data, candidate_autos, auto_sums, cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8fad94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flag_FM(flags, freqs, freq_range=[87.5e6, 108e6]):\n",
    "    '''Apply flags to all frequencies within freq_range (in Hz).'''\n",
    "    flags[:, np.logical_and(freqs >= freq_range[0], freqs <= freq_range[1])] = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd843892",
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_FM(initial_cal_flags, cs.freqs, freq_range=FM_freq_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbca83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flag_sun(flags, times, max_solar_alt=0):\n",
    "    '''Apply flags to all times where the solar altitude is greater than max_solar_alt (in degrees).'''\n",
    "    solar_altitudes_degrees = utils.get_sun_alt(times)\n",
    "    flags[solar_altitudes_degrees >= max_solar_alt, :] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13627d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_sun(initial_cal_flags, cs.time_grid, max_solar_alt=MAX_SOLAR_ALT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2feb59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if APRIORI_YAML_PATH is not None:\n",
    "    initial_cal_flags[apriori_flags, :] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d31acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_auto_noise(auto, dt, df, nsamples=1):\n",
    "    '''Predict noise on an (antenna-averaged) autocorrelation. The product of Delta t and Delta f\n",
    "    must be unitless. For N autocorrelations averaged together, use nsamples=N.'''\n",
    "    int_count = int(dt * df) * nsamples\n",
    "    return np.abs(auto) / np.sqrt(int_count / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988a7261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure out noise and weights\n",
    "int_time = 24 * 3600 * np.median(np.diff(cs.time_grid))\n",
    "chan_res = np.median(np.diff(cs.freqs))\n",
    "noise = predict_auto_noise(avg_candidate_auto, int_time, chan_res, nsamples=1)\n",
    "wgts = np.where(initial_cal_flags, 0, noise**-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba012c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get slices to index into region of waterfall outwide of which it's 100% flagged\n",
    "unflagged_ints = np.squeeze(np.argwhere(~np.all(initial_cal_flags, axis=1)))\n",
    "ints_to_filt = slice(unflagged_ints[0], unflagged_ints[-1] + 1)\n",
    "unflagged_chans = np.squeeze(np.argwhere(~np.all(initial_cal_flags, axis=0)))\n",
    "chans_to_filt = slice(unflagged_chans[0], unflagged_chans[-1] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da2b18a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Filter every autocorrelation individually\n",
    "cached_output = {}\n",
    "models = {}\n",
    "sqrt_mean_sqs = {}\n",
    "time_filters, freq_filters = dpss_filters(freqs=cs.freqs[chans_to_filt], # Hz\n",
    "                                          times=cs.time_grid[ints_to_filt], # JD\n",
    "                                          freq_scale=FREQ_FILTER_SCALE,\n",
    "                                          time_scale=TIME_FILTER_SCALE,\n",
    "                                          eigenval_cutoff=EIGENVAL_CUTOFF)\n",
    "\n",
    "for bl in candidate_autos:\n",
    "    auto_here = average_autos(good_data, [bl], auto_sums, cs)\n",
    "\n",
    "    models[bl] = np.array(auto_here)\n",
    "    model, cached_output = solve_2D_DPSS(auto_here[ints_to_filt, chans_to_filt], wgts[ints_to_filt, chans_to_filt], \n",
    "                                         time_filters, freq_filters, method='lu_solve', cached_input=cached_output)\n",
    "    models[bl][ints_to_filt, chans_to_filt] = model\n",
    "    \n",
    "    noise_model = predict_auto_noise(models[bl], int_time, chan_res, nsamples=1)   \n",
    "    sqrt_mean_sqs[bl] = np.nanmean(np.where(initial_cal_flags, np.nan, (auto_here - models[bl]) / noise_model)**2)**.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d95b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick best autocorrelations to filter on\n",
    "L2_bound = max(np.quantile(list(sqrt_mean_sqs.values()), MIN_FRAC_OF_AUTOS), MAX_AUTO_L2)\n",
    "good_auto_bls = [bl for bl in candidate_autos if sqrt_mean_sqs[bl] <= L2_bound]\n",
    "print(f'Using {len(good_auto_bls)} out of {len(candidate_autos)} candidate autocorrelations ({len(good_auto_bls) / len(candidate_autos):.2%}).') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8704844",
   "metadata": {},
   "outputs": [],
   "source": [
    "extent = [cs.freqs[0]/1e6, cs.freqs[-1]/1e6, cs.time_grid[-1] - int(cs.time_grid[0]), cs.time_grid[0] - int(cs.time_grid[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b685f4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_all_filtered_bls(N_per_row=8):\n",
    "    N_rows = int(np.ceil(len(candidate_autos) / N_per_row))\n",
    "    fig, axes = plt.subplots(N_rows, N_per_row, figsize=(14, 3 * N_rows), dpi=100,\n",
    "                             sharex=True, sharey=True, gridspec_kw={'wspace': 0, 'hspace': .18})\n",
    "\n",
    "    for i, (ax, bl) in enumerate(zip(axes.flatten(), sorted(sqrt_mean_sqs.keys(), key=lambda bl: sqrt_mean_sqs[bl]))):\n",
    "        auto_here = average_autos(good_data, [bl], auto_sums, cs)\n",
    "        noise_model = predict_auto_noise(models[bl], int_time, chan_res, nsamples=1)\n",
    "\n",
    "        im = ax.imshow(np.where(initial_cal_flags, np.nan, (auto_here - models[bl]) / noise_model).real, \n",
    "                       aspect='auto', interpolation='none', cmap='bwr', vmin=-10, vmax=10, extent=extent)\n",
    "        ax.set_title(f'{bl[0]}{bl[2][0]}: {sqrt_mean_sqs[bl]:.3}', color=('k' if sqrt_mean_sqs[bl] <= L2_bound else 'r'), fontsize=10)\n",
    "\n",
    "        if i == 0:\n",
    "            plt.colorbar(im, ax=axes, location='top', label=r'Autocorrelation z-score after DPSS filtering (with $\\langle z^2 \\rangle^{1/2}$)', extend='both', aspect=40, pad=.015)\n",
    "        if i % N_per_row == 0:\n",
    "            ax.set_ylabel(f'JD - {int(cs.time_grid[0])}')       \n",
    "    for ax in axes[-1, :]:\n",
    "        ax.set_xlabel('Frequency (MHz)')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    antpols = [(int(ap[:-1]), utils.comply_pol(ap[-1])) for ap in ap_strs]\n",
    "    other_autos = [f'{ap[0]}{ap[-1][-1]}' for ap in antpols if utils.join_bl(ap, ap) not in candidate_autos]\n",
    "    print('Not plotted here due to prior antenna flagging:')\n",
    "    print('\\t' + '\\n\\t'.join(textwrap.wrap(', '.join(other_autos), 80, break_long_words=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf57772",
   "metadata": {},
   "source": [
    "# *Figure 1: Show All DPSS Residual z-Scores*\n",
    "\n",
    "This figure shows the z-score waterfall of each antenna. Also shown is the square root of the mean of the square of each waterfall, as a metric of its instability. Antennas in red are excluded from the average of most stable antennas that are used for subsequent flagging. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afe8d48",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_all_filtered_bls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841b10b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute average autocorrelation and DPSS filter it\n",
    "avg_auto = average_autos(good_data, good_auto_bls, auto_sums, cs)\n",
    "model = np.array(avg_auto)\n",
    "submodel, _ = solve_2D_DPSS(avg_auto[ints_to_filt, chans_to_filt], wgts[ints_to_filt, chans_to_filt], \n",
    "                                time_filters, freq_filters, method='lu_solve', cached_input=cached_output)\n",
    "model[ints_to_filt, chans_to_filt] = submodel\n",
    "noise_model = predict_auto_noise(np.abs(model), int_time, chan_res, nsamples=len(good_auto_bls))\n",
    "zscore = ((avg_auto - model) / noise_model).real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41616d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_z_score(flags, zscore):\n",
    "    plt.figure(figsize=(14,10), dpi=100)\n",
    "    plt.imshow(np.where(flags, np.nan, zscore.real), aspect='auto', cmap='bwr', interpolation='none', vmin=-10, vmax=10, extent=extent)\n",
    "    plt.colorbar(location='top', label='z score', extend='both')\n",
    "    plt.xlabel('Frequency (MHz)')\n",
    "    plt.ylabel(f'JD - {int(cs.time_grid[0])}')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc11036",
   "metadata": {},
   "source": [
    "# *Figure 2: z-Score of DPSS-Filtered, Averaged Good Autocorrelation and Initial Flags*\n",
    "\n",
    "This plot shows the z-score of a DPSS-filtered, deeply averaged autocorrelation, where the noise is inferred from the integration time, channel width, and DPSS model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a338958",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_z_score(initial_cal_flags, zscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7124185b",
   "metadata": {},
   "source": [
    "## Expand original flags to include potential RFI missed by the file_calibration notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6780277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flag outliers and perform watershed for lesser outliers neighboring flags\n",
    "round_1_flags = copy.deepcopy(initial_cal_flags)\n",
    "round_1_flags[zscore > Z_THRESH] = True\n",
    "ws_flags = xrfi._ws_flag_waterfall(zscore, round_1_flags, WS_Z_THRESH)\n",
    "round_1_flags |= ws_flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255a64d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iteratively_flag_on_averaged_zscore(flags, zscore, avg_z_thresh=1.5, verbose=True):\n",
    "    '''Flag whole integrations or channels based on average z-score. This is done\n",
    "    iteratively to prevent bad times affecting channel averages or vice versa.'''\n",
    "    flagged_chan_count = 0\n",
    "    flagged_int_count = 0\n",
    "    while True:\n",
    "        zspec = np.nanmean(np.where(flags, np.nan, zscore), axis=0)\n",
    "        ztseries = np.nanmean(np.where(flags, np.nan, zscore), axis=1)\n",
    "\n",
    "        if (np.nanmax(zspec) < avg_z_thresh) and (np.nanmax(ztseries) < avg_z_thresh):\n",
    "            break\n",
    "\n",
    "        if np.nanmax(zspec) >= np.nanmax(ztseries):\n",
    "            flagged_chan_count += np.sum((zspec >= max(ztseries)) & (zspec >= avg_z_thresh))\n",
    "            flags[:, (zspec >= max(ztseries)) & (zspec >= avg_z_thresh)] = True\n",
    "        else:\n",
    "            flagged_int_count += np.sum((ztseries >= max(zspec)) & (ztseries >= avg_z_thresh))\n",
    "            flags[(ztseries >= max(zspec)) & (ztseries >= avg_z_thresh), :] = True\n",
    "\n",
    "    if verbose:\n",
    "        print(f'Flagging an additional {flagged_int_count} integrations and {flagged_chan_count} channels.')\n",
    "\n",
    "def impose_max_chan_flag_frac(flags, max_flag_frac=.25, verbose=True):\n",
    "    '''Flag channels already flagged more than max_flag_frac (excluding completely flagged times).'''\n",
    "    unflagged_times = ~np.all(flags, axis=1)\n",
    "    frequently_flagged_chans =  np.mean(flags[unflagged_times, :], axis=0) >= max_flag_frac\n",
    "    if verbose:\n",
    "        print(f'Flagging {np.sum(frequently_flagged_chans) - np.sum(np.all(flags, axis=0))} channels previously flagged {max_flag_frac:.2%} or more.')        \n",
    "    flags[:, frequently_flagged_chans] = True \n",
    "        \n",
    "def impose_max_time_flag_frac(flags, max_flag_frac=.25, verbose=True):\n",
    "    '''Flag times already flagged more than max_flag_frac (excluding completely flagged channels).'''\n",
    "    unflagged_chans = ~np.all(flags, axis=0)\n",
    "    frequently_flagged_times =  np.mean(flags[:, unflagged_chans], axis=1) >= max_flag_frac\n",
    "    if verbose:\n",
    "        print(f'Flagging {np.sum(frequently_flagged_times) - np.sum(np.all(flags, axis=1))} times previously flagged {max_flag_frac:.2%} or more.')\n",
    "    flags[frequently_flagged_times, :] = True             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c067b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag whole integrations or channels\n",
    "iteratively_flag_on_averaged_zscore(round_1_flags, zscore, avg_z_thresh=AVG_Z_THRESH, verbose=True)\n",
    "impose_max_chan_flag_frac(round_1_flags, max_flag_frac=MAX_FREQ_FLAG_FRAC, verbose=True)\n",
    "impose_max_time_flag_frac(round_1_flags, max_flag_frac=MAX_TIME_FLAG_FRAC, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04163759",
   "metadata": {},
   "source": [
    "# *Figure 3: z-Score of DPSS-Filtered, Averaged Good Autocorrelation and Expanded Flags*\n",
    "\n",
    "This is the same as Figure 2, but includes additional flags identified based on a full 2D DPSS filter of this waterfall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9505b02e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_z_score(round_1_flags, zscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec58448",
   "metadata": {},
   "source": [
    "# Make new flags from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54193f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = predict_auto_noise(avg_auto, int_time, chan_res, nsamples=len(good_auto_bls))\n",
    "wgts = wgts = np.where(round_1_flags, 0, noise**-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875e5853",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_filters, freq_filters = dpss_filters(freqs=cs.freqs[chans_to_filt], # Hz\n",
    "                                          times=cs.time_grid[ints_to_filt], # JD\n",
    "                                          freq_scale=FREQ_FILTER_SCALE,\n",
    "                                          time_scale=TIME_FILTER_SCALE,\n",
    "                                          eigenval_cutoff=EIGENVAL_CUTOFF)\n",
    "model = np.array(avg_auto)\n",
    "submodel, _ = solve_2D_DPSS(avg_auto[ints_to_filt, chans_to_filt], wgts[ints_to_filt, chans_to_filt],\n",
    "                                   time_filters, freq_filters, method='lu_solve')\n",
    "model[ints_to_filt, chans_to_filt] = submodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74a73e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_model = predict_auto_noise(np.abs(model), int_time, chan_res, nsamples=len(good_auto_bls))\n",
    "zscore = ((avg_auto - model) / noise_model).real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdb75d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "round_2_flags = np.zeros_like(round_1_flags)\n",
    "\n",
    "# flag any integrations fully-flagged by the notebooks (also accounts for missing data)\n",
    "round_2_flags[np.all(initial_cal_flags, axis=1), :] = True\n",
    "\n",
    "# flag on FM, sun-up data, and a priori flags\n",
    "flag_FM(round_2_flags, cs.freqs, freq_range=FM_freq_range)\n",
    "flag_sun(round_2_flags, cs.time_grid, max_solar_alt=MAX_SOLAR_ALT)\n",
    "if APRIORI_YAML_PATH is not None:\n",
    "    round_2_flags[apriori_flags, :] = True\n",
    "\n",
    "# flag any round 1 flags that are still moderately high z-score\n",
    "round_2_flags[round_1_flags & (zscore > REPEAT_FLAG_Z_THRESH)] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1f0721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag outliers and then perform watershed flagging\n",
    "round_2_flags[zscore.real > Z_THRESH] = True\n",
    "ws_flags = xrfi._ws_flag_waterfall(zscore.real, round_2_flags, WS_Z_THRESH)\n",
    "round_2_flags |= ws_flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63829dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag whole integrations or channels\n",
    "iteratively_flag_on_averaged_zscore(round_2_flags, zscore, avg_z_thresh=AVG_Z_THRESH, verbose=True)\n",
    "impose_max_chan_flag_frac(round_2_flags, max_flag_frac=MAX_FREQ_FLAG_FRAC, verbose=True)\n",
    "impose_max_time_flag_frac(round_2_flags, max_flag_frac=MAX_TIME_FLAG_FRAC, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01154686",
   "metadata": {},
   "source": [
    "# *Figure 4: z-Score of DPSS-Filtered, Averaged Good Autocorrelation and Final, Re-Computed Flags*\n",
    "\n",
    "This is the same as Figures 2 and 3, but now includes only the final set of flags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36360d30",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_z_score(round_2_flags, zscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d478af5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def summarize_flagging():\n",
    "    plt.figure(figsize=(14,10), dpi=100)\n",
    "    cmap = matplotlib.colors.ListedColormap(((0, 0, 0),) + matplotlib.cm.get_cmap(\"Set2\").colors[:3])\n",
    "    plt.imshow(np.where(initial_cal_flags & round_2_flags, 1, np.where(initial_cal_flags, 2, np.where(round_2_flags, 3, 0))), \n",
    "               aspect='auto', cmap=cmap, interpolation='none', extent=extent)\n",
    "    plt.clim([-.5, 3.5])\n",
    "    cbar = plt.colorbar(location='top', aspect=40, pad=.02)\n",
    "    cbar.set_ticks([0, 1, 2, 3])\n",
    "    cbar.set_ticklabels(['Unflagged', 'Flagged by both file_calibration and here', 'Flagged by file_calibration only', 'Flagged here only'])\n",
    "    plt.xlabel('Frequency (MHz)')\n",
    "    plt.ylabel(f'JD - {int(cs.time_grid[0])}')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5e82ba",
   "metadata": {},
   "source": [
    "# *Figure 5: Summary of Flags Before and After Recomputing Them*\n",
    "\n",
    "This plot shows which times and frequencies were flagged by either the [file_calibration](https://github.com/HERA-Team/hera_notebook_templates/blob/master/notebooks/file_calibration.ipynb) notebook (which also includes a priori flags imposed here like FM), which ones were flagged only in this notebook, and which ones were flagged consistently (and often independently) in both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc57b1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_flagging()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85224d0f",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14884b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_to_history = 'Produced by full_day_rfi notebook with the following environment:\\n' + '=' * 65 + '\\n' + os.popen('conda env export').read() + '=' * 65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadfc1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_flag_files = [auto_sum.replace(SUM_AUTOS_SUFFIX, OUT_FLAG_SUFFIX) for auto_sum in auto_sums]\n",
    "for auto_sum, cal, out_flag_file in zip(auto_sums, cs.cals, out_flag_files):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        # create UVFlag object based on UVData\n",
    "        uvd = UVData()\n",
    "        uvd.read(auto_sum)\n",
    "        uvf = UVFlag(uvd, waterfall=True, mode='flag')\n",
    "        uvf.use_future_array_shapes()\n",
    "\n",
    "        # fill out flags\n",
    "        for p in range(uvf.Npols):\n",
    "            uvf.flag_array[:, :, p] = round_2_flags[cs.time_indices[cal], :]\n",
    "\n",
    "        # write to disk\n",
    "        uvf.history += add_to_history\n",
    "    uvf.write(out_flag_file, clobber=True)\n",
    "    \n",
    "print(f'Saved {len(out_flag_files)} *.{OUT_FLAG_SUFFIX} files starting with {out_flag_files[0]}.')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae98d71c",
   "metadata": {},
   "source": [
    "## TODO: Explore per-antenna flagging using DPSS filters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e1ea9f",
   "metadata": {},
   "source": [
    "## Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b67d967",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for repo in ['hera_cal', 'hera_qm', 'hera_filters', 'hera_notebook_templates', 'pyuvdata']:\n",
    "    exec(f'from {repo} import __version__')\n",
    "    print(f'{repo}: {__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f691a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Finished execution in {(time.time() - tstart) / 60:.2f} minutes.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
