{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4658fc9c",
   "metadata": {},
   "source": [
    "# Full-Day Autocorrelation Checking\n",
    "\n",
    "**by Josh Dillon and Steven Murray**, last updated May 27, 2024\n",
    "\n",
    "This notebook is designed to assess per-day data quality from just autocorrelations, enabling a quick assessment of whether the day is worth pushing through further analysis. In particular, it is designed to look for times that are particularly contaminated by broadband RFI (e.g. from lightning), picking out fraction of days worth analyzing. It's output is a an a priori flag yaml file readable by `hera_qm.metrics_io` functions `read_a_priori_chan_flags()`, `read_a_priori_int_flags()`, and `read_a_priori_ant_flags()`.\n",
    "\n",
    "Here's a set of links to skip to particular figures:\n",
    "\n",
    "# [• Figure 1: Preliminary Array Flag Fraction Summary](#Figure-1:-Preliminary-Array-Flag-Fraction-Summary)\n",
    "# [• Figure 2: z-Score of DPSS-Filtered, Averaged Good Autocorrelation and Initial Flags](#Figure-2:-z-Score-of-DPSS-Filtered,-Averaged-Good-Autocorrelation-and-Initial-Flags)\n",
    "# [• Figure 3: Proposed A Priori Time Flags Based on Frequency-Averaged and Convolved z-Score Magnitude](#Figure-3:-Proposed-A-Priori-Time-Flags-Based-on-Frequency-Averaged-and-Convolved-z-Score-Magnitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de88684",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "tstart = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f7bd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HDF5_USE_FILE_LOCKING'] = 'FALSE'\n",
    "import h5py\n",
    "import hdf5plugin  # REQUIRED to have the compression plugins available\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import copy\n",
    "from scipy.ndimage import convolve\n",
    "from hera_cal import io, utils\n",
    "from hera_cal.smooth_cal import dpss_filters, solve_2D_DPSS\n",
    "from hera_qm import ant_class, xrfi, metrics_io\n",
    "from hera_qm.time_series_metrics import true_stretches, impose_max_flag_gap, metric_convolution_flagging\n",
    "from hera_filters import dspec\n",
    "import warnings\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "%matplotlib inline\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "_ = np.seterr(all='ignore')  # get rid of red warnings\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e679d6",
   "metadata": {},
   "source": [
    "## Parse input and output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed38a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get filenames\n",
    "AUTO_FILE = os.environ.get(\"AUTO_FILE\", None)\n",
    "# AUTO_FILE = '/mnt/sn1/data1/2460457/zen.2460457.16886.sum.autos.uvh5'\n",
    "\n",
    "SUM_AUTOS_SUFFIX = os.environ.get(\"SUM_AUTOS_SUFFIX\", 'sum.autos.uvh5')\n",
    "DIFF_AUTOS_SUFFIX = os.environ.get(\"DIFF_AUTOS_SUFFIX\", 'diff.autos.uvh5')\n",
    "OUT_YAML_SUFFIX = os.environ.get(\"OUT_YAML_SUFFIX\", '_apriori_flags.yaml')\n",
    "OUT_YAML_DIR = os.environ.get(\"OUT_YAML_DIR\", None)\n",
    "# OUT_YAML_DIR = '/lustre/aoc/projects/hera/jsdillon/H6C/'\n",
    "\n",
    "if OUT_YAML_DIR is None:\n",
    "    OUT_YAML_DIR = os.path.dirname(AUTO_FILE)\n",
    "\n",
    "auto_sums_glob = '.'.join(AUTO_FILE.split('.')[:-4]) + '.*.' + SUM_AUTOS_SUFFIX\n",
    "auto_diffs_glob = auto_sums_glob.replace(SUM_AUTOS_SUFFIX, DIFF_AUTOS_SUFFIX)\n",
    "out_yaml_file = os.path.join(OUT_YAML_DIR, AUTO_FILE.split('.')[-5] + OUT_YAML_SUFFIX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bbe5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_sums = sorted(glob.glob(auto_sums_glob))\n",
    "print(f'Found {len(auto_sums)} *.{SUM_AUTOS_SUFFIX} files starting with {auto_sums[0]}.')\n",
    "\n",
    "auto_diffs = sorted(glob.glob(auto_diffs_glob))\n",
    "print(f'Found {len(auto_diffs)} *.{DIFF_AUTOS_SUFFIX} files starting with {auto_diffs[0]}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b4db82",
   "metadata": {},
   "source": [
    "## Parse settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47e3af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bounds on zeros in spectra\n",
    "good_zeros_per_eo_spectrum = (0, int(os.environ.get(\"MAX_ZEROS_PER_EO_SPEC_GOOD\", 2)))\n",
    "suspect_zeros_per_eo_spectrum = (0, int(os.environ.get(\"MAX_ZEROS_PER_EO_SPEC_SUSPECT\", 8)))\n",
    "\n",
    "# bounds on autocorrelation power\n",
    "auto_power_good = (float(os.environ.get(\"AUTO_POWER_GOOD_LOW\", 5)), float(os.environ.get(\"AUTO_POWER_GOOD_HIGH\", 30)))\n",
    "auto_power_suspect = (float(os.environ.get(\"AUTO_POWER_SUSPECT_LOW\", 1)), float(os.environ.get(\"AUTO_POWER_SUSPECT_HIGH\", 60)))\n",
    "\n",
    "# bounds on autocorrelation slope\n",
    "auto_slope_good = (float(os.environ.get(\"AUTO_SLOPE_GOOD_LOW\", -0.4)), float(os.environ.get(\"AUTO_SLOPE_GOOD_HIGH\", 0.4)))\n",
    "auto_slope_suspect = (float(os.environ.get(\"AUTO_SLOPE_SUSPECT_LOW\", -0.6)), float(os.environ.get(\"AUTO_SLOPE_SUSPECT_HIGH\", 0.6)))\n",
    "\n",
    "# bounds on autocorrelation RFI\n",
    "auto_rfi_good = (0, float(os.environ.get(\"AUTO_RFI_GOOD\", 1.5)))\n",
    "auto_rfi_suspect = (0, float(os.environ.get(\"AUTO_RFI_SUSPECT\", 2)))\n",
    "\n",
    "# bounds on autocorrelation shape\n",
    "auto_shape_good = (0, float(os.environ.get(\"AUTO_SHAPE_GOOD\", 0.1)))\n",
    "auto_shape_suspect = (0, float(os.environ.get(\"AUTO_SHAPE_SUSPECT\", 0.2)))\n",
    "\n",
    "# parse RFI settings for antenna flagging\n",
    "RFI_DPSS_HALFWIDTH = float(os.environ.get(\"RFI_DPSS_HALFWIDTH\", 300e-9)) # in s\n",
    "RFI_NSIG = float(os.environ.get(\"RFI_NSIG\", 6))\n",
    "\n",
    "# parse settings for identifying mislabeled data by X-engine\n",
    "BAD_XENGINE_ZCUT = float(os.environ.get(\"BAD_XENGINE_ZCUT\", 10))\n",
    "\n",
    "# DPSS settings for full-day filtering\n",
    "FREQ_FILTER_SCALE = float(os.environ.get(\"FREQ_FILTER_SCALE\", 5.0)) # in MHz\n",
    "TIME_FILTER_SCALE = float(os.environ.get(\"TIME_FILTER_SCALE\", 450.0)) # in s\n",
    "EIGENVAL_CUTOFF = float(os.environ.get(\"EIGENVAL_CUTOFF\", 1e-12))\n",
    "\n",
    "# A priori flag settings\n",
    "FM_LOW_FREQ = float(os.environ.get(\"FM_LOW_FREQ\", 87.5)) # in MHz\n",
    "FM_HIGH_FREQ = float(os.environ.get(\"FM_HIGH_FREQ\", 108.0)) # in MHz\n",
    "FM_freq_range = [FM_LOW_FREQ * 1e6, FM_HIGH_FREQ * 1e6]\n",
    "MAX_SOLAR_ALT = float(os.environ.get(\"MAX_SOLAR_ALT\", 0.0)) # in degrees\n",
    "SMOOTHED_ABS_Z_THRESH = float(os.environ.get(\"SMOOTHED_ABS_Z_THRESH\", 10))\n",
    "WHOLE_DAY_FLAG_THRESH = float(os.environ.get(\"WHOLE_DAY_FLAG_THRESH\", 0.5))\n",
    "\n",
    "for setting in ['good_zeros_per_eo_spectrum', 'suspect_zeros_per_eo_spectrum', 'auto_power_good', 'auto_power_suspect', \n",
    "                'auto_slope_good', 'auto_slope_suspect', 'auto_rfi_good', 'auto_rfi_suspect',\n",
    "                'auto_shape_good', 'auto_shape_suspect', 'BAD_XENGINE_ZCUT', 'RFI_DPSS_HALFWIDTH', 'RFI_NSIG',\n",
    "                'FREQ_FILTER_SCALE', 'TIME_FILTER_SCALE', 'EIGENVAL_CUTOFF', 'FM_freq_range',\n",
    "                'MAX_SOLAR_ALT', 'SMOOTHED_ABS_Z_THRESH', 'WHOLE_DAY_FLAG_THRESH']:\n",
    "        print(f'{setting} = {eval(setting)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f134e729",
   "metadata": {},
   "source": [
    "## Classify Antennas and Find RFI Per-File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7f021c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0665db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_bl_zscores(data, flag_array, cache={}):\n",
    "    '''This function computes z-score arrays for each delay-filtered autocorrelation, normalized by the expected noise. \n",
    "    Flagged times/channels for the whole array are given 0 weight in filtering and are np.nan in the z-score.'''\n",
    "    zscores = {}\n",
    "    for bl in auto_bls:\n",
    "        wgts = np.array(np.logical_not(flag_array), dtype=np.float64)\n",
    "        model, _, _ = dspec.fourier_filter(hd.freqs, data[bl], wgts, filter_centers=[0], filter_half_widths=[RFI_DPSS_HALFWIDTH], mode='dpss_solve',\n",
    "                                            suppression_factors=[1e-9], eigenval_cutoff=[1e-9], cache=cache)\n",
    "        res = data[bl] - model\n",
    "        int_time = 24 * 3600 * np.median(np.diff(data.times))\n",
    "        chan_res = np.median(np.diff(data.freqs))\n",
    "        int_count = int(int_time * chan_res)\n",
    "        sigma = np.abs(model) / np.sqrt(int_count / 2)\n",
    "        zscores[bl] = res / sigma    \n",
    "        zscores[bl][flag_array] = np.nan\n",
    "\n",
    "    return zscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd19643",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rfi_from_avg_autos(data, auto_bls_to_use, prior_flags=None, nsig=RFI_NSIG):\n",
    "    '''Average together all baselines in auto_bls_to_use, then find an RFI mask by looking for outliers after DPSS filtering.'''\n",
    "    \n",
    "    # Compute int_count for all unflagged autocorrelations averaged together\n",
    "    int_time = 24 * 3600 * np.median(np.diff(data.times_by_bl[auto_bls[0][0:2]]))\n",
    "    chan_res = np.median(np.diff(data.freqs))\n",
    "    int_count = int(int_time * chan_res) * len(auto_bls_to_use)\n",
    "    avg_auto = {(-1, -1, 'ee'): np.mean([data[bl] for bl in auto_bls_to_use], axis=0)}\n",
    "    \n",
    "    # Flag RFI first with channel differences and then with DPSS\n",
    "    antenna_flags, _ = xrfi.flag_autos(avg_auto, int_count=int_count, nsig=(RFI_NSIG * 5))\n",
    "    if prior_flags is not None:\n",
    "        antenna_flags[(-1, -1, 'ee')] = prior_flags\n",
    "    _, rfi_flags = xrfi.flag_autos(avg_auto, int_count=int_count, flag_method='dpss_flagger',\n",
    "                                   flags=antenna_flags, freqs=data.freqs, filter_centers=[0],\n",
    "                                   filter_half_widths=[RFI_DPSS_HALFWIDTH], eigenval_cutoff=[1e-9], nsig=nsig)\n",
    "\n",
    "    return rfi_flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ce4810",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_autos_and_preliminary_rfi(auto_sum_file, auto_diff_file):\n",
    "    \n",
    "    hd_sum = io.HERADataFastReader(auto_sum_file)\n",
    "    sum_autos, _, _ = hd_sum.read(read_flags=False, read_nsamples=False)\n",
    "    hd_diff = io.HERADataFastReader(auto_diff_file)\n",
    "    diff_autos, _, _ = hd_diff.read(read_flags=False, read_nsamples=False, dtype=np.complex64, fix_autos_func=np.real)\n",
    "    ants = sorted(set([ant for bl in hd_sum.bls for ant in utils.split_bl(bl)]))\n",
    "    \n",
    "    # check for zeros in the evens or odds\n",
    "    zeros_class = ant_class.even_odd_zeros_checker(sum_autos, diff_autos, good=good_zeros_per_eo_spectrum, suspect=suspect_zeros_per_eo_spectrum)\n",
    "\n",
    "    # check for problems in auto power or slope\n",
    "    auto_power_class = ant_class.auto_power_checker(sum_autos, good=auto_power_good, suspect=auto_power_suspect)\n",
    "    auto_slope_class = ant_class.auto_slope_checker(sum_autos, good=auto_slope_good, suspect=auto_slope_suspect, edge_cut=100, filt_size=17)   \n",
    "    \n",
    "    overall_class = zeros_class + auto_power_class + auto_slope_class\n",
    "    if len(overall_class.good_ants) + len(overall_class.suspect_ants) > 0:\n",
    "        return overall_class, np.ones((len(sum_autos.times), len(sum_autos.freqs)), dtype=bool)\n",
    "    \n",
    "    # find initial set of flags\n",
    "    antenna_flags, array_flags = xrfi.flag_autos(sum_autos, flag_method=\"channel_diff_flagger\", nsig=RFI_NSIG * 5, \n",
    "                                                 antenna_class=overall_class, flag_broadcast_thresh=.5)\n",
    "    for key in antenna_flags:\n",
    "        antenna_flags[key] = array_flags\n",
    "    cache = {}\n",
    "    _, array_flags = xrfi.flag_autos(sum_autos, freqs=sum_autos.freqs, flag_method=\"dpss_flagger\",\n",
    "                                     nsig=RFI_NSIG, antenna_class=overall_class,\n",
    "                                     filter_centers=[0], filter_half_widths=[RFI_DPSS_HALFWIDTH],\n",
    "                                     eigenval_cutoff=[1e-9], flags=antenna_flags, mode='dpss_matrix', \n",
    "                                     cache=cache, flag_broadcast_thresh=.5)        \n",
    "\n",
    "    # check for non-noiselike x-engine diffs\n",
    "    xengine_diff_class = ant_class.non_noiselike_diff_by_xengine_checker(sum_autos, diff_autos, flag_waterfall=array_flags, \n",
    "                                                                 antenna_class=overall_class, \n",
    "                                                                 xengine_chans=96, bad_xengine_zcut=bad_xengine_zcut)\n",
    "\n",
    "    # update overall_class and return if all antennas are bad\n",
    "    overall_class += xengine_diff_class\n",
    "    if len(overall_class.good_ants) + len(overall_class.suspect_ants) > 0:\n",
    "        return overall_class, np.ones((len(sum_autos.times), len(sum_autos.freqs)), dtype=bool)\n",
    "    \n",
    "    \n",
    "    # Iteratively develop RFI mask, excess RFI classification, and autocorrelation shape classification\n",
    "    stage = 1\n",
    "    rfi_flags = np.array(array_flags)\n",
    "    prior_end_states = set()\n",
    "    while True:\n",
    "        # compute DPSS-filtered z-scores with current array-wide RFI mask\n",
    "        zscores = auto_bl_zscores(sum_autos, rfi_flags)\n",
    "        rms = {bl: np.nanmean(zscores[bl]**2)**.5 if np.any(np.isfinite(zscores[bl])) else np.inf for bl in zscores}\n",
    "\n",
    "        # figure out which autos to use for finding new set of flags\n",
    "        candidate_autos = [bl for bl in auto_bls if overall_class[utils.split_bl(bl)[0]] != 'bad']\n",
    "        if stage == 1:\n",
    "            # use best half of the unflagged antennas\n",
    "            med_rms = np.nanmedian([rms[bl] for bl in candidate_autos])\n",
    "            autos_to_use = [bl for bl in candidate_autos if rms[bl] <= med_rms]\n",
    "        elif stage == 2:\n",
    "            # use all unflagged antennas which are auto RFI good, or the best half, whichever is larger\n",
    "            med_rms = np.nanmedian([rms[bl] for bl in candidate_autos])\n",
    "            best_half_autos = [bl for bl in candidate_autos if rms[bl] <= med_rms]\n",
    "            good_autos = [bl for bl in candidate_autos if (overall_class[utils.split_bl(bl)[0]] != 'bad')\n",
    "                          and (auto_rfi_class[utils.split_bl(bl)[0]] == 'good')]\n",
    "            autos_to_use = (best_half_autos if len(best_half_autos) > len(good_autos) else good_autos)\n",
    "        elif stage == 3:\n",
    "            # use all unflagged antennas which are auto RFI good or suspect\n",
    "            autos_to_use = [bl for bl in candidate_autos if (overall_class[utils.split_bl(bl)[0]] != 'bad')]\n",
    "\n",
    "        # compute new RFI flags\n",
    "        rfi_flags = rfi_from_avg_autos(sum_autos, autos_to_use)\n",
    "\n",
    "        # perform auto shape and RFI classification\n",
    "        overall_class = auto_power_class + auto_slope_class + zeros_class + xengine_diff_class\n",
    "        auto_rfi_class = ant_class.antenna_bounds_checker(rms, good=auto_rfi_good, suspect=auto_rfi_suspect, bad=(0, np.inf))\n",
    "        overall_class += auto_rfi_class\n",
    "        auto_shape_class = ant_class.auto_shape_checker(sum_autos, good=auto_shape_good, suspect=auto_shape_suspect,\n",
    "                                                        flag_spectrum=np.sum(rfi_flags, axis=0).astype(bool), \n",
    "                                                        antenna_class=overall_class)\n",
    "        overall_class += auto_shape_class\n",
    "\n",
    "        # check for convergence by seeing whether we've previously gotten to this number of flagged antennas and channels\n",
    "        if stage == 3:\n",
    "            if (len(overall_class.bad_ants), np.sum(rfi_flags)) in prior_end_states:\n",
    "                break\n",
    "            prior_end_states.add((len(overall_class.bad_ants), np.sum(rfi_flags)))\n",
    "        else:\n",
    "            stage += 1    \n",
    "    \n",
    "    # return all flagged if all antennnas are bad, otherwise return overall class and rfi_flags\n",
    "    if len(overall_class.good_ants) + len(overall_class.suspect_ants) > 0:\n",
    "        return overall_class, np.ones((len(sum_autos.times), len(sum_autos.freqs)), dtype=bool)\n",
    "    else:\n",
    "        overall_class, rfi_flags\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04048d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifications = {}\n",
    "preliminary_rfi_flags = {}\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    for auto_sum_file, auto_diff_file in list(zip(auto_sums, auto_diffs)):\n",
    "        classifications[auto_sum_file], preliminary_rfi_flags[auto_sum_file] = classify_autos_and_preliminary_rfi(auto_sum_file, auto_diff_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061ad4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ants = set([ant for auto_sum_file in classifications for ant in classifications[auto_sum_file]])\n",
    "ant_flag_fracs = {ant: 0 for ant in all_ants}\n",
    "for classification in classifications.values():\n",
    "    for ant in classification.bad_ants:\n",
    "        ant_flag_fracs[ant] += 1\n",
    "ant_flag_fracs = {ant: ant_flag_fracs[ant] / len(classifications) for ant in all_ants}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b2fd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flag_frac_array_plot():\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 8), dpi=100, gridspec_kw={'width_ratios': [2, 1]})\n",
    "\n",
    "    def flag_frac_panel(ax, antnums, radius=7, legend=False):\n",
    "\n",
    "        ang_dict = {'Jee': (225, 405), 'Jnn': (45, 225)}\n",
    "        hd_sum = io.HERADataFastReader(auto_sums[-1])\n",
    "        xpos = np.array([hd_sum.antpos[ant[0]][0] for ant in all_ants if ant[0] in antnums])\n",
    "        ypos = np.array([hd_sum.antpos[ant[0]][1] for ant in all_ants if ant[0] in antnums])\n",
    "        scatter = ax.scatter(xpos, ypos, c='w', s=0)\n",
    "        for ant in all_ants:\n",
    "            antnum, pol = ant\n",
    "            if antnum in antnums:\n",
    "                ax.add_artist(matplotlib.patches.Wedge(tuple(hd_sum.antpos[antnum][0:2]), radius, *ang_dict[pol], color='grey'))\n",
    "                flag_frac = ant_flag_fracs[ant]\n",
    "                if flag_frac > .05:\n",
    "                    ax.add_artist(matplotlib.patches.Wedge(tuple(hd_sum.antpos[antnum][0:2]), radius * np.sqrt(flag_frac), *ang_dict[pol], color='r'))\n",
    "                ax.text(hd_sum.antpos[antnum][0], hd_sum.antpos[antnum][1], str(antnum), color='w',  va='center', ha='center', zorder=100)\n",
    "\n",
    "        ax.axis('equal')\n",
    "        ax.set_xlim([np.min(xpos) - radius * 2, np.max(xpos) + radius * 2])\n",
    "        ax.set_ylim([np.min(ypos) - radius * 2, np.max(ypos) + radius * 2])\n",
    "        ax.set_xlabel(\"East-West Position (meters)\", size=12)\n",
    "        ax.set_ylabel(\"North-South Position (meters)\", size=12)\n",
    "\n",
    "        if legend:\n",
    "            legend_objs = []\n",
    "            legend_labels = []\n",
    "\n",
    "            legend_objs.append(matplotlib.lines.Line2D([0], [0], marker='o', color='w', markeredgecolor='grey', markerfacecolor='grey', markersize=15))\n",
    "            unflagged_nights = lambda pol: np.sum([1 - ant_flag_fracs[ant] for ant in all_ants if ant[-1] == pol])\n",
    "            legend_labels.append((' \\u2571\\n').join([f'{unflagged_nights(pol):.1f} unflagged {pol[-1]}-polarized\\nantenna-nights.' for pol in ['Jee', 'Jnn']]))\n",
    "\n",
    "            legend_objs.append(matplotlib.lines.Line2D([0], [0], marker='o', color='w', markeredgecolor='red', markerfacecolor='red', markersize=15))\n",
    "            unflagged_nights = lambda pol: np.sum([ant_flag_fracs[ant] for ant in all_ants if ant[-1] == pol])\n",
    "            legend_labels.append((' \\u2571\\n').join([f'{unflagged_nights(pol):.1f} flagged {pol[-1]}-polarized\\nantenna-nights.' for pol in ['Jee', 'Jnn']]))        \n",
    "            ax.legend(legend_objs, legend_labels, ncol=1, fontsize=12)\n",
    "\n",
    "    flag_frac_panel(axes[0], sorted(set([ant[0] for ant in all_ants if ant[0] < 320])), radius=7)\n",
    "    flag_frac_panel(axes[1], sorted(set([ant[0] for ant in all_ants if ant[0] >= 320])), radius=50, legend=True)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c72ea75",
   "metadata": {},
   "source": [
    "# *Figure 1: Preliminary Array Flag Fraction Summary*\n",
    "\n",
    "Per-antenna flagging fraction of data based purely on metrics that only use autocorrelations. This is likely an underestimate of flags, since it ignores low correlation, cross-polarized antennas, and high redcal $\\chi^2$, among other factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76be9b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_frac_array_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042e61ee",
   "metadata": {},
   "source": [
    "## Load and Average Unflagged Autocorrelations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6896917",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_flag_frac = np.min(list(ant_flag_fracs.values()))\n",
    "least_flagged_ants = sorted([ant for ant in all_ants if ant_flag_fracs[ant] == min_flag_frac])\n",
    "least_flagged_autos = [utils.join_bl(ant, ant) for ant in least_flagged_ants]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe2131a",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_autos = {}\n",
    "times = []\n",
    "for auto_sum_file in auto_sums:\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "    hd_sum = io.HERADataFastReader(auto_sum_file)\n",
    "    sum_autos, _, _ = hd_sum.read(bls=least_flagged_autos, read_flags=False, read_nsamples=False)\n",
    "    avg_autos[auto_sum_file] = np.mean([sum_autos[bl] for bl in sum_autos], axis=0)\n",
    "    times.extend(hd_sum.times)\n",
    "times = np.array(times)\n",
    "freqs = hd_sum.freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d884dd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = np.vstack([flag for flag in preliminary_rfi_flags.values()])\n",
    "avg_auto = np.vstack([auto for auto in avg_autos.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60630e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flag_FM(flags, freqs, freq_range=[87.5e6, 108e6]):\n",
    "    '''Apply flags to all frequencies within freq_range (in Hz).'''\n",
    "    flags[:, np.logical_and(freqs >= freq_range[0], freqs <= freq_range[1])] = True \n",
    "    \n",
    "def flag_sun(flags, times, max_solar_alt=0):\n",
    "    '''Apply flags to all times where the solar altitude is greater than max_solar_alt (in degrees).'''\n",
    "    solar_altitudes_degrees = utils.get_sun_alt(times)\n",
    "    flags[solar_altitudes_degrees >= max_solar_alt, :] = True    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435fa581",
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_FM(flags, freqs, freq_range=FM_freq_range)\n",
    "solar_flags = np.zeros_like(flags)\n",
    "flag_sun(solar_flags, times, max_solar_alt=MAX_SOLAR_ALT)\n",
    "flags |= solar_flags\n",
    "all_flagged_times = np.all(flags, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324d4099",
   "metadata": {},
   "source": [
    "## DPSS Filter Average Autocorrlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa2326d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_auto_noise(auto, dt, df, nsamples=1):\n",
    "    '''Predict noise on an (antenna-averaged) autocorrelation. The product of Delta t and Delta f\n",
    "    must be unitless. For N autocorrelations averaged together, use nsamples=N.'''\n",
    "    int_count = int(dt * df) * nsamples\n",
    "    return np.abs(auto) / np.sqrt(int_count / 2)\n",
    "\n",
    "# Figure out noise and weights\n",
    "dt = np.median(np.diff(times))\n",
    "int_time = 24 * 3600 * dt\n",
    "chan_res = np.median(np.diff(freqs))\n",
    "noise = predict_auto_noise(avg_auto, int_time, chan_res, nsamples=len(least_flagged_ants))\n",
    "wgts = np.where(flags, 0, noise**-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3308fd10-c6f4-4252-8df3-52e91c9f4c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable handling of missing file gaps\n",
    "time_grid = np.arange(times[0], times[-1] + dt, dt)\n",
    "time_indices = {i: np.searchsorted(time_grid, t) for i, t in enumerate(times)}\n",
    "avg_auto_on_grid = np.zeros((len(time_grid), len(freqs)), dtype=float)\n",
    "wgts_on_grid = np.zeros((len(time_grid), len(freqs)), dtype=float)\n",
    "flags_on_grid = np.ones((len(time_grid), len(freqs)), dtype=bool)\n",
    "for i in time_indices:\n",
    "    avg_auto_on_grid[time_indices[i], :] = avg_auto[i, :]\n",
    "    wgts_on_grid[time_indices[i], :] = wgts[i, :]\n",
    "    flags_on_grid[time_indices[i], :] = flags[i, :]\n",
    "all_flagged_times_on_grid = np.all(flags_on_grid, axis=1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da794652",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_filters, freq_filters = dpss_filters(freqs=freqs, # Hz\n",
    "                                          times=time_grid, # JD\n",
    "                                          freq_scale=FREQ_FILTER_SCALE,\n",
    "                                          time_scale=TIME_FILTER_SCALE,\n",
    "                                          eigenval_cutoff=EIGENVAL_CUTOFF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb070c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, fit_info = solve_2D_DPSS(avg_auto_on_grid, wgts_on_grid, time_filters, freq_filters, method='lu_solve')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4703ca6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_model = predict_auto_noise(np.abs(model), int_time, chan_res, nsamples=len(least_flagged_ants))\n",
    "zscore = ((avg_auto_on_grid - model) / noise_model).real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a7f650",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_z_score(flags, zscore):\n",
    "    plt.figure(figsize=(14,10), dpi=100)\n",
    "    extent = [freqs[0]/1e6, freqs[-1]/1e6, times[-1] - int(times[0]), times[0] - int(times[0])]\n",
    "    plt.imshow(np.where(flags, np.nan, zscore.real), aspect='auto', cmap='bwr', interpolation='none', vmin=-SMOOTHED_ABS_Z_THRESH, vmax=SMOOTHED_ABS_Z_THRESH, extent=extent)\n",
    "    plt.colorbar(location='top', label='z score', extend='both')\n",
    "    plt.xlabel('Frequency (MHz)')\n",
    "    plt.ylabel(f'JD - {int(times[0])}')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5d8fea",
   "metadata": {},
   "source": [
    "# *Figure 2: z-Score of DPSS-Filtered, Averaged Good Autocorrelation and Initial Flags*\n",
    "\n",
    "This plot shows the z-score of a DPSS-filtered, deeply averaged autocorrelation, where the noise is inferred from the integration time, channel width, and DPSS model. DPSS was performed using the per-file RFI flagging analogous to that used in the file_calibration notebook, which is generally insensitive to broadband RFI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bafa722",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_z_score(flags_on_grid, zscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8c2189",
   "metadata": {},
   "source": [
    "## Find Bad Time Ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c54930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Propose flags for night times when there's too much temporal structure due to, e.g. lightning\n",
    "sigma = TIME_FILTER_SCALE / int_time\n",
    "metric = np.nanmean(np.where(flags_on_grid, np.nan, np.abs(zscore)), axis=1)\n",
    "kernel = np.exp(-np.arange(-len(metric) // 2, len(metric) // 2 + 1)**2 / 2 / sigma**2)\n",
    "kernel /= np.sum(kernel)\n",
    "convolved_metric = np.full_like(metric, np.nan)\n",
    "convolved_metric[~all_flagged_times_on_grid] = convolve(metric[~all_flagged_times_on_grid], kernel, mode='reflect')\n",
    "apriori_time_flags = np.ones_like(metric, dtype=bool)\n",
    "apriori_time_flags[~all_flagged_times_on_grid] = metric_convolution_flagging(metric[~all_flagged_times_on_grid], convolved_metric[~all_flagged_times_on_grid] >= SMOOTHED_ABS_Z_THRESH, \n",
    "                                                                             [0, SMOOTHED_ABS_Z_THRESH], sigma=(TIME_FILTER_SCALE / int_time), max_flag_gap=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518f2b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag whole day if too much of the day is flagged\n",
    "apriori_flag_frac = np.mean(apriori_time_flags[~all_flagged_times_on_grid])\n",
    "if apriori_flag_frac > WHOLE_DAY_FLAG_THRESH:\n",
    "    print(f'A priori time flag fraction of {apriori_flag_frac:.2%} is greater than {WHOLE_DAY_FLAG_THRESH:.2%}... Flagging whole day.')\n",
    "    apriori_time_flags = np.ones_like(apriori_time_flags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa04073a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apriori_flag_plot():\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.semilogy(time_grid - int(time_grid[0]), np.nanmean(np.where(flags_on_grid, np.nan, np.abs(zscore)), axis=1), label='Average |z| Over Frequency')\n",
    "    plt.semilogy(time_grid - int(time_grid[0]), convolved_metric, label=f'Convolved on {TIME_FILTER_SCALE}-second timescale')\n",
    "    plt.axhline(SMOOTHED_ABS_Z_THRESH, color='k', ls='--', label='Threshold on Convolved |z|')\n",
    "    for i, apf in enumerate(true_stretches(apriori_time_flags)):\n",
    "        plt.axvspan((time_grid - int(time_grid[0]))[apf.start], (time_grid - int(time_grid[0]))[apf.stop - 1], color='r', zorder=0, alpha=.3, \n",
    "                    label=('Proposed A Priori Flags' if i == 0 else None))\n",
    "    plt.legend()\n",
    "\n",
    "    plt.xlabel(f'JD - {int(time_grid[0])}')\n",
    "    plt.ylabel('Frequency Averaged |z-score|')\n",
    "    plt.tight_layout()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5deefd76",
   "metadata": {},
   "source": [
    "# *Figure 3: Proposed A Priori Time Flags Based on Frequency-Averaged and Convolved z-Score Magnitude*\n",
    "\n",
    "This plot shows the average (over frequency) magnitude of z-scores as a function of time. This metric is smoothed to pick out ranges of times where the DPSS residual reveals persistent temporal structure. Flags due to the sun being above the horizon are also shown. The unflagged range of times is required to be contiguous. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e25e608",
   "metadata": {},
   "outputs": [],
   "source": [
    "apriori_flag_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e0403c",
   "metadata": {},
   "source": [
    "## Write a priori flags to a yaml\n",
    "\n",
    "Also writing as a priori flags channels that are 100% flagged and antennas that are 100% flagged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0105cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = int_time / 3600 / 24 / 2\n",
    "out_yml_str = 'JD_flags: ' + str([[time_grid[apf][0] - dt / 2, time_grid[apf][-1] + dt / 2] for apf in true_stretches(apriori_time_flags)])\n",
    "out_yml_str += '\\n\\nfreq_flags: ' + str([[freqs[apf][0] - chan_res / 2, freqs[apf][-1] + chan_res / 2] for apf in true_stretches(np.all(flags, axis=0))])\n",
    "out_yml_str += '\\n\\nex_ants: ' + str(sorted([ant for ant in all_ants if ant_flag_fracs[ant] == 1])).replace(\"'\", \"\").replace('(', '[').replace(')', ']')\n",
    "out_yml_str += '\\n\\nall_ant: ' + str(sorted(all_ants)).replace(\"'\", \"\").replace('(', '[').replace(')', ']')\n",
    "out_yml_str += f'\\n\\njd_range: [{times.min()}, {times.max()}]'\n",
    "out_yml_str += f'\\n\\nfreq_range: [{freqs.min()}, {freqs.max()}]'\n",
    "\n",
    "print(f'Writing the following to {out_yaml_file}\\n' + '-' * (25 + len(out_yaml_file)))\n",
    "print(out_yml_str)\n",
    "with open(out_yaml_file, 'w') as outfile:\n",
    "    outfile.writelines(out_yml_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a54f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check output yaml file\n",
    "flagged_indices = metrics_io.read_a_priori_int_flags(out_yaml_file, time_grid)\n",
    "for i, apf in enumerate(apriori_time_flags):\n",
    "    if i in flagged_indices:\n",
    "        assert apf\n",
    "    else:\n",
    "        assert not apf\n",
    "flagged_chans = metrics_io.read_a_priori_chan_flags(out_yaml_file, freqs=freqs)\n",
    "for chan in range(len(freqs)):\n",
    "    if chan in flagged_chans:\n",
    "        assert np.all(flags[:, chan])\n",
    "    else:\n",
    "        assert not np.all(flags[:, chan])\n",
    "flagged_ants = metrics_io.read_a_priori_ant_flags(out_yaml_file)\n",
    "for ant in all_ants:\n",
    "    if ant_flag_fracs[ant] == 1:\n",
    "        assert ant in flagged_ants\n",
    "    else:\n",
    "        assert ant not in flagged_ants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e34796d",
   "metadata": {},
   "source": [
    "## Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f645e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for repo in ['hera_cal', 'hera_qm', 'hera_filters', 'hera_notebook_templates', 'pyuvdata']:\n",
    "    exec(f'from {repo} import __version__')\n",
    "    print(f'{repo}: {__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda2938e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Finished execution in {(time.time() - tstart) / 60:.2f} minutes.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
