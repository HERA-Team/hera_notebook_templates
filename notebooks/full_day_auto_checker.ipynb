{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4658fc9c",
   "metadata": {},
   "source": [
    "# Full-Day Autocorrelation Checking\n",
    "\n",
    "**by Josh Dillon and Steven Murray**, last updated April 11, 2023\n",
    "\n",
    "This notebook is designed to assess per-day data quality from just autocorrelations, enabling a quick assessment of whether the day is worth pushing through further analysis. In particular, it is designed to look for times that are particularly contaminated by broadband RFI (e.g. from lightning), picking out fraction of days worth analyzing. It's output is a an a priori flag yaml file readable by `hera_qm.metrics_io` functions `read_a_priori_chan_flags()`, `read_a_priori_int_flags()`, and `read_a_priori_ant_flags()`.\n",
    "\n",
    "Here's a set of links to skip to particular figures:\n",
    "\n",
    "# [• Figure 1: Preliminary Array Flag Fraction Summary](#Figure-1:-Preliminary-Array-Flag-Fraction-Summary)\n",
    "# [• Figure 2: z-Score of DPSS-Filtered, Averaged Good Autocorrelation and Initial Flags](#Figure-2:-z-Score-of-DPSS-Filtered,-Averaged-Good-Autocorrelation-and-Initial-Flags)\n",
    "# [• Figure 3: Proposed A Priori Time Flags Based on Frequency-Averaged and Convolved z-Score Magnitude](#Figure-3:-Proposed-A-Priori-Time-Flags-Based-on-Frequency-Averaged-and-Convolved-z-Score-Magnitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de88684",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "tstart = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f7bd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HDF5_USE_FILE_LOCKING'] = 'FALSE'\n",
    "import h5py\n",
    "import hdf5plugin  # REQUIRED to have the compression plugins available\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import copy\n",
    "from scipy.ndimage import convolve\n",
    "from hera_cal import io, utils\n",
    "from hera_cal.smooth_cal import dpss_filters, solve_2D_DPSS\n",
    "from hera_qm import ant_class, xrfi, metrics_io\n",
    "from hera_qm.time_series_metrics import true_stretches, impose_max_flag_gap, metric_convolution_flagging\n",
    "from hera_filters import dspec\n",
    "import warnings\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "%matplotlib inline\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "_ = np.seterr(all='ignore')  # get rid of red warnings\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e679d6",
   "metadata": {},
   "source": [
    "## Parse input and output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed38a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get filenames\n",
    "AUTO_FILE = os.environ.get(\"AUTO_FILE\", None)\n",
    "# AUTO_FILE = '/lustre/aoc/projects/hera/h6c-analysis/IDR2/2459869/zen.2459869.25299.sum.autos.uvh5'\n",
    "\n",
    "SUM_AUTOS_SUFFIX = os.environ.get(\"SUM_AUTOS_SUFFIX\", 'sum.autos.uvh5')\n",
    "DIFF_AUTOS_SUFFIX = os.environ.get(\"DIFF_AUTOS_SUFFIX\", 'diff.autos.uvh5')\n",
    "OUT_YAML_SUFFIX = os.environ.get(\"OUT_YAML_SUFFIX\", '_apriori_flags.yaml')\n",
    "OUT_YAML_DIR = os.environ.get(\"OUT_YAML_DIR\", None)\n",
    "# OUT_YAML_DIR = '/lustre/aoc/projects/hera/jsdillon/H6C/'\n",
    "\n",
    "if OUT_YAML_DIR is None:\n",
    "    OUT_YAML_DIR = os.path.dirname(AUTO_FILE)\n",
    "\n",
    "auto_sums_glob = '.'.join(AUTO_FILE.split('.')[:-4]) + '.*.' + SUM_AUTOS_SUFFIX\n",
    "auto_diffs_glob = auto_sums_glob.replace(SUM_AUTOS_SUFFIX, DIFF_AUTOS_SUFFIX)\n",
    "out_yaml_file = os.path.join(OUT_YAML_DIR, AUTO_FILE.split('.')[-5] + OUT_YAML_SUFFIX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bbe5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_sums = sorted(glob.glob(auto_sums_glob))\n",
    "print(f'Found {len(auto_sums)} *.{SUM_AUTOS_SUFFIX} files starting with {auto_sums[0]}.')\n",
    "\n",
    "auto_diffs = sorted(glob.glob(auto_diffs_glob))\n",
    "print(f'Found {len(auto_diffs)} *.{DIFF_AUTOS_SUFFIX} files starting with {auto_diffs[0]}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b4db82",
   "metadata": {},
   "source": [
    "## Parse settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47e3af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bounds on zeros in spectra\n",
    "good_zeros_per_eo_spectrum = (0, int(os.environ.get(\"MAX_ZEROS_PER_EO_SPEC_GOOD\", 2)))\n",
    "suspect_zeros_per_eo_spectrum = (0, int(os.environ.get(\"MAX_ZEROS_PER_EO_SPEC_SUSPECT\", 8)))\n",
    "\n",
    "# bounds on autocorrelation power\n",
    "auto_power_good = (float(os.environ.get(\"AUTO_POWER_GOOD_LOW\", 5)), float(os.environ.get(\"AUTO_POWER_GOOD_HIGH\", 30)))\n",
    "auto_power_suspect = (float(os.environ.get(\"AUTO_POWER_SUSPECT_LOW\", 1)), float(os.environ.get(\"AUTO_POWER_SUSPECT_HIGH\", 60)))\n",
    "\n",
    "# bounds on autocorrelation slope\n",
    "auto_slope_good = (float(os.environ.get(\"AUTO_SLOPE_GOOD_LOW\", -0.4)), float(os.environ.get(\"AUTO_SLOPE_GOOD_HIGH\", 0.4)))\n",
    "auto_slope_suspect = (float(os.environ.get(\"AUTO_SLOPE_SUSPECT_LOW\", -0.6)), float(os.environ.get(\"AUTO_SLOPE_SUSPECT_HIGH\", 0.6)))\n",
    "\n",
    "# bounds on autocorrelation RFI\n",
    "auto_rfi_good = (0, float(os.environ.get(\"AUTO_RFI_GOOD\", 0.015)))\n",
    "auto_rfi_suspect = (0, float(os.environ.get(\"AUTO_RFI_SUSPECT\", 0.03)))\n",
    "\n",
    "# bounds on autocorrelation shape\n",
    "auto_shape_good = (0, float(os.environ.get(\"AUTO_SHAPE_GOOD\", 0.1)))\n",
    "auto_shape_suspect = (0, float(os.environ.get(\"AUTO_SHAPE_SUSPECT\", 0.2)))\n",
    "\n",
    "# parse RFI settings for antenna flagging\n",
    "RFI_DPSS_HALFWIDTH = float(os.environ.get(\"RFI_DPSS_HALFWIDTH\", 300e-9)) # in s\n",
    "RFI_NSIG = float(os.environ.get(\"RFI_NSIG\", 6))\n",
    "\n",
    "# DPSS settings for full-day filtering\n",
    "FREQ_FILTER_SCALE = float(os.environ.get(\"FREQ_FILTER_SCALE\", 5.0)) # in MHz\n",
    "TIME_FILTER_SCALE = float(os.environ.get(\"TIME_FILTER_SCALE\", 450.0)) # in s\n",
    "EIGENVAL_CUTOFF = float(os.environ.get(\"EIGENVAL_CUTOFF\", 1e-12))\n",
    "\n",
    "# A priori flag settings\n",
    "FM_LOW_FREQ = float(os.environ.get(\"FM_LOW_FREQ\", 87.5)) # in MHz\n",
    "FM_HIGH_FREQ = float(os.environ.get(\"FM_HIGH_FREQ\", 108.0)) # in MHz\n",
    "FM_freq_range = [FM_LOW_FREQ * 1e6, FM_HIGH_FREQ * 1e6]\n",
    "MAX_SOLAR_ALT = float(os.environ.get(\"MAX_SOLAR_ALT\", 0.0)) # in degrees\n",
    "SMOOTHED_ABS_Z_THRESH = float(os.environ.get(\"SMOOTHED_ABS_Z_THRESH\", 10))\n",
    "WHOLE_DAY_FLAG_THRESH = float(os.environ.get(\"WHOLE_DAY_FLAG_THRESH\", 0.5))\n",
    "\n",
    "for setting in ['good_zeros_per_eo_spectrum', 'suspect_zeros_per_eo_spectrum', 'auto_power_good', 'auto_power_suspect', \n",
    "                'auto_slope_good', 'auto_slope_suspect', 'auto_rfi_good', 'auto_rfi_suspect',\n",
    "                'auto_shape_good', 'auto_shape_suspect', 'RFI_DPSS_HALFWIDTH', 'RFI_NSIG',\n",
    "                'FREQ_FILTER_SCALE', 'TIME_FILTER_SCALE', 'EIGENVAL_CUTOFF', \n",
    "                'FM_freq_range', 'MAX_SOLAR_ALT', 'SMOOTHED_ABS_Z_THRESH', 'WHOLE_DAY_FLAG_THRESH']:\n",
    "        print(f'{setting} = {eval(setting)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f134e729",
   "metadata": {},
   "source": [
    "## Classify Antennas and Find RFI Per-File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7f021c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ce4810",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_autos_and_preliminary_rfi(auto_sum_file, auto_diff_file):\n",
    "    \n",
    "    hd_sum = io.HERADataFastReader(auto_sum_file)\n",
    "    sum_autos, _, _ = hd_sum.read(read_flags=False, read_nsamples=False)\n",
    "    hd_diff = io.HERADataFastReader(auto_diff_file)\n",
    "    diff_autos, _, _ = hd_diff.read(read_flags=False, read_nsamples=False)    \n",
    "    ants = sorted(set([ant for bl in hd_sum.bls for ant in utils.split_bl(bl)]))\n",
    "    \n",
    "    zeros_class = ant_class.even_odd_zeros_checker(sum_autos, diff_autos, good=good_zeros_per_eo_spectrum, suspect=suspect_zeros_per_eo_spectrum)\n",
    "    auto_power_class = ant_class.auto_power_checker(sum_autos, good=auto_power_good, suspect=auto_power_suspect)\n",
    "    auto_slope_class = ant_class.auto_slope_checker(sum_autos, good=auto_slope_good, suspect=auto_slope_suspect, edge_cut=100, filt_size=17)\n",
    "    auto_rfi_class = ant_class.auto_rfi_checker(sum_autos, good=auto_rfi_good, suspect=auto_rfi_suspect, \n",
    "                                                filter_half_widths=[RFI_DPSS_HALFWIDTH], nsig=RFI_NSIG, cache=cache)\n",
    "    auto_class = auto_power_class + auto_slope_class + auto_rfi_class    \n",
    "    \n",
    "    final_class = zeros_class + auto_class\n",
    "    \n",
    "    if len(final_class.good_ants) + len(final_class.suspect_ants) > 0:\n",
    "        # Compute int_count for all unflagged autocorrelations averaged together\n",
    "        int_time = 24 * 3600 * np.median(np.diff(sum_autos.times_by_bl[hd_sum.bls[0][0:2]]))\n",
    "        chan_res = np.median(np.diff(sum_autos.freqs))\n",
    "        int_count = int(int_time * chan_res) * (len(final_class.good_ants) + len(final_class.suspect_ants))\n",
    "    \n",
    "        avg_auto = {(-1, -1, 'ee'): np.mean([sum_autos[bl] for bl in hd_sum.bls if final_class[utils.split_bl(bl)[0]] != 'bad'], axis=0)}\n",
    "        \n",
    "        # Flag RFI first with channel differences and then with DPSS\n",
    "        antenna_flags, _ = xrfi.flag_autos(avg_auto, int_count=int_count, nsig=(RFI_NSIG * 5))\n",
    "        _, rfi_flags = xrfi.flag_autos(avg_auto, int_count=int_count, flag_method='dpss_flagger',\n",
    "                                       flags=antenna_flags, freqs=sum_autos.freqs, filter_centers=[0],\n",
    "                                       filter_half_widths=[RFI_DPSS_HALFWIDTH], eigenval_cutoff=[1e-9], nsig=RFI_NSIG)\n",
    "        auto_shape_class = ant_class.auto_shape_checker(sum_autos, good=auto_shape_good, suspect=auto_shape_suspect,\n",
    "                                                flag_spectrum=np.sum(rfi_flags, axis=0).astype(bool), antenna_class=final_class)\n",
    "        final_class += auto_shape_class\n",
    "\n",
    "    else:\n",
    "        rfi_flags = np.ones((len(sum_autos.times), len(sum_autos.freqs)), dtype=bool)\n",
    "\n",
    "    return final_class, rfi_flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04048d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifications = {}\n",
    "preliminary_rfi_flags = {}\n",
    "for auto_sum_file, auto_diff_file in list(zip(auto_sums, auto_diffs)):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"once\")\n",
    "        classifications[auto_sum_file], preliminary_rfi_flags[auto_sum_file] = classify_autos_and_preliminary_rfi(auto_sum_file, auto_diff_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061ad4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ants = set([ant for auto_sum_file in classifications for ant in classifications[auto_sum_file]])\n",
    "ant_flag_fracs = {ant: 0 for ant in all_ants}\n",
    "for classification in classifications.values():\n",
    "    for ant in classification.bad_ants:\n",
    "        ant_flag_fracs[ant] += 1\n",
    "ant_flag_fracs = {ant: ant_flag_fracs[ant] / len(classifications) for ant in all_ants}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b2fd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flag_frac_array_plot():\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 8), dpi=100, gridspec_kw={'width_ratios': [2, 1]})\n",
    "\n",
    "    def flag_frac_panel(ax, antnums, radius=7, legend=False):\n",
    "\n",
    "        ang_dict = {'Jee': (225, 405), 'Jnn': (45, 225)}\n",
    "        hd_sum = io.HERADataFastReader(auto_sums[-1])\n",
    "        xpos = np.array([hd_sum.antpos[ant[0]][0] for ant in all_ants if ant[0] in antnums])\n",
    "        ypos = np.array([hd_sum.antpos[ant[0]][1] for ant in all_ants if ant[0] in antnums])\n",
    "        scatter = ax.scatter(xpos, ypos, c='w', s=0)\n",
    "        for ant in all_ants:\n",
    "            antnum, pol = ant\n",
    "            if antnum in antnums:\n",
    "                ax.add_artist(matplotlib.patches.Wedge(tuple(hd_sum.antpos[antnum][0:2]), radius, *ang_dict[pol], color='grey'))\n",
    "                flag_frac = ant_flag_fracs[ant]\n",
    "                if flag_frac > .05:\n",
    "                    ax.add_artist(matplotlib.patches.Wedge(tuple(hd_sum.antpos[antnum][0:2]), radius * np.sqrt(flag_frac), *ang_dict[pol], color='r'))\n",
    "                ax.text(hd_sum.antpos[antnum][0], hd_sum.antpos[antnum][1], str(antnum), color='w',  va='center', ha='center', zorder=100)\n",
    "\n",
    "        ax.axis('equal')\n",
    "        ax.set_xlim([np.min(xpos) - radius * 2, np.max(xpos) + radius * 2])\n",
    "        ax.set_ylim([np.min(ypos) - radius * 2, np.max(ypos) + radius * 2])\n",
    "        ax.set_xlabel(\"East-West Position (meters)\", size=12)\n",
    "        ax.set_ylabel(\"North-South Position (meters)\", size=12)\n",
    "\n",
    "        if legend:\n",
    "            legend_objs = []\n",
    "            legend_labels = []\n",
    "\n",
    "            legend_objs.append(matplotlib.lines.Line2D([0], [0], marker='o', color='w', markeredgecolor='grey', markerfacecolor='grey', markersize=15))\n",
    "            unflagged_nights = lambda pol: np.sum([1 - ant_flag_fracs[ant] for ant in all_ants if ant[-1] == pol])\n",
    "            legend_labels.append((' \\u2571\\n').join([f'{unflagged_nights(pol):.1f} unflagged {pol[-1]}-polarized\\nantenna-nights.' for pol in ['Jee', 'Jnn']]))\n",
    "\n",
    "            legend_objs.append(matplotlib.lines.Line2D([0], [0], marker='o', color='w', markeredgecolor='red', markerfacecolor='red', markersize=15))\n",
    "            unflagged_nights = lambda pol: np.sum([ant_flag_fracs[ant] for ant in all_ants if ant[-1] == pol])\n",
    "            legend_labels.append((' \\u2571\\n').join([f'{unflagged_nights(pol):.1f} flagged {pol[-1]}-polarized\\nantenna-nights.' for pol in ['Jee', 'Jnn']]))        \n",
    "            ax.legend(legend_objs, legend_labels, ncol=1, fontsize=12)\n",
    "\n",
    "    flag_frac_panel(axes[0], sorted(set([ant[0] for ant in all_ants if ant[0] < 320])), radius=7)\n",
    "    flag_frac_panel(axes[1], sorted(set([ant[0] for ant in all_ants if ant[0] >= 320])), radius=50, legend=True)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c72ea75",
   "metadata": {},
   "source": [
    "# *Figure 1: Preliminary Array Flag Fraction Summary*\n",
    "\n",
    "Per-antenna flagging fraction of data based purely on metrics that only use autocorrelations. This is likely an underestimate of flags, since it ignores low correlation, cross-polarized antennas, and high redcal $\\chi^2$, among other factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76be9b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_frac_array_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042e61ee",
   "metadata": {},
   "source": [
    "## Load and Average Unflagged Autocorrelations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6896917",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_flag_frac = np.min(list(ant_flag_fracs.values()))\n",
    "least_flagged_ants = sorted([ant for ant in all_ants if ant_flag_fracs[ant] == min_flag_frac])\n",
    "least_flagged_autos = [utils.join_bl(ant, ant) for ant in least_flagged_ants]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe2131a",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_autos = {}\n",
    "times = []\n",
    "for auto_sum_file in auto_sums:\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "    hd_sum = io.HERADataFastReader(auto_sum_file)\n",
    "    sum_autos, _, _ = hd_sum.read(bls=least_flagged_autos, read_flags=False, read_nsamples=False)\n",
    "    avg_autos[auto_sum_file] = np.mean([sum_autos[bl] for bl in sum_autos], axis=0)\n",
    "    times.extend(hd_sum.times)\n",
    "times = np.array(times)\n",
    "freqs = hd_sum.freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d884dd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = np.vstack([flag for flag in preliminary_rfi_flags.values()])\n",
    "avg_auto = np.vstack([auto for auto in avg_autos.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60630e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flag_FM(flags, freqs, freq_range=[87.5e6, 108e6]):\n",
    "    '''Apply flags to all frequencies within freq_range (in Hz).'''\n",
    "    flags[:, np.logical_and(freqs >= freq_range[0], freqs <= freq_range[1])] = True \n",
    "    \n",
    "def flag_sun(flags, times, max_solar_alt=0):\n",
    "    '''Apply flags to all times where the solar altitude is greater than max_solar_alt (in degrees).'''\n",
    "    solar_altitudes_degrees = utils.get_sun_alt(times)\n",
    "    flags[solar_altitudes_degrees >= max_solar_alt, :] = True    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435fa581",
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_FM(flags, freqs, freq_range=FM_freq_range)\n",
    "solar_flags = np.zeros_like(flags)\n",
    "flag_sun(solar_flags, times, max_solar_alt=MAX_SOLAR_ALT)\n",
    "flags |= solar_flags\n",
    "all_flagged_times = np.all(flags, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324d4099",
   "metadata": {},
   "source": [
    "## DPSS Filter Average Autocorrlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa2326d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_auto_noise(auto, dt, df, nsamples=1):\n",
    "    '''Predict noise on an (antenna-averaged) autocorrelation. The product of Delta t and Delta f\n",
    "    must be unitless. For N autocorrelations averaged together, use nsamples=N.'''\n",
    "    int_count = int(dt * df) * nsamples\n",
    "    return np.abs(auto) / np.sqrt(int_count / 2)\n",
    "\n",
    "# Figure out noise and weights\n",
    "int_time = 24 * 3600 * np.median(np.diff(times))\n",
    "chan_res = np.median(np.diff(freqs))\n",
    "noise = predict_auto_noise(avg_auto, int_time, chan_res, nsamples=len(least_flagged_ants))\n",
    "wgts = np.where(flags, 0, noise**-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da794652",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_filters, freq_filters = dpss_filters(freqs=freqs, # Hz\n",
    "                                          times=times, # JD\n",
    "                                          freq_scale=FREQ_FILTER_SCALE,\n",
    "                                          time_scale=TIME_FILTER_SCALE,\n",
    "                                          eigenval_cutoff=EIGENVAL_CUTOFF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb070c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, fit_info = solve_2D_DPSS(avg_auto, wgts, time_filters, freq_filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4703ca6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_model = predict_auto_noise(np.abs(model), int_time, chan_res, nsamples=len(least_flagged_ants))\n",
    "zscore = ((avg_auto - model) / noise_model).real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a7f650",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_z_score(flags, zscore):\n",
    "    plt.figure(figsize=(14,10), dpi=100)\n",
    "    extent = [freqs[0]/1e6, freqs[-1]/1e6, times[-1] - int(times[0]), times[0] - int(times[0])]\n",
    "    plt.imshow(np.where(flags, np.nan, zscore.real), aspect='auto', cmap='bwr', interpolation='none', vmin=-SMOOTHED_ABS_Z_THRESH, vmax=SMOOTHED_ABS_Z_THRESH, extent=extent)\n",
    "    plt.colorbar(location='top', label='z score', extend='both')\n",
    "    plt.xlabel('Frequency (MHz)')\n",
    "    plt.ylabel(f'JD - {int(times[0])}')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5d8fea",
   "metadata": {},
   "source": [
    "# *Figure 2: z-Score of DPSS-Filtered, Averaged Good Autocorrelation and Initial Flags*\n",
    "\n",
    "This plot shows the z-score of a DPSS-filtered, deeply averaged autocorrelation, where the noise is inferred from the integration time, channel width, and DPSS model. DPSS was performed using the per-file RFI flagging analogous to that used in the file_calibration notebook, which is generally insensitive to broadband RFI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bafa722",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_z_score(flags, zscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8c2189",
   "metadata": {},
   "source": [
    "## Find Bad Time Ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c54930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Propose flags for night times when there's too much temporal structure due to, e.g. lightning\n",
    "sigma = TIME_FILTER_SCALE / int_time\n",
    "metric = np.nanmean(np.where(flags, np.nan, np.abs(zscore)), axis=1)\n",
    "kernel = np.exp(-np.arange(-len(metric) // 2, len(metric) // 2 + 1)**2 / 2 / sigma**2)\n",
    "kernel /= np.sum(kernel)\n",
    "convolved_metric = np.full_like(metric, np.nan)\n",
    "convolved_metric[~all_flagged_times] = convolve(metric[~all_flagged_times], kernel, mode='reflect')\n",
    "apriori_time_flags = np.ones_like(metric, dtype=bool)\n",
    "apriori_time_flags[~all_flagged_times] = metric_convolution_flagging(metric[~all_flagged_times], convolved_metric[~all_flagged_times] >= SMOOTHED_ABS_Z_THRESH, \n",
    "                                                               [0, SMOOTHED_ABS_Z_THRESH], sigma=(TIME_FILTER_SCALE / int_time), max_flag_gap=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518f2b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag whole day if too much of the day is flagged\n",
    "apriori_flag_frac = np.mean(apriori_time_flags[~all_flagged_times])\n",
    "if apriori_flag_frac > WHOLE_DAY_FLAG_THRESH:\n",
    "    print(f'A priori time flag fraction of {apriori_flag_frac:.2%} is greater than {WHOLE_DAY_FLAG_THRESH:.2%}... Flagging whole day.')\n",
    "    apriori_time_flags = np.ones_like(apriori_time_flags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa04073a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apriori_flag_plot():\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.semilogy(times - int(times[0]), np.nanmean(np.where(flags, np.nan, np.abs(zscore)), axis=1), label='Average |z| Over Frequency')\n",
    "    plt.semilogy(times - int(times[0]), convolved_metric, label=f'Convolved on {TIME_FILTER_SCALE}-second timescale')\n",
    "    plt.axhline(SMOOTHED_ABS_Z_THRESH, color='k', ls='--', label='Threshold on Convolved |z|')\n",
    "    for i, apf in enumerate(true_stretches(apriori_time_flags)):\n",
    "        plt.axvspan((times - int(times[0]))[apf.start], (times - int(times[0]))[apf.stop - 1], color='r', zorder=0, alpha=.3, \n",
    "                    label=('Proposed A Priori Flags' if i == 0 else None))\n",
    "    plt.legend()\n",
    "\n",
    "    plt.xlabel(f'JD - {int(times[0])}')\n",
    "    plt.ylabel('Frequency Averaged |z-score|')\n",
    "    plt.tight_layout()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5deefd76",
   "metadata": {},
   "source": [
    "# *Figure 3: Proposed A Priori Time Flags Based on Frequency-Averaged and Convolved z-Score Magnitude*\n",
    "\n",
    "This plot shows the average (over frequency) magnitude of z-scores as a function of time. This metric is smoothed to pick out ranges of times where the DPSS residual reveals persistent temporal structure. Flags due to the sun being above the horizon are also shown. The unflagged range of times is required to be contiguous. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e25e608",
   "metadata": {},
   "outputs": [],
   "source": [
    "apriori_flag_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e0403c",
   "metadata": {},
   "source": [
    "## Write a priori flags to a yaml\n",
    "\n",
    "Also writing as a priori flags channels that are 100% flagged and antennas that are 100% flagged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0105cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = int_time / 3600 / 24 / 2\n",
    "out_yml_str = 'JD_flags: ' + str([[times[apf][0] - dt, times[apf][-1] + dt] for apf in true_stretches(apriori_time_flags)])\n",
    "out_yml_str += '\\n\\nfreq_flags: ' + str([[freqs[apf][0] - chan_res / 2, freqs[apf][-1] + chan_res / 2] for apf in true_stretches(np.all(flags, axis=0))])\n",
    "out_yml_str += '\\n\\nex_ants: ' + str(sorted([ant for ant in all_ants if ant_flag_fracs[ant] == 1])).replace(\"'\", \"\").replace('(', '[').replace(')', ']')\n",
    "out_yml_str += '\\n\\nall_ant: ' + str(sorted(all_ants)).replace(\"'\", \"\").replace('(', '[').replace(')', ']')\n",
    "out_yml_str += f'\\n\\njd_range: [{times.min()}, {times.max()}]'\n",
    "out_yml_str += f'\\n\\nfreq_range: [{freqs.min()}, {freqs.max()}]'\n",
    "\n",
    "print(f'Writing the following to {out_yaml_file}\\n' + '-' * (25 + len(out_yaml_file)))\n",
    "print(out_yml_str)\n",
    "with open(out_yaml_file, 'w') as outfile:\n",
    "    outfile.writelines(out_yml_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a54f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check output yaml file\n",
    "flagged_indices = metrics_io.read_a_priori_int_flags(out_yaml_file, times)\n",
    "for i, apf in enumerate(apriori_time_flags):\n",
    "    if i in flagged_indices:\n",
    "        assert apf\n",
    "    else:\n",
    "        assert not apf\n",
    "flagged_chans = metrics_io.read_a_priori_chan_flags(out_yaml_file, freqs=freqs)\n",
    "for chan in range(len(freqs)):\n",
    "    if chan in flagged_chans:\n",
    "        assert np.all(flags[:, chan])\n",
    "    else:\n",
    "        assert not np.all(flags[:, chan])\n",
    "flagged_ants = metrics_io.read_a_priori_ant_flags(out_yaml_file)\n",
    "for ant in all_ants:\n",
    "    if ant_flag_fracs[ant] == 1:\n",
    "        assert ant in flagged_ants\n",
    "    else:\n",
    "        assert ant not in flagged_ants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e34796d",
   "metadata": {},
   "source": [
    "## Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f645e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for repo in ['hera_cal', 'hera_qm', 'hera_filters', 'hera_notebook_templates', 'pyuvdata']:\n",
    "    exec(f'from {repo} import __version__')\n",
    "    print(f'{repo}: {__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda2938e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Finished execution in {(time.time() - tstart) / 60:.2f} minutes.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
