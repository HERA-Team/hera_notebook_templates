{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a6c2fe1",
   "metadata": {},
   "source": [
    "# Single File Delay Filtered Average Z-Score\n",
    "\n",
    "**by Josh Dillon**, last updated July 26, 2023\n",
    "\n",
    "This notebook is designed to calculate a metric used for finding low-level RFI in redundantly-averaged cross-correlations, which are then incoherently averaged across well-sampled baselines.\n",
    "\n",
    "The actual decision of which times to flag is deferred to another notebook, full_day_rfi_round_2.ipynb\n",
    "\n",
    "Here's a set of links to skip to particular figures and tables:\n",
    "# [• Figure 1: z-Score Spectra for All Integrations in the File](#Figure-1:-z-Score-Spectra-for-All-Integrations-in-the-File)\n",
    "# [• Figure 2: Histogram of z-Scores](#Figure-2:-Histogram-of-z-Scores)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab747439",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "tstart = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d960a410",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HDF5_USE_FILE_LOCKING'] = 'FALSE'\n",
    "import h5py\n",
    "import hdf5plugin  # REQUIRED to have the compression plugins available\n",
    "import numpy as np\n",
    "import copy\n",
    "import glob\n",
    "from hera_cal import io, utils, redcal, apply_cal, datacontainer, vis_clean\n",
    "from hera_filters import dspec\n",
    "from pyuvdata import UVFlag, UVData\n",
    "from scipy import constants\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542e161e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get input data file names\n",
    "SUM_FILE = os.environ.get(\"SUM_FILE\", None)\n",
    "# SUM_FILE = '/lustre/aoc/projects/hera/h6c-analysis/IDR2/2459861/zen.2459861.59008.sum.uvh5'\n",
    "SUM_SUFFIX = os.environ.get(\"SUM_SUFFIX\", 'sum.uvh5')\n",
    "\n",
    "# get a posteriori yaml from smooth_cal\n",
    "APOSTERIORI_YAML_SUFFIX = os.environ.get(\"APOSTERIORI_YAML_SUFFIX\", '_aposteriori_flags.yaml')\n",
    "aposteriori_yaml_file = os.path.join(os.path.dirname(SUM_FILE), SUM_FILE.split('.')[-4] + APOSTERIORI_YAML_SUFFIX)\n",
    "\n",
    "# get input calibration files and flags\n",
    "SMOOTH_CAL_SUFFIX = os.environ.get(\"CAL_SUFFIX\", 'sum.smooth.calfits')\n",
    "SMOOTH_CAL_FILE = SUM_FILE.replace(SUM_SUFFIX, SMOOTH_CAL_SUFFIX)\n",
    "\n",
    "# get output file suffix\n",
    "ZSCORE_SUFFIX =  os.environ.get(\"ZSCORE_SUFFIX\", 'sum.zscore.h5')\n",
    "ZSCORE_OUTFILE =  SUM_FILE.replace(SUM_SUFFIX, ZSCORE_SUFFIX)\n",
    "\n",
    "# get delay filtering parameters\n",
    "FM_LOW_FREQ = float(os.environ.get(\"FM_LOW_FREQ\", 87.5)) # in MHz\n",
    "FM_HIGH_FREQ = float(os.environ.get(\"FM_HIGH_FREQ\", 108.0)) # in MHz\n",
    "MIN_SAMP_FRAC = float(os.environ.get(\"MIN_SAMP_FRAC\", .05))\n",
    "FILTER_DELAY = float(os.environ.get(\"FILTER_DELAY\", 500e-9))\n",
    "EIGENVAL_CUTOFF = float(os.environ.get(\"EIGENVAL_CUTOFF\", 1e-12))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee02976c",
   "metadata": {},
   "source": [
    "## Load data, calibrate, and redundantly average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a2ff7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sum and diff data\n",
    "hd = io.HERADataFastReader(SUM_FILE)\n",
    "data, flags, nsamples = hd.read(pols=['ee', 'nn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752598ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure out high and low bands\n",
    "low_band = slice(0, np.argwhere(hd.freqs > FM_LOW_FREQ * 1e6)[0][0])\n",
    "high_band = slice(np.argwhere(hd.freqs > FM_HIGH_FREQ * 1e6)[0][0], len(hd.freqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bf4d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load calibration solutions and gain flags\n",
    "hc_smooth = io.HERACal(SMOOTH_CAL_FILE)\n",
    "smooth_gains, cal_flags, _, _ = hc_smooth.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bba40b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle the the case where the smooth_cal flags are all True, trying to maintain consistent data shapes\n",
    "ALL_FLAGGED = False\n",
    "if np.all([flag for flag in cal_flags.values()]):\n",
    "    print('This file is entirely flagged. Proceeding with averaging and filtering using earlier flags, '\n",
    "          'but the output data products will still be fully flagged.')\n",
    "    ALL_FLAGGED = True\n",
    "    \n",
    "    # Likely the file was fully flagged for broadband RFI, so instead use the original flags from file_calibration\n",
    "    cal_flags = abs_cal_flags\n",
    "    # And if that didn't work, just make the flags all False for now (though ex_ants will still be exluded via reds)\n",
    "    if np.all([flag for flag in cal_flags.values()]):\n",
    "        cal_flags = {ant: np.zeros_like(cal_flags[ant]) for ant in cal_flags}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25e1a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def red_average(reds, data, nsamples, gains, flags={}, cal_flags={}):    \n",
    "    # Redundantly average data\n",
    "    wgts = datacontainer.DataContainer({bl: nsamples[bl] * ~(flags.get(bl, False) | cal_flags.get(utils.split_bl(bl)[0], False) \\\n",
    "                                                             | cal_flags.get(utils.split_bl(bl)[1], False)) for bl in nsamples})\n",
    "    sol = redcal.RedSol(reds, gains=gains)\n",
    "    sol.update_vis_from_data(data, wgts=wgts)\n",
    "    \n",
    "    # Figure out redundantly averaged flags and nsamples\n",
    "    red_avg_flags = {}\n",
    "    red_avg_nsamples = {}\n",
    "    for red in reds:\n",
    "        if red[0] in sol.vis:\n",
    "            red_avg_flags[red[0]] = np.all([wgts[bl] == 0 for bl in red], axis=0) | ~np.isfinite(sol.vis[red[0]])\n",
    "            red_avg_nsamples[red[0]] = np.sum([nsamples[bl] for bl in red if not np.all(wgts[bl] == 0)], axis=0)\n",
    "        else:\n",
    "            # empty placeholders to make sure every file has the same shape for the whole day\n",
    "            sol.vis[red[0]] = np.zeros_like(next(iter(data.values())))\n",
    "            red_avg_flags[red[0]] = np.ones_like(next(iter(flags.values())))\n",
    "            red_avg_nsamples[red[0]] = np.zeros_like(next(iter(nsamples.values())))\n",
    "    sol.make_sol_finite()\n",
    "    \n",
    "    # Build output RedDataContainers \n",
    "    red_avg_data = datacontainer.RedDataContainer(sol.vis, reds)\n",
    "    red_avg_flags = datacontainer.RedDataContainer(red_avg_flags, reds)\n",
    "    red_avg_nsamples = datacontainer.RedDataContainer(red_avg_nsamples, reds)\n",
    "    return red_avg_data, red_avg_flags, red_avg_nsamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7656e36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reds = redcal.get_reds(hd.data_antpos, pols=['ee', 'nn'], include_autos=True)\n",
    "red_avg_data, red_avg_flags, red_avg_nsamples = red_average(reds, data, nsamples, smooth_gains, flags=flags, cal_flags=cal_flags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4859827",
   "metadata": {},
   "outputs": [],
   "source": [
    "del data, nsamples, flags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3ec0d8",
   "metadata": {},
   "source": [
    "## Delay filter redundantly-averaged data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d700b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute weights based on RFI flags and autocorrelations, assumes that nsamples is constant across a spectrum\n",
    "wgts = {}\n",
    "for pol in ['ee', 'nn']:\n",
    "    rfi_flags = np.all([red_avg_flags[bl] for bl in red_avg_flags if bl[2] == pol], axis=0)\n",
    "    auto_bl = [bl for bl in red_avg_data if bl[0] == bl[1] and bl[2] == pol][0]\n",
    "    wgts[pol] = np.where(rfi_flags, 0, np.abs(red_avg_data[auto_bl])**-2)\n",
    "    wgts[pol] /= np.nanmean(np.where(rfi_flags, np.nan, wgts[pol]))  # avoid dynamic range issues\n",
    "    wgts[pol][~np.isfinite(wgts[pol])] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35293279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick out baselines with enough median nsamples and light-travel times shorter than the filter delay\n",
    "min_nsamples = np.max([np.max(red_avg_nsamples[bl]) for bl in red_avg_nsamples]) * MIN_SAMP_FRAC\n",
    "bls_to_filter = [bl for bl in red_avg_data if (np.median(red_avg_nsamples[bl]) >= min_nsamples)]\n",
    "bls_to_filter = [bl for bl in bls_to_filter if np.linalg.norm(hd.antpos[bl[0]] - hd.antpos[bl[1]]) / constants.c < FILTER_DELAY]\n",
    "\n",
    "# perform delay filter\n",
    "cache = {}\n",
    "dly_filt_red_avg_data = copy.deepcopy(red_avg_data)\n",
    "for bl in bls_to_filter:\n",
    "    d_mdl = np.zeros_like(dly_filt_red_avg_data[bl])\n",
    "    for band in [low_band, high_band]:\n",
    "        d_mdl[:, band], _, info = dspec.fourier_filter(hd.freqs[band], dly_filt_red_avg_data[bl][:, band], \n",
    "                                                       wgts=wgts[bl[2]][:, band], filter_centers=[0], \n",
    "                                                       filter_half_widths=[FILTER_DELAY], mode='dpss_solve', \n",
    "                                                       eigenval_cutoff=[EIGENVAL_CUTOFF], suppression_factors=[EIGENVAL_CUTOFF], \n",
    "                                                       max_contiguous_edge_flags=len(hd.freqs), cache=cache)\n",
    "    dly_filt_red_avg_data[bl] = np.where(red_avg_flags[bl], 0, red_avg_data[bl] - d_mdl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8035cece",
   "metadata": {},
   "source": [
    "## Calculate z-scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58111145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate how much signal loss we should expect for white noise\n",
    "filters_low = dspec.dpss_operator(hd.freqs[low_band], [0], [FILTER_DELAY], eigenval_cutoff=[EIGENVAL_CUTOFF])[0]\n",
    "filter_frac_low = filters_low.shape[1] / filters_low.shape[0]\n",
    "filters_high = dspec.dpss_operator(hd.freqs[high_band], [0], [FILTER_DELAY], eigenval_cutoff=[EIGENVAL_CUTOFF])[0]\n",
    "filter_frac_high = filters_high.shape[1] / filters_high.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc8c61e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "zscore = {}\n",
    "for pol in ['ee', 'nn']:\n",
    "    to_avg = []\n",
    "    weights = []\n",
    "\n",
    "    for bl in bls_to_filter:\n",
    "        auto_bl = auto_bl = [k for k in red_avg_data if k[0] == k[1] and k[2] == bl[2]][0]\n",
    "        if (bl[2] == pol) and (bl != auto_bl):            \n",
    "            # calcualte predicted variance\n",
    "            dt = np.median(np.diff(hd.times)) * 24 * 3600\n",
    "            df = np.median(np.diff(hd.freqs)) \n",
    "            predicted_variance = (np.abs(red_avg_data[auto_bl]))**2 / red_avg_nsamples[bl] / dt / df \n",
    "            predicted_variance[:, low_band] *= (1 - filter_frac_low)\n",
    "            predicted_variance[:, high_band] *= (1 - filter_frac_high)\n",
    "            predicted_variance[red_avg_flags[bl]] = np.nan\n",
    "            \n",
    "            # prep for inverse variance weighting\n",
    "            if np.any(np.isfinite(predicted_variance)):\n",
    "                weights.append(np.where(np.isfinite(predicted_variance) & np.isfinite(dly_filt_red_avg_data[bl]), predicted_variance**-1, 0))\n",
    "                to_avg.append(np.where(np.isfinite(predicted_variance) & np.isfinite(dly_filt_red_avg_data[bl]),  np.abs(dly_filt_red_avg_data[bl]), 0))\n",
    "    \n",
    "    # perform inverse variance weighred average\n",
    "    Wsum = np.sum(weights, axis=0)**-1\n",
    "    estimator = np.einsum(\"mij,mij->ij\", to_avg, weights) * Wsum\n",
    "    \n",
    "    # turn estimator into z-score assuming Rayleigh-distributed data (appropriate for averaging magnitudes of visibilities incoherently)\n",
    "    predicted_mean = np.sum(np.array(weights)**.5, axis=0) * Wsum * (np.pi / 4)**.5\n",
    "    predicted_var = (4 - np.pi) / 4 * Wsum\n",
    "    zscore[pol] = (estimator - predicted_mean) / predicted_var**.5          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6f7a49",
   "metadata": {},
   "source": [
    "## Plotting Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec86321",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_zscores():\n",
    "    fig, axes = plt.subplots(2, 1, sharey=True, sharex=True, figsize=(12, 6), gridspec_kw={'hspace': 0})\n",
    "    for ax, pol in zip(axes, ['ee', 'nn']):\n",
    "\n",
    "        for i, time in enumerate(hd.times):\n",
    "            ax.plot(hd.freqs / 1e6, zscore[pol][i, :], label=f'JD: {hd.times[i]:.6f}', alpha=.75)\n",
    "        \n",
    "        ax.set_ylabel(f'{pol}-polarized z-score')\n",
    "    axes[0].legend()        \n",
    "    axes[1].set_xlabel('Frequency (MHz)')\n",
    "    plt.tight_layout()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7183c226",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_zscore_hist():\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    bins = np.arange(-np.nanmax(np.abs(list(zscore.values()))) - 1, np.nanmax(np.abs(list(zscore.values()))) + 1, .1)\n",
    "    hist_ee = plt.hist(np.ravel(zscore['ee']), bins=bins, density=True, label='ee-polarized z-scores', alpha=.5)\n",
    "    hist_nn = plt.hist(np.ravel(zscore['nn']), bins=bins, density=True, label='nn-polarized z-scores', alpha=.5)\n",
    "    plt.plot(bins, (2*np.pi)**-.5 * np.exp(-bins**2 / 2), 'k--', label='Gaussian approximate\\nnoise-only distribution')\n",
    "    plt.yscale('log')\n",
    "    all_densities = np.concatenate([hist_ee[0][hist_ee[0] > 0], hist_nn[0][hist_nn[0] > 0]]) \n",
    "    plt.ylim(np.min(all_densities) / 2, np.max(all_densities) * 2)\n",
    "    plt.legend()\n",
    "    plt.xlabel('z-score')\n",
    "    plt.ylabel('Density')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4b3353",
   "metadata": {},
   "source": [
    "# *Figure 1: z-Score Spectra for All Integrations in the File*\n",
    "This plot shows the z-score spectrum for each integration and for both polarizations. This is what we'll use in full_day_rfi_round_2.ipynb to further refine the flagging waterfall. Negative-going excursions near prior flag boundaries are expected: the filter can overfit the noise when it is unconstrained on one side. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef575489",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_zscores()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b2ebf9",
   "metadata": {},
   "source": [
    "# *Figure 2: Histogram of z-Scores*\n",
    "\n",
    "Shows a comparison of the histogram of z-scores in this file (one per polarization) to a Gaussian approximation of what one might expect from thermal noise. Without filtering, the actual distribution is a weighted sum of Rayleigh distributions. Filtering further complicates this, and we approximate the signal loss as a simple fraction of modes filtered, which would be appropriate for white noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b119dcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_zscore_hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1663fe9",
   "metadata": {},
   "source": [
    "## Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16b114c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results as a UVFlag file of waterfall type and metric mode\n",
    "uvd = UVData()\n",
    "uvd.read(SUM_FILE, read_data=False)\n",
    "uvf = UVFlag(uvd, waterfall=True, mode='metric')\n",
    "uvf.select(polarizations=['ee', 'nn'])\n",
    "uvf.history += '\\nProduced by delay_filtered_average_zscore notebook with the following environment:\\n' + '=' * 65 + '\\n' + os.popen('conda env export').read() + '=' * 65\n",
    "for pol in ['ee', 'nn']:\n",
    "    uvf.metric_array[:, :, np.argwhere(uvf.polarization_array == utils.polstr2num(pol, x_orientation=uvf.x_orientation))[0][0]] = zscore[pol]\n",
    "uvf.write(ZSCORE_OUTFILE, clobber=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0c9da9",
   "metadata": {},
   "source": [
    "## Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce5e574",
   "metadata": {},
   "outputs": [],
   "source": [
    "for repo in ['hera_cal', 'hera_qm', 'hera_filters', 'hera_notebook_templates', 'pyuvdata']:\n",
    "    exec(f'from {repo} import __version__')\n",
    "    print(f'{repo}: {__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421828ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Finished execution in {(time.time() - tstart) / 60:.2f} minutes.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
