{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a6c2fe1",
   "metadata": {},
   "source": [
    "# Single File Delay Filtered Average Z-Score\n",
    "\n",
    "**by Josh Dillon and Tyler Cox**, last updated May 1, 2025\n",
    "\n",
    "This notebook is designed to calculate a metric used for finding low-level RFI in redundantly-averaged cross-correlations, which are then incoherently averaged across well-sampled baselines.\n",
    "\n",
    "The actual decision of which times to flag is deferred to another notebook, full_day_rfi_round_2.ipynb\n",
    "\n",
    "Here's a set of links to skip to particular figures and tables:\n",
    "# [• Figure 1: z-Score Spectra for All Integrations in the File](#Figure-1:-z-Score-Spectra-for-All-Integrations-in-the-File)\n",
    "# [• Figure 2: Histogram of z-Scores](#Figure-2:-Histogram-of-z-Scores)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab747439",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "tstart = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d960a410",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HDF5_USE_FILE_LOCKING'] = 'FALSE'\n",
    "import h5py\n",
    "import hdf5plugin  # REQUIRED to have the compression plugins available\n",
    "import numpy as np\n",
    "import copy\n",
    "import glob\n",
    "from hera_cal import io, utils, redcal, apply_cal, datacontainer, vis_clean, noise\n",
    "from hera_filters import dspec\n",
    "from pyuvdata import UVFlag, UVData\n",
    "from scipy import constants\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542e161e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get input data file names\n",
    "SUM_FILE = os.environ.get(\"SUM_FILE\", None)\n",
    "# SUM_FILE = '/lustre/aoc/projects/hera/h6c-analysis/IDR2/2459861/zen.2459861.35453.sum.uvh5'\n",
    "SUM_SUFFIX = os.environ.get(\"SUM_SUFFIX\", 'sum.uvh5')\n",
    "\n",
    "# get input calibration files and flags\n",
    "SMOOTH_CAL_SUFFIX = os.environ.get(\"CAL_SUFFIX\", 'sum.smooth.calfits')\n",
    "SMOOTH_CAL_FILE = SUM_FILE.replace(SUM_SUFFIX, SMOOTH_CAL_SUFFIX)\n",
    "\n",
    "# get output file suffix\n",
    "ZSCORE_SUFFIX =  os.environ.get(\"ZSCORE_SUFFIX\", 'sum.red_avg_zscore.h5')\n",
    "ZSCORE_OUTFILE =  SUM_FILE.replace(SUM_SUFFIX, ZSCORE_SUFFIX)\n",
    "\n",
    "# get delay filtering parameters\n",
    "FM_LOW_FREQ = float(os.environ.get(\"FM_LOW_FREQ\", 87.5)) # in MHz\n",
    "FM_HIGH_FREQ = float(os.environ.get(\"FM_HIGH_FREQ\", 108.0)) # in MHz\n",
    "MIN_SAMP_FRAC = float(os.environ.get(\"MIN_SAMP_FRAC\", .15))\n",
    "FILTER_DELAY = float(os.environ.get(\"FILTER_DELAY\", 750)) # in ns\n",
    "EIGENVAL_CUTOFF = float(os.environ.get(\"EIGENVAL_CUTOFF\", 1e-12))\n",
    "\n",
    "for setting in ['SUM_FILE', 'SMOOTH_CAL_FILE', 'ZSCORE_OUTFILE', 'FM_LOW_FREQ', 'FM_HIGH_FREQ', \n",
    "                'MIN_SAMP_FRAC', 'FILTER_DELAY', 'EIGENVAL_CUTOFF',]:\n",
    "    if isinstance(eval(setting), str):\n",
    "        print(f'{setting} = \"{eval(setting)}\"')\n",
    "    else:\n",
    "        print(f'{setting} = {eval(setting)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee02976c",
   "metadata": {},
   "source": [
    "## Load data, calibrate, and redundantly average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bf4d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load calibration solutions and gain flags\n",
    "t = time.time()\n",
    "hc_smooth = io.HERACal(SMOOTH_CAL_FILE)\n",
    "smooth_gains, cal_flags, _, _ = hc_smooth.read()\n",
    "print(f'Finished loading smoothed calibration file in {(time.time() - t) / 60:.2f} minutes.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d6de68",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'full_day_rfi_round_2' in hc_smooth.history:\n",
    "    raise ValueError('It looks like the pipeline is trying to be re-run midway through. '\n",
    "                     'It is strongly recommended to go back and re-run smooth_cal first to avoid state-dependent results.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bba40b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle the the case where the visibility flags are all True for at least one pol, trying to maintain consistent data shapes\n",
    "ALL_FLAGGED = False\n",
    "if np.all([flag for flag in cal_flags.values()]):\n",
    "    print('This file is entirely flagged.')\n",
    "    ALL_FLAGGED = True\n",
    "else:\n",
    "    for pol in ('Jee', 'Jnn'):\n",
    "        if len([ant for ant, flag in cal_flags.items() if ant[1] == pol and not np.all(flag)]) <= 1:\n",
    "            print(f'Effectively all {pol}-polarized antennas are flagged, so flagging the entire file.')\n",
    "            ALL_FLAGGED = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25e1a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def red_average(reds, data, nsamples, gains, flags={}, cal_flags={}):    \n",
    "    # Redundantly average data\n",
    "    wgts = datacontainer.DataContainer({bl: nsamples[bl] * ~(flags.get(bl, False) | cal_flags.get(utils.split_bl(bl)[0], False) \\\n",
    "                                                             | cal_flags.get(utils.split_bl(bl)[1], False)) for bl in nsamples})\n",
    "    sol = redcal.RedSol(reds, gains=gains)\n",
    "    sol.update_vis_from_data(data, wgts=wgts)\n",
    "    \n",
    "    # Figure out redundantly averaged flags and nsamples\n",
    "    red_avg_flags = {}\n",
    "    red_avg_nsamples = {}\n",
    "    for red in reds:\n",
    "        if red[0] in sol.vis:\n",
    "            red_avg_flags[red[0]] = np.all([wgts[bl] == 0 for bl in red], axis=0) | ~np.isfinite(sol.vis[red[0]])\n",
    "            red_avg_nsamples[red[0]] = np.sum([nsamples[bl] for bl in red if not np.all(wgts[bl] == 0)], axis=0)\n",
    "        else:\n",
    "            # empty placeholders to make sure every file has the same shape for the whole day\n",
    "            sol.vis[red[0]] = np.zeros_like(next(iter(data.values())))\n",
    "            red_avg_flags[red[0]] = np.ones_like(next(iter(flags.values())))\n",
    "            red_avg_nsamples[red[0]] = np.zeros_like(next(iter(nsamples.values())))\n",
    "    sol.make_sol_finite()\n",
    "    \n",
    "    # Build output RedDataContainers \n",
    "    red_avg_data = datacontainer.RedDataContainer(sol.vis, reds)\n",
    "    red_avg_flags = datacontainer.RedDataContainer(red_avg_flags, reds)\n",
    "    red_avg_nsamples = datacontainer.RedDataContainer(red_avg_nsamples, reds)\n",
    "    return red_avg_data, red_avg_flags, red_avg_nsamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a2ff7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not ALL_FLAGGED:\n",
    "    # Load sum and diff data\n",
    "    t = time.time()\n",
    "    hd = io.HERADataFastReader(SUM_FILE)\n",
    "    data, flags, nsamples = hd.read(pols=['ee', 'nn'])\n",
    "    print(f'Finished reading data in {(time.time() - t) / 60:.2f} minutes.')\n",
    "    \n",
    "    # figure out high and low bands\n",
    "    low_band = slice(0, np.argwhere(hd.freqs > FM_LOW_FREQ * 1e6)[0][0])\n",
    "    high_band = slice(np.argwhere(hd.freqs > FM_HIGH_FREQ * 1e6)[0][0], len(hd.freqs))\n",
    "    \n",
    "    # redundantly average\n",
    "    t = time.time()\n",
    "    reds = redcal.get_reds(hd.data_antpos, pols=['ee', 'nn'], include_autos=True, bl_error_tol=2.0)\n",
    "    red_avg_data, red_avg_flags, red_avg_nsamples = red_average(reds, data, nsamples, smooth_gains, flags=flags, cal_flags=cal_flags)\n",
    "    print(f'Finished redundantly averaging data in {(time.time() - t) / 60:.2f} minutes.')\n",
    "\n",
    "    del data, nsamples, flags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3ec0d8",
   "metadata": {},
   "source": [
    "## Delay filter redundantly-averaged SNRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c582d3-78c4-45e7-8ff5-b6bf35c11f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not ALL_FLAGGED:\n",
    "    t = time.time()\n",
    "\n",
    "    # insert auto_bls into red_avg_data for the purpose of calculating noise\n",
    "    all_ants_in_keys = set([ant for bl in red_avg_data.keys() for ant in bl[0:2]])\n",
    "    for pol in ['ee', 'nn']:\n",
    "        auto_bl = [bl for bl in red_avg_data if bl[0] == bl[1] and bl[2] == pol][0]\n",
    "        for ant in all_ants_in_keys:\n",
    "            if (ant, ant, pol) not in red_avg_data:\n",
    "                red_avg_data[(ant, ant, pol)] = red_avg_data[auto_bl]\n",
    "\n",
    "    # predict noise to compute red_avg_SNRs\n",
    "    red_avg_SNRs = copy.deepcopy(red_avg_data)\n",
    "    dt = np.median(np.diff(hd.times)) * 24 * 3600\n",
    "    df = np.median(np.diff(hd.freqs)) \n",
    "    for bl in red_avg_SNRs:\n",
    "        if bl[0] != bl[1]:\n",
    "            noise_var = noise.predict_noise_variance_from_autos(bl, red_avg_data, dt=dt, df=df, nsamples=red_avg_nsamples)\n",
    "            red_avg_SNRs[bl] /= noise_var**.5\n",
    "\n",
    "    # pick out baselines with enough median nsamples and light-travel times shorter than the filter delay\n",
    "    max_nsamples_by_pol = {pol: np.max([np.max(red_avg_nsamples[bl]) for bl in red_avg_nsamples if bl[2] == pol]) for pol in ['ee', 'nn']}\n",
    "    bls_to_filter = [bl for bl in red_avg_data if (np.median(red_avg_nsamples[bl]) >= (max_nsamples_by_pol[bl[2]] * MIN_SAMP_FRAC))]\n",
    "    bls_to_filter = [bl for bl in bls_to_filter if np.linalg.norm(hd.antpos[bl[0]] - hd.antpos[bl[1]]) / constants.c * 1e9 < FILTER_DELAY]\n",
    "    bls_to_filter = [bl for bl in bls_to_filter if bl[0] != bl[1]]\n",
    "    \n",
    "    # perform delay filter\n",
    "    wgts = (~np.all(list(red_avg_flags.values()), axis=0)).astype(float)\n",
    "    cache = {}\n",
    "    dly_filt_SNRs = copy.deepcopy(red_avg_SNRs)\n",
    "    for bl in bls_to_filter:\n",
    "        d_mdl = np.zeros_like(dly_filt_SNRs[bl])\n",
    "        for band in [low_band, high_band]:\n",
    "            d_mdl[:, band], _, info = dspec.fourier_filter(hd.freqs[band], \n",
    "                                                           dly_filt_SNRs[bl][:, band], \n",
    "                                                           wgts=wgts[:, band], filter_centers=[0], \n",
    "                                                           filter_half_widths=[FILTER_DELAY / 1e9], mode='dpss_solve', \n",
    "                                                           eigenval_cutoff=[EIGENVAL_CUTOFF], suppression_factors=[EIGENVAL_CUTOFF], \n",
    "                                                           max_contiguous_edge_flags=len(hd.freqs), cache=cache)\n",
    "        dly_filt_SNRs[bl] = np.where(red_avg_flags[bl], 0, red_avg_SNRs[bl] - d_mdl)\n",
    "\n",
    "    # calculate and apply correction factor based on the leverage to flatten out the SNR\n",
    "    correction_factors = np.full_like(wgts, np.nan)\n",
    "    for band in [low_band, high_band]:\n",
    "        X = dspec.dpss_operator(hd.freqs[band], [0], filter_half_widths=[FILTER_DELAY / 1e9], eigenval_cutoff=[EIGENVAL_CUTOFF])[0]\n",
    "        for tind in range(wgts.shape[0]):\n",
    "            W = wgts[tind, band]\n",
    "            leverage = np.diag(X @ np.linalg.pinv(np.dot(X.T * W, X)) @ (X.T * W))\n",
    "            correction_factors[tind, band] = np.where(leverage > 0, np.sqrt(np.pi)/2 * (1 - leverage)**.5, np.nan)\n",
    "    for bl in dly_filt_SNRs:\n",
    "        dly_filt_SNRs[bl] /= correction_factors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1d3a3a-67d6-45bd-aa36-e0135b4b401d",
   "metadata": {},
   "source": [
    "## Calculate z-scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca3a96e-6413-4ef4-a37b-dc34e73e7aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not ALL_FLAGGED:\n",
    "    zscore = {}\n",
    "    for pol in ['ee', 'nn']:\n",
    "        abs_SNRs_this_pol = [np.abs(dly_filt_SNRs[bl]) for bl in bls_to_filter if (bl[2] == pol) and (bl[0] != bl[1])]\n",
    "\n",
    "        predicted_mean = 1.0\n",
    "        sigma = predicted_mean * np.sqrt(2 / np.pi)\n",
    "        variance_expected = (4 - np.pi) / 2 * sigma**2 / len(abs_SNRs_this_pol)\n",
    "\n",
    "        zscore[pol] = (np.nanmean(abs_SNRs_this_pol, axis=0) - predicted_mean) / variance_expected**.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6f7a49",
   "metadata": {},
   "source": [
    "## Plotting Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec86321",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_zscores():\n",
    "    if ALL_FLAGGED:\n",
    "        print('All integrations are flagged. Nothing to plot.')\n",
    "        return    \n",
    "    \n",
    "    fig, axes = plt.subplots(2, 1, sharey=True, sharex=True, figsize=(12, 6), gridspec_kw={'hspace': 0})\n",
    "    for ax, pol in zip(axes, ['ee', 'nn']):\n",
    "\n",
    "        for i, time in enumerate(hd.times):\n",
    "            ax.plot(hd.freqs / 1e6, zscore[pol][i, :], label=f'JD: {hd.times[i]:.6f}', alpha=.75)\n",
    "        \n",
    "        ax.set_ylabel(f'{pol}-polarized z-score')\n",
    "    axes[0].legend()        \n",
    "    axes[1].set_xlabel('Frequency (MHz)')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7183c226",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_zscore_hist():\n",
    "    if ALL_FLAGGED:\n",
    "        print('All integrations are flagged. Nothing to plot.')\n",
    "        return    \n",
    "    \n",
    "    plt.figure(figsize=(12, 4))\n",
    "    all_abs_z = np.abs(list(zscore.values()))\n",
    "    all_abs_z = all_abs_z[np.isfinite(all_abs_z)]\n",
    "    bins = np.arange(-np.max(all_abs_z) - 1, np.max(all_abs_z) + 1, .1)\n",
    "    hist_ee = plt.hist(np.ravel(zscore['ee']), bins=bins, density=True, label='ee-polarized z-scores', alpha=.5)\n",
    "    hist_nn = plt.hist(np.ravel(zscore['nn']), bins=bins, density=True, label='nn-polarized z-scores', alpha=.5)\n",
    "    plt.plot(bins, (2*np.pi)**-.5 * np.exp(-bins**2 / 2), 'k--', label='Gaussian approximate\\nnoise-only distribution')\n",
    "    plt.yscale('log')\n",
    "    all_densities = np.concatenate([hist_ee[0][hist_ee[0] > 0], hist_nn[0][hist_nn[0] > 0]]) \n",
    "    plt.ylim(np.min(all_densities) / 2, np.max(all_densities) * 2)\n",
    "    plt.legend()\n",
    "    plt.xlabel('z-score')\n",
    "    plt.ylabel('Density')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4b3353",
   "metadata": {},
   "source": [
    "# *Figure 1: z-Score Spectra for All Integrations in the File*\n",
    "This plot shows the z-score spectrum for each integration and for both polarizations. This is what we'll use in full_day_rfi_round_2.ipynb to further refine the flagging waterfall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef575489",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_zscores()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b2ebf9",
   "metadata": {},
   "source": [
    "# *Figure 2: Histogram of z-Scores*\n",
    "\n",
    "Shows a comparison of the histogram of z-scores in this file (one per polarization) to a Gaussian approximation of what one might expect from thermal noise. Without filtering, the actual distribution is a weighted sum of Rayleigh distributions. Filtering further complicates this, and we approximate the signal loss as a simple fraction of modes filtered, which would be appropriate for white noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b119dcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_zscore_hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1663fe9",
   "metadata": {},
   "source": [
    "## Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16b114c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results as a UVFlag file of waterfall type and metric mode\n",
    "t = time.time()\n",
    "uvd = UVData()\n",
    "uvd.read(SUM_FILE, read_data=False)\n",
    "uvf = UVFlag(uvd, waterfall=True, mode='metric')\n",
    "uvf.select(polarizations=['ee', 'nn'])\n",
    "uvf.history += '\\nProduced by delay_filtered_average_zscore notebook with the following environment:\\n' + '=' * 65 + '\\n' + os.popen('conda env export').read() + '=' * 65\n",
    "if ALL_FLAGGED:\n",
    "    uvf.metric_array[:, :, :] = np.nan\n",
    "else:\n",
    "    x_orientation = uvf.telescope.get_x_orientation_from_feeds()\n",
    "    for pol in ['ee', 'nn']:\n",
    "        uvf.metric_array[:, :, np.argwhere(uvf.polarization_array == utils.polstr2num(pol, x_orientation=x_orientation))[0][0]] = zscore[pol]\n",
    "uvf.write(ZSCORE_OUTFILE, clobber=True)\n",
    "print(f'Finished writing z-scores in {(time.time() - t) / 60:.2f} minutes.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0c9da9",
   "metadata": {},
   "source": [
    "## Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce5e574",
   "metadata": {},
   "outputs": [],
   "source": [
    "for repo in ['hera_cal', 'hera_qm', 'hera_filters', 'hera_notebook_templates', 'pyuvdata']:\n",
    "    exec(f'from {repo} import __version__')\n",
    "    print(f'{repo}: {__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421828ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Finished execution in {(time.time() - tstart) / 60:.2f} minutes.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e54a619-bd83-4001-a008-eed4180e2df6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
