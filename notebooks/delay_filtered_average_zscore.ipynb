{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a6c2fe1",
   "metadata": {},
   "source": [
    "# Single File Delay Filtered Average Z-Score\n",
    "\n",
    "**by Josh Dillon**, last updated May 12, 2024\n",
    "\n",
    "This notebook is designed to calculate a metric used for finding low-level RFI in redundantly-averaged cross-correlations, which are then incoherently averaged across well-sampled baselines.\n",
    "\n",
    "The actual decision of which times to flag is deferred to another notebook, full_day_rfi_round_2.ipynb\n",
    "\n",
    "Here's a set of links to skip to particular figures and tables:\n",
    "# [• Figure 1: z-Score Spectra for All Integrations in the File](#Figure-1:-z-Score-Spectra-for-All-Integrations-in-the-File)\n",
    "# [• Figure 2: Histogram of z-Scores](#Figure-2:-Histogram-of-z-Scores)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab747439",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "tstart = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d960a410",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HDF5_USE_FILE_LOCKING'] = 'FALSE'\n",
    "import h5py\n",
    "import hdf5plugin  # REQUIRED to have the compression plugins available\n",
    "import numpy as np\n",
    "import copy\n",
    "import glob\n",
    "from hera_cal import io, utils, redcal, apply_cal, datacontainer, vis_clean\n",
    "from hera_filters import dspec\n",
    "from pyuvdata import UVFlag, UVData\n",
    "from scipy import constants\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542e161e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get input data file names\n",
    "SUM_FILE = os.environ.get(\"SUM_FILE\", None)\n",
    "# SUM_FILE = '/lustre/aoc/projects/hera/h6c-analysis/IDR2/2459861/zen.2459861.59008.sum.uvh5'\n",
    "SUM_SUFFIX = os.environ.get(\"SUM_SUFFIX\", 'sum.uvh5')\n",
    "\n",
    "# get input calibration files and flags\n",
    "SMOOTH_CAL_SUFFIX = os.environ.get(\"CAL_SUFFIX\", 'sum.smooth.calfits')\n",
    "SMOOTH_CAL_FILE = SUM_FILE.replace(SUM_SUFFIX, SMOOTH_CAL_SUFFIX)\n",
    "\n",
    "# get output file suffix\n",
    "ZSCORE_SUFFIX =  os.environ.get(\"ZSCORE_SUFFIX\", 'sum.red_avg_zscore.h5')\n",
    "ZSCORE_OUTFILE =  SUM_FILE.replace(SUM_SUFFIX, ZSCORE_SUFFIX)\n",
    "\n",
    "# get delay filtering parameters\n",
    "FM_LOW_FREQ = float(os.environ.get(\"FM_LOW_FREQ\", 87.5)) # in MHz\n",
    "FM_HIGH_FREQ = float(os.environ.get(\"FM_HIGH_FREQ\", 108.0)) # in MHz\n",
    "MIN_SAMP_FRAC = float(os.environ.get(\"MIN_SAMP_FRAC\", .05))\n",
    "FILTER_DELAY = float(os.environ.get(\"FILTER_DELAY\", 500)) # in ns\n",
    "EIGENVAL_CUTOFF = float(os.environ.get(\"EIGENVAL_CUTOFF\", 1e-12))\n",
    "\n",
    "for setting in ['SUM_FILE', 'SMOOTH_CAL_FILE', 'ZSCORE_OUTFILE', 'FM_LOW_FREQ', 'FM_HIGH_FREQ', \n",
    "                'MIN_SAMP_FRAC', 'FILTER_DELAY', 'EIGENVAL_CUTOFF',]:\n",
    "    if isinstance(eval(setting), str):\n",
    "        print(f'{setting} = \"{eval(setting)}\"')\n",
    "    else:\n",
    "        print(f'{setting} = {eval(setting)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee02976c",
   "metadata": {},
   "source": [
    "## Load data, calibrate, and redundantly average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bf4d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load calibration solutions and gain flags\n",
    "t = time.time()\n",
    "hc_smooth = io.HERACal(SMOOTH_CAL_FILE)\n",
    "smooth_gains, cal_flags, _, _ = hc_smooth.read()\n",
    "print(f'Finished loading smoothed calibration file in {(time.time() - t) / 60:.2f} minutes.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d6de68",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'full_day_rfi_round_2' in hc_smooth.history:\n",
    "    raise ValueError('It looks like the pipeline is trying to be re-run midway through. '\n",
    "                     'It is strongly recommended to go back and re-run smooth_cal first to avoid state-dependent results.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bba40b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle the the case where the visibility flags are all True for at least one pol, trying to maintain consistent data shapes\n",
    "ALL_FLAGGED = False\n",
    "if np.all([flag for flag in cal_flags.values()]):\n",
    "    print('This file is entirely flagged.')\n",
    "    ALL_FLAGGED = True\n",
    "else:\n",
    "    for pol in ('Jee', 'Jnn'):\n",
    "        if len([ant for ant, flag in cal_flags.items() if ant[1] == pol and not np.all(flag)]) <= 1:\n",
    "            print(f'Effectively all {pol}-polarized antennas are flagged, so flagging the entire file.')\n",
    "            ALL_FLAGGED = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25e1a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def red_average(reds, data, nsamples, gains, flags={}, cal_flags={}):    \n",
    "    # Redundantly average data\n",
    "    wgts = datacontainer.DataContainer({bl: nsamples[bl] * ~(flags.get(bl, False) | cal_flags.get(utils.split_bl(bl)[0], False) \\\n",
    "                                                             | cal_flags.get(utils.split_bl(bl)[1], False)) for bl in nsamples})\n",
    "    sol = redcal.RedSol(reds, gains=gains)\n",
    "    sol.update_vis_from_data(data, wgts=wgts)\n",
    "    \n",
    "    # Figure out redundantly averaged flags and nsamples\n",
    "    red_avg_flags = {}\n",
    "    red_avg_nsamples = {}\n",
    "    for red in reds:\n",
    "        if red[0] in sol.vis:\n",
    "            red_avg_flags[red[0]] = np.all([wgts[bl] == 0 for bl in red], axis=0) | ~np.isfinite(sol.vis[red[0]])\n",
    "            red_avg_nsamples[red[0]] = np.sum([nsamples[bl] for bl in red if not np.all(wgts[bl] == 0)], axis=0)\n",
    "        else:\n",
    "            # empty placeholders to make sure every file has the same shape for the whole day\n",
    "            sol.vis[red[0]] = np.zeros_like(next(iter(data.values())))\n",
    "            red_avg_flags[red[0]] = np.ones_like(next(iter(flags.values())))\n",
    "            red_avg_nsamples[red[0]] = np.zeros_like(next(iter(nsamples.values())))\n",
    "    sol.make_sol_finite()\n",
    "    \n",
    "    # Build output RedDataContainers \n",
    "    red_avg_data = datacontainer.RedDataContainer(sol.vis, reds)\n",
    "    red_avg_flags = datacontainer.RedDataContainer(red_avg_flags, reds)\n",
    "    red_avg_nsamples = datacontainer.RedDataContainer(red_avg_nsamples, reds)\n",
    "    return red_avg_data, red_avg_flags, red_avg_nsamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a2ff7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not ALL_FLAGGED:\n",
    "    # Load sum and diff data\n",
    "    t = time.time()\n",
    "    hd = io.HERADataFastReader(SUM_FILE)\n",
    "    data, flags, nsamples = hd.read(pols=['ee', 'nn'])\n",
    "    print(f'Finished reading data in {(time.time() - t) / 60:.2f} minutes.')\n",
    "    \n",
    "    # figure out high and low bands\n",
    "    low_band = slice(0, np.argwhere(hd.freqs > FM_LOW_FREQ * 1e6)[0][0])\n",
    "    high_band = slice(np.argwhere(hd.freqs > FM_HIGH_FREQ * 1e6)[0][0], len(hd.freqs))\n",
    "    \n",
    "    # redundantly average\n",
    "    t = time.time()\n",
    "    reds = redcal.get_reds(hd.data_antpos, pols=['ee', 'nn'], include_autos=True, bl_error_tol=2.0)\n",
    "    red_avg_data, red_avg_flags, red_avg_nsamples = red_average(reds, data, nsamples, smooth_gains, flags=flags, cal_flags=cal_flags)\n",
    "    print(f'Finished redundantly averaging data in {(time.time() - t) / 60:.2f} minutes.')\n",
    "\n",
    "    del data, nsamples, flags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3ec0d8",
   "metadata": {},
   "source": [
    "## Delay filter redundantly-averaged data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d700b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not ALL_FLAGGED:\n",
    "    t = time.time()\n",
    "    # compute weights based on RFI flags and autocorrelations, assumes that nsamples is constant across a spectrum\n",
    "    wgts = {}\n",
    "    for pol in ['ee', 'nn']:\n",
    "        rfi_flags = np.all([red_avg_flags[bl] for bl in red_avg_flags if bl[2] == pol], axis=0)\n",
    "        auto_bl = [bl for bl in red_avg_data if bl[0] == bl[1] and bl[2] == pol][0]\n",
    "        wgts[pol] = np.where(rfi_flags, 0, np.abs(red_avg_data[auto_bl])**-2)\n",
    "        wgts[pol] /= np.nanmean(np.where(rfi_flags, np.nan, wgts[pol]))  # avoid dynamic range issues\n",
    "        wgts[pol][~np.isfinite(wgts[pol])] = 0\n",
    "        \n",
    "    # pick out baselines with enough median nsamples and light-travel times shorter than the filter delay\n",
    "    min_nsamples = np.max([np.max(red_avg_nsamples[bl]) for bl in red_avg_nsamples]) * MIN_SAMP_FRAC\n",
    "    bls_to_filter = [bl for bl in red_avg_data if (np.median(red_avg_nsamples[bl]) >= min_nsamples)]\n",
    "    bls_to_filter = [bl for bl in bls_to_filter if np.linalg.norm(hd.antpos[bl[0]] - hd.antpos[bl[1]]) / constants.c * 1e9 < FILTER_DELAY]\n",
    "\n",
    "    # perform delay filter\n",
    "    cache = {}\n",
    "    dly_filt_red_avg_data = copy.deepcopy(red_avg_data)\n",
    "    for bl in bls_to_filter:\n",
    "        d_mdl = np.zeros_like(dly_filt_red_avg_data[bl])\n",
    "        for band in [low_band, high_band]:\n",
    "            d_mdl[:, band], _, info = dspec.fourier_filter(hd.freqs[band], dly_filt_red_avg_data[bl][:, band], \n",
    "                                                           wgts=wgts[bl[2]][:, band], filter_centers=[0], \n",
    "                                                           filter_half_widths=[FILTER_DELAY / 1e9], mode='dpss_solve', \n",
    "                                                           eigenval_cutoff=[EIGENVAL_CUTOFF], suppression_factors=[EIGENVAL_CUTOFF], \n",
    "                                                           max_contiguous_edge_flags=len(hd.freqs), cache=cache)\n",
    "        dly_filt_red_avg_data[bl] = np.where(red_avg_flags[bl], 0, red_avg_data[bl] - d_mdl)\n",
    "    print(f'Finished delay-filtering in {(time.time() - t) / 60:.2f} minutes.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8035cece",
   "metadata": {},
   "source": [
    "## Calculate z-scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc8c61e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not ALL_FLAGGED:\n",
    "    t = time.time()\n",
    "    # estimate how much signal loss we should expect for white noise\n",
    "    filters_low = dspec.dpss_operator(hd.freqs[low_band], [0], [FILTER_DELAY / 1e9], eigenval_cutoff=[EIGENVAL_CUTOFF])[0]\n",
    "    filter_frac_low = filters_low.shape[1] / filters_low.shape[0]\n",
    "    filters_high = dspec.dpss_operator(hd.freqs[high_band], [0], [FILTER_DELAY / 1e9], eigenval_cutoff=[EIGENVAL_CUTOFF])[0]\n",
    "    filter_frac_high = filters_high.shape[1] / filters_high.shape[0]\n",
    "    \n",
    "    # calculate z-scores\n",
    "    zscore = {}\n",
    "    for pol in ['ee', 'nn']:\n",
    "        to_avg = []\n",
    "        weights = []\n",
    "\n",
    "        for bl in bls_to_filter:\n",
    "            auto_bl = auto_bl = [k for k in red_avg_data if k[0] == k[1] and k[2] == bl[2]][0]\n",
    "            if (bl[2] == pol) and (bl != auto_bl):            \n",
    "                # calcualte predicted variance\n",
    "                dt = np.median(np.diff(hd.times)) * 24 * 3600\n",
    "                df = np.median(np.diff(hd.freqs)) \n",
    "                predicted_variance = (np.abs(red_avg_data[auto_bl]))**2 / red_avg_nsamples[bl] / dt / df \n",
    "                predicted_variance[:, low_band] *= (1 - filter_frac_low)\n",
    "                predicted_variance[:, high_band] *= (1 - filter_frac_high)\n",
    "                predicted_variance[red_avg_flags[bl]] = np.nan\n",
    "\n",
    "                # prep for inverse variance weighting\n",
    "                if np.any(np.isfinite(predicted_variance)):\n",
    "                    weights.append(np.where(np.isfinite(predicted_variance) & np.isfinite(dly_filt_red_avg_data[bl]), predicted_variance**-1, 0))\n",
    "                    to_avg.append(np.where(np.isfinite(predicted_variance) & np.isfinite(dly_filt_red_avg_data[bl]),  np.abs(dly_filt_red_avg_data[bl]), 0))\n",
    "\n",
    "        # Check if one polarization is effectively entirely flagged, and if so, flag the whole file\n",
    "        if len(weights) == 0:\n",
    "            print(f'No unique baselines have enough {pol}-polarized samples to merit filtering. Flagging the entire file.')\n",
    "            ALL_FLAGGED = True\n",
    "            continue\n",
    "        \n",
    "        # perform inverse variance weighred average\n",
    "        Wsum = np.sum(weights, axis=0)**-1\n",
    "        estimator = np.einsum(\"mij,mij->ij\", to_avg, weights) * Wsum\n",
    "\n",
    "        # turn estimator into z-score assuming Rayleigh-distributed data (appropriate for averaging magnitudes of visibilities incoherently)\n",
    "        predicted_mean = np.sum(np.array(weights)**.5, axis=0) * Wsum * (np.pi / 4)**.5\n",
    "        predicted_var = (4 - np.pi) / 4 * Wsum\n",
    "        zscore[pol] = (estimator - predicted_mean) / predicted_var**.5\n",
    "    print(f'Finished computing z-scores in {(time.time() - t) / 60:.2f} minutes.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6f7a49",
   "metadata": {},
   "source": [
    "## Plotting Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec86321",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_zscores():\n",
    "    if ALL_FLAGGED:\n",
    "        print('All integrations are flagged. Nothing to plot.')\n",
    "        return    \n",
    "    \n",
    "    fig, axes = plt.subplots(2, 1, sharey=True, sharex=True, figsize=(12, 6), gridspec_kw={'hspace': 0})\n",
    "    for ax, pol in zip(axes, ['ee', 'nn']):\n",
    "\n",
    "        for i, time in enumerate(hd.times):\n",
    "            ax.plot(hd.freqs / 1e6, zscore[pol][i, :], label=f'JD: {hd.times[i]:.6f}', alpha=.75)\n",
    "        \n",
    "        ax.set_ylabel(f'{pol}-polarized z-score')\n",
    "    axes[0].legend()        \n",
    "    axes[1].set_xlabel('Frequency (MHz)')\n",
    "    plt.tight_layout()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7183c226",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_zscore_hist():\n",
    "    if ALL_FLAGGED:\n",
    "        print('All integrations are flagged. Nothing to plot.')\n",
    "        return    \n",
    "    \n",
    "    plt.figure(figsize=(12, 4))\n",
    "    bins = np.arange(-np.nanmax(np.abs(list(zscore.values()))) - 1, np.nanmax(np.abs(list(zscore.values()))) + 1, .1)\n",
    "    hist_ee = plt.hist(np.ravel(zscore['ee']), bins=bins, density=True, label='ee-polarized z-scores', alpha=.5)\n",
    "    hist_nn = plt.hist(np.ravel(zscore['nn']), bins=bins, density=True, label='nn-polarized z-scores', alpha=.5)\n",
    "    plt.plot(bins, (2*np.pi)**-.5 * np.exp(-bins**2 / 2), 'k--', label='Gaussian approximate\\nnoise-only distribution')\n",
    "    plt.yscale('log')\n",
    "    all_densities = np.concatenate([hist_ee[0][hist_ee[0] > 0], hist_nn[0][hist_nn[0] > 0]]) \n",
    "    plt.ylim(np.min(all_densities) / 2, np.max(all_densities) * 2)\n",
    "    plt.legend()\n",
    "    plt.xlabel('z-score')\n",
    "    plt.ylabel('Density')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4b3353",
   "metadata": {},
   "source": [
    "# *Figure 1: z-Score Spectra for All Integrations in the File*\n",
    "This plot shows the z-score spectrum for each integration and for both polarizations. This is what we'll use in full_day_rfi_round_2.ipynb to further refine the flagging waterfall. Negative-going excursions near prior flag boundaries are expected: the filter can overfit the noise when it is unconstrained on one side. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef575489",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_zscores()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b2ebf9",
   "metadata": {},
   "source": [
    "# *Figure 2: Histogram of z-Scores*\n",
    "\n",
    "Shows a comparison of the histogram of z-scores in this file (one per polarization) to a Gaussian approximation of what one might expect from thermal noise. Without filtering, the actual distribution is a weighted sum of Rayleigh distributions. Filtering further complicates this, and we approximate the signal loss as a simple fraction of modes filtered, which would be appropriate for white noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b119dcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_zscore_hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1663fe9",
   "metadata": {},
   "source": [
    "## Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16b114c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results as a UVFlag file of waterfall type and metric mode\n",
    "t = time.time()\n",
    "uvd = UVData()\n",
    "uvd.read(SUM_FILE, read_data=False)\n",
    "uvf = UVFlag(uvd, waterfall=True, mode='metric')\n",
    "uvf.select(polarizations=['ee', 'nn'])\n",
    "uvf.history += '\\nProduced by delay_filtered_average_zscore notebook with the following environment:\\n' + '=' * 65 + '\\n' + os.popen('conda env export').read() + '=' * 65\n",
    "if ALL_FLAGGED:\n",
    "    uvf.metric_array[:, :, :] = np.nan\n",
    "else:\n",
    "    for pol in ['ee', 'nn']:\n",
    "        uvf.metric_array[:, :, np.argwhere(uvf.polarization_array == utils.polstr2num(pol, x_orientation=uvf.x_orientation))[0][0]] = zscore[pol]\n",
    "uvf.write(ZSCORE_OUTFILE, clobber=True)\n",
    "print(f'Finished writing z-scores in {(time.time() - t) / 60:.2f} minutes.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0c9da9",
   "metadata": {},
   "source": [
    "## Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce5e574",
   "metadata": {},
   "outputs": [],
   "source": [
    "for repo in ['hera_cal', 'hera_qm', 'hera_filters', 'hera_notebook_templates', 'pyuvdata']:\n",
    "    exec(f'from {repo} import __version__')\n",
    "    print(f'{repo}: {__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421828ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Finished execution in {(time.time() - tstart) / 60:.2f} minutes.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
