{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0883faa6-98cc-486f-9ab3-3fa9dbc87666",
   "metadata": {},
   "source": [
    "# Power Spectrum Summary\n",
    "\n",
    "**by Josh Dillon**, last updated October 17, 2024\n",
    "\n",
    "The purpose of this notebook is to pull together results from power spectra from single, redundantly-averaged baselines (typically cross-power spectra from interleaved sets of times) as produced by the [Single Baseline Filtering and Power Spectrum Estimation\n",
    "notebook](https://github.com/HERA-Team/hera_notebook_templates/blob/master/notebooks/single_baseline_postprocessing_and_pspec.ipynb). It is supposed to be roughly comparable to [a similar notebook from H1C](https://github.com/HERA-Team/H1C_IDR3_Power_Spectra/blob/main/SPOILERS/All_Epochs_Power_Spectra/H1C_IDR3_Power_Spectra.ipynb).\n",
    "\n",
    "# [• Figure 1: P(k) Averaged Over Baseline vs. LST](#Figure-1:-P(k)-Averaged-Over-Baseline-vs.-LST)\n",
    "# [• Figure 2: Per-Baseline, Time-Averaged High Delay Average SNR](#Figure-2:-Per-Baseline,-Time-Averaged-High-Delay-Average-SNR)\n",
    "# [• Figure 3: Histograms of Time-Averaged High-Delay SNRs](#Figure-3:-Histograms-of-Time-Averaged-High-Delay-SNRs)\n",
    "# [• Figure 4: Time-Averaged Cylindrical P(k)](#Figure-4:-Time-Averaged-Cylindrical-P(k))\n",
    "# [• Figure 5: Time-Averaged Cylindrical SNR](#Figure-5:-Time-Averaged-Cylindrical-SNR)\n",
    "# [• Figure 6: Spherically-Averaged $\\Delta^2$](#Figure-6:-Spherically-Averaged-%24%5CDelta%5E2%24)\n",
    "# [• Table 1: Power Spectra, Error Bars, and Upper Limits](#Table-1:-Power-Spectra,-Error-Bars,-and-Upper-Limits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44e5b26-a0f2-4cd4-bc61-802f83ac457f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "tstart = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29051700-db27-4b1f-901d-1694dfc20bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HDF5_USE_FILE_LOCKING'] = 'FALSE'\n",
    "import h5py\n",
    "import hdf5plugin  # REQUIRED to have the compression plugins available\n",
    "import numpy as np\n",
    "import glob\n",
    "import copy\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from hera_cal import io, utils\n",
    "import hera_pspec as hp\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from scipy import constants\n",
    "from IPython.core.display import display, HTML\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff985df0-2524-4474-b0b7-1e61b03d323a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: remove\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c398cc6d-a3b3-4b87-a5f4-92b48cdc5fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data settings\n",
    "PSPEC_FOLDER = os.environ.get(\"PSPEC_FOLDER\", '') # folder in which to look for *.PSPEC_SUFFIX files\n",
    "# PSPEC_FOLDER = '/lustre/aoc/projects/hera/h6c-analysis/IDR2/pspec/redavg-smoothcal-inpaint-500ns-lstcal-14bands'\n",
    "PSPEC_SUFFIX = os.environ.get(\"PSPEC_SUFFIX\", \"pspec.h5\")\n",
    "\n",
    "COMBINED_PSPEC_FILE = os.environ.get(\"COMBINED_PSPEC_FILE\", '') \n",
    "# COMBINED_PSPEC_FILE = '/lustre/aoc/projects/hera/jsdillon/H6C/PSPEC/pspec_out/all_baselines_interleaved_IDR2.3_500ns_14band.pspec.h5'\n",
    "COHERENT_AVG_CORRECTION_FACTOR_FILE = os.environ.get(\"COHERENT_AVG_CORRECTION_FACTOR_FILE\", '') \n",
    "# COHERENT_AVG_CORRECTION_FACTOR_FILE = '/lustre/aoc/projects/hera/jsdillon/H6C/PSPEC/pspec_out/all_baselines_interleaved_IDR2.3_500ns_14band.coherent_avg_correction_factor.p'\n",
    "RELOAD_PSPEC = os.environ.get(\"RELOAD_PSPEC\", \"TRUE\").upper() == \"TRUE\"\n",
    "RELOAD_PSPEC = False\n",
    "\n",
    "RESULTS_FOLDER = os.environ.get(\"RESULTS_FOLDER\", '') \n",
    "# RESULTS_FOLDER = '/lustre/aoc/projects/hera/jsdillon/H6C/PSPEC/pspec_out/500ns_14band_results/'\n",
    "\n",
    "BANDS_TO_USE = [int(band) for band in os.environ.get(\"BANDS_TO_USE\", \"1,2,3,5,6,9,10,13\").split(\",\")] # 1 indexed\n",
    "\n",
    "WEDGE_BUFFER_NS = float(os.environ.get(\"WEDGE_BUFFER_NS\", 300))\n",
    "\n",
    "LST_RANGE_HOURS = [float(lst) for lst in os.environ.get(\"LST_RANGE_HOURS\", \"1.5,5.5\").split(\",\")]\n",
    "if LST_RANGE_HOURS[0] > LST_RANGE_HOURS[-1]:\n",
    "    LST_RANGE_HOURS[0] -= 24"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70906d6a-bc47-4265-b8b9-39bc6aaaecca",
   "metadata": {},
   "source": [
    "## Load Power Spectra and Apply Bias Corrections\n",
    "\n",
    "**TODO:**\n",
    "* Ensure that when Mike's updated FRF errorbars are added, the correction factor for time-averaged power spectra is correctly propagated: dpss_coherent_avg_corrections\n",
    "* Calculate signal loss due to redundant averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0771a6df-fd4a-4472-a8f3-7389455c4374",
   "metadata": {},
   "outputs": [],
   "source": [
    "pspec_files = sorted(glob.glob(os.path.join(PSPEC_FOLDER, '*.' + PSPEC_SUFFIX)))\n",
    "print(f\"Found {len(pspec_files)} *.{PSPEC_SUFFIX} in files starting with {pspec_files[0]}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea6584f-c24a-444e-a902-685d82f76220",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_correction(uvp, total_bias=None, data_bias=None):\n",
    "    \"\"\"\n",
    "    Apply correction factors to power spectra. Operates inplace.\n",
    "\n",
    "    Args:\n",
    "        uvp : UVPSpec object\n",
    "        total_bias : dict\n",
    "            bias correction to data and errors, e.g. abscal bias\n",
    "            keys are spw integers, values are correction scalars\n",
    "        data_bias : dict\n",
    "            bias correction only to data, e.g. fringe-rate filtering\n",
    "            keys are spw integers, values are correction scalars\n",
    "    \"\"\"\n",
    "    for spw in uvp.spw_array:\n",
    "        if total_bias is not None:\n",
    "            uvp.data_array[spw] *= total_bias[spw]\n",
    "            if hasattr(uvp, 'cov_array_real'):\n",
    "                uvp.cov_array_real[spw] *= total_bias[spw]**2\n",
    "                uvp.cov_array_imag[spw] *= total_bias[spw]**2\n",
    "            if hasattr(uvp, 'stats_array'):\n",
    "                for stat in uvp.stats_array:\n",
    "                    uvp.stats_array[stat][spw] *= total_bias[spw]\n",
    "                    # TODO: this is right for P_N but not quite right for P_SN (though I'm not sure how much we care)\n",
    "        if data_bias is not None:\n",
    "            uvp.data_array[spw] *= data_bias[spw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afd123b-445b-4053-a074-29de1237c88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RELOAD_PSPEC or not os.path.exists(COMBINED_PSPEC_FILE):\n",
    "    uvp_by_blp = {}\n",
    "    dpss_coherent_avg_corrections = {}\n",
    "    frf_losses = {}\n",
    "    \n",
    "    for df in tqdm(pspec_files):\n",
    "        # load power spectra\n",
    "        psc = hp.container.PSpecContainer(df, mode='r', keep_open=False)\n",
    "        uvp = psc.get_pspec('stokespol', 'interleave_averaged') # TODO: GENERALIZE\n",
    "        blp = uvp.get_blpairs()[0]    \n",
    "        uvp_by_blp[blp] = uvp\n",
    "    \n",
    "        # load additional metadata stored in header\n",
    "        with h5py.File(df, 'r') as f:\n",
    "            dpss_coherent_avg_corrections[blp] = f['header'].attrs['dpss_coherent_avg_corrections']\n",
    "            # TODO: ensure that that this factor is \n",
    "            frf_losses[blp] = f['header'].attrs['frf_losses']\n",
    "        \n",
    "        # apply FRF loss bias correction\n",
    "        bias_correction(uvp_by_blp[blp], data_bias={spw: (1.0 - loss)**-1 for spw, loss in enumerate(frf_losses[blp])})\n",
    "\n",
    "    # write dpss_coherent_avg_corrections to a pickle\n",
    "    with open(COHERENT_AVG_CORRECTION_FACTOR_FILE, 'wb') as f:\n",
    "        pickle.dump(dpss_coherent_avg_corrections, f)\n",
    "    \n",
    "    uvp = hp.uvpspec.recursive_combine_uvpspec(list(uvp_by_blp.values()))\n",
    "    psc = hp.PSpecContainer(COMBINED_PSPEC_FILE, mode='rw', keep_open=False)\n",
    "    psc.set_pspec('stokespol', 'all_baselines', uvp, overwrite=True) \n",
    "else:\n",
    "    # read previously calculated results\n",
    "    psc = hp.PSpecContainer(COMBINED_PSPEC_FILE, mode='rw', keep_open=False)\n",
    "    uvp = psc.get_pspec('stokespol', 'all_baselines')\n",
    "    with open(COHERENT_AVG_CORRECTION_FACTOR_FILE, 'rb') as f:\n",
    "        dpss_coherent_avg_corrections = pickle.load(f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e29897-6ed6-48e1-b95e-b92c95748665",
   "metadata": {},
   "outputs": [],
   "source": [
    "dlst = np.median(np.diff(uvp.lst_avg_array[uvp.key_to_indices((0, ((0, 1), (0, 1)), ('pI', 'pI')))[1]]))\n",
    "all_lsts = np.unique(uvp.lst_avg_array)\n",
    "all_lsts[all_lsts * 12 / np.pi > 17.75] -= 2 * np.pi\n",
    "lst_grid = np.arange(np.min(all_lsts) - dlst, np.max(all_lsts) + dlst, dlst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c1365f-3a6d-41a2-bfa6-9450db9b42e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a subset of bands to analyze. From now on, bands will be re-numbered\n",
    "uvp.select(spws=[b - 1 for b in BANDS_TO_USE])\n",
    "# select pI and pQ (ignore pU and pV, which are experimental)\n",
    "uvp.select(polpairs=[('pI', 'pI'), ('pQ', 'pQ')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e4292d-8050-4e3d-88d3-30d120852c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict to map baseline pairs to baseline vectors\n",
    "blp_to_blvec_dict = {blp: blpv for blp, blpv in zip(uvp.get_blpairs(), uvp.get_blpair_blvecs())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a8ff01-2700-4258-a3d6-d0c3e72011ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get mean redshifts for each spw\n",
    "zs = np.array([np.mean(1.420405751e9 / uvp.freq_array[uvp.spw_freq_array == spw] - 1) for spw in uvp.spw_array])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76306d9-5ba2-4d34-ac52-83251bb18d58",
   "metadata": {},
   "source": [
    "## Examine LST structure of the power spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef68c7f-7abb-492f-8878-df46d3833664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build dictionary mapping an individual baseline's LST range to the wider set of LSTs\n",
    "blp_to_lst_indices = {}\n",
    "for key in uvp.get_all_keys():\n",
    "    if not key[1] in blp_to_lst_indices:\n",
    "        lsts = uvp.lst_avg_array[uvp.key_to_indices(key)[1]]\n",
    "        lsts[lsts * 12 / np.pi > 17.75] -= 2 * np.pi    \n",
    "        blp_to_lst_indices[key[1]] = np.searchsorted(lst_grid, lsts[0]) + np.arange(len(lsts), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c32a2b-1fea-4e64-95e6-cd9a2adf0327",
   "metadata": {},
   "outputs": [],
   "source": [
    "wgts_sum = {spw: np.zeros((len(lst_grid), len(uvp.get_dlys(spw)))) for spw in uvp.spw_array}\n",
    "pI_sum = {spw: np.zeros((len(lst_grid), len(uvp.get_dlys(spw))), dtype=complex) for spw in uvp.spw_array}\n",
    "pQ_sum = {spw: np.zeros((len(lst_grid), len(uvp.get_dlys(spw))), dtype=complex) for spw in uvp.spw_array}\n",
    "\n",
    "# average power spectra over baselines\n",
    "for key in tqdm(list(uvp.get_all_keys())):\n",
    "    spw, blp, pp = key\n",
    "    if pp != ('pI', 'pI'):\n",
    "        continue\n",
    "\n",
    "    # inverse variance weight baselines\n",
    "    wgt = np.abs(uvp.get_stats('P_N', key))**-2\n",
    "    \n",
    "    wedge_plus_buffer = np.linalg.norm(np.linalg.norm(blp_to_blvec_dict[key[1]])) / constants.c + WEDGE_BUFFER_NS * 1e-9\n",
    "    wgt[:, np.abs(uvp.get_dlys(spw)) < wedge_plus_buffer] = 0\n",
    "    pI_sum[spw][blp_to_lst_indices[blp], :] += uvp.get_data(key) * wgt\n",
    "    pQ_sum[spw][blp_to_lst_indices[blp], :] += uvp.get_data((spw, blp, ('pQ', 'pQ'))) * wgt\n",
    "    wgts_sum[spw][blp_to_lst_indices[blp], :] += wgt\n",
    "\n",
    "# normalize average\n",
    "for spw in pI_sum:\n",
    "    pI_sum[spw] /= wgts_sum[spw]\n",
    "    pQ_sum[spw] /= wgts_sum[spw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87fc989-04a2-4569-b321-8796d6942451",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_power_spectrum_avg_vs_LST():\n",
    "    fig, axes = plt.subplots(len(uvp.spw_array), 2, figsize=(8, len(uvp.spw_array) * 2), sharey=True, sharex=True, gridspec_kw={'wspace': 0, 'hspace': 0})\n",
    "    \n",
    "    for spw, ax in enumerate(axes):\n",
    "        dlys = uvp.get_dlys(spw) * 1e9\n",
    "        \n",
    "        im = ax[0].imshow(pI_sum[spw].real, aspect='auto', interpolation='none', cmap='turbo', norm=matplotlib.colors.LogNorm(vmin=1e5, vmax=1e13), \n",
    "                          extent=[dlys[0], dlys[-1], lst_grid[-1] * 12 / np.pi, lst_grid[0] * 12 / np.pi])\n",
    "    \n",
    "        im = ax[1].imshow(pQ_sum[spw].real, aspect='auto', interpolation='none', cmap='turbo', norm=matplotlib.colors.LogNorm(vmin=1e5, vmax=1e13), \n",
    "                          extent=[dlys[0], dlys[-1], lst_grid[-1] * 12 / np.pi, lst_grid[0] * 12 / np.pi])\n",
    "    \n",
    "        \n",
    "        for a in ax:\n",
    "            a.set_xticks([-3000, -1500, 0, 1500, 3000])\n",
    "            if spw == len(uvp.spw_array) - 1:\n",
    "                a.set_xlabel('Delay (ns)')\n",
    "        ax[0].set_yticks(ax[0].get_yticks())\n",
    "        ax[0].set_yticklabels(['',] + list(ax[0].get_yticks() % 24)[1:])            \n",
    "        ax[0].set_ylabel('LST (hours)')\n",
    "    \n",
    "        for a, pol in zip(ax, ['pI', 'pQ']):\n",
    "            a.text(a.get_xlim()[0] + 300, a.get_ylim()[-1] + .4, f'{pol} Band {spw + 1}\\nz = {zs[spw]:.1f}', ha='left', va='top',\n",
    "                     bbox=dict(facecolor='w', edgecolor='black', alpha=.75, boxstyle='round', ls='-'))\n",
    "            a.tick_params(axis='x', direction='in')\n",
    "    \n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.colorbar(im, ax=axes, pad=.02, aspect=40, extend='both', location='top', label=f'Baselined-Averaged Outside Wedge + {WEDGE_BUFFER_NS} ns' + 'Re[$P(k)$] (mK$^2$ $h^{-3}$ Mpc$^3$)')\n",
    "    plt.savefig(os.path.join(RESULTS_FOLDER, 'baseline_avg_vs_LST_all_bands_2pol.pdf'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf6611a-ed6b-4d86-8258-c24cb272e38f",
   "metadata": {},
   "source": [
    "# Figure 1: P(k) Averaged Over Baseline vs. LST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2643d0e3-92a7-435e-b82a-61d517ef970a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_power_spectrum_avg_vs_LST()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f00476-9211-48a4-ae64-8a4e66a2f85c",
   "metadata": {},
   "source": [
    "## Compute time-averaged statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a9f9a3-ab82-4604-a1ad-8f683345efd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pefrom time average in range set by LST_RANGE_HOURS\n",
    "lst_subset = uvp.lst_avg_array[(uvp.lst_avg_array >= LST_RANGE_HOURS[0] * np.pi / 12) & (uvp.lst_avg_array <= LST_RANGE_HOURS[1] * np.pi / 12)]\n",
    "uvp_tavg = uvp.select(lsts=lst_subset, polpairs=[('pI', 'pI')], inplace=False)\n",
    "uvp_tavg.average_spectra(time_avg=True, error_weights='P_N', error_field=['P_SN'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f0bf0d-0789-49ca-b137-534cf15f554f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct noise statistics by the coherent average correction factor, which accounts for the FRF\n",
    "for key in uvp_tavg.get_all_keys():\n",
    "    uvp_tavg.set_stats('P_N', key, uvp_tavg.get_stats('P_N', key) * dpss_coherent_avg_corrections[key[1]][key[0]]**.5)\n",
    "    uvp_tavg.set_stats('P_SN', key, uvp_tavg.get_stats('P_SN', key) * dpss_coherent_avg_corrections[key[1]][key[0]]**.5)\n",
    "    # TODO: Jianrong to figure out whether P_SN should be treated differently here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fbe19f-579b-4649-9c29-604a2f6da8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_baseline_SNR():\n",
    "    # Look for individual baselines with high SNR outside the wedge, a sign of a particular failure mode\n",
    "    low_dly = 1000\n",
    "    high_dly = 2000\n",
    "    fig, axes = plt.subplots(int(np.ceil(uvp.Nspws / 2)),2, figsize=(12, 12), sharex=True, sharey=True, gridspec_kw={'wspace': 0, 'hspace':0})\n",
    "    \n",
    "    for spw, ax in enumerate(axes.ravel()):\n",
    "        if spw == len(uvp.spw_array):\n",
    "            break\n",
    "        bl_vecs = {}\n",
    "        pk_avgs = {}\n",
    "        snr_avgs = {}\n",
    "        dlys = uvp_tavg.get_dlys(spw) * 1e9\n",
    "        dlys_to_use = (low_dly <= np.abs(dlys)) & (np.abs(dlys) <= high_dly)\n",
    "        for key in uvp_tavg.get_all_keys():\n",
    "            _spw, blp, pp = key\n",
    "            if _spw != spw: \n",
    "                continue\n",
    "            if pp[0] != 'pI':\n",
    "                continue\n",
    "            blv = blp_to_blvec_dict[blp]\n",
    "            if blv[1] < 0:\n",
    "                bl_vecs[blp] = -blv\n",
    "            else:\n",
    "                bl_vecs[blp] = blv\n",
    "            pk_avgs[blp] = np.mean(uvp_tavg.get_data(key)[0, dlys_to_use].real)\n",
    "            snr_avgs[blp] = pk_avgs[blp] / np.mean(uvp_tavg.get_stats('P_N', key)[0, dlys_to_use].real)\n",
    "            \n",
    "        if spw % 2 == 0:\n",
    "            ax.set_ylabel('NS Baseline Component (m)')\n",
    "        if spw >= int(np.ceil(uvp.Nspws / 2)) - 2:\n",
    "            ax.set_xlabel('EW Baseline Component (m)')\n",
    "        \n",
    "        blps = list(bl_vecs.keys())\n",
    "        im = ax.scatter([bl_vecs[blp][0] for blp in blps], [bl_vecs[blp][1] for blp in blps], c=[snr_avgs[blp] for blp in blps], \n",
    "                        s=20, cmap='bwr', vmin=-2, vmax=2)\n",
    "    \n",
    "        ax.text(ax.get_xlim()[0] + 10, ax.get_ylim()[-1] - 10, f'Band {spw + 1}\\nz = {zs[spw]:.1f}', ha='left', va='top',\n",
    "                         bbox=dict(facecolor='w', edgecolor='black', alpha=.75, boxstyle='round', ls='-'))\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.colorbar(im, ax=axes, pad=.02, aspect=40, extend='both', location='top', label=f'Average $P(k) / P_N$ Between {low_dly} and {high_dly} ns')\n",
    "    plt.savefig(os.path.join(RESULTS_FOLDER, 'per_baseline_SNR_all_bands.pdf'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f556bdf-9281-41bb-bd8c-46ab1cfc7e89",
   "metadata": {},
   "source": [
    "# Figure 2: Per-Baseline, Time-Averaged High Delay Average SNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1051fc6f-7834-4a5e-b685-5c711e6382a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_baseline_SNR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf60ebf-367f-4ab3-9555-03e9d5510eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_SNR_histograms():\n",
    "    # show the distribution of time-averaged power spectra (before spherical averaging)\n",
    "    low_dly = 1000\n",
    "    high_dly = 2000\n",
    "    fig, axes = plt.subplots(int(np.ceil(uvp.Nspws / 2)), 2, figsize=(12, 12), sharex=True, sharey=True, gridspec_kw={'wspace': 0, 'hspace':0})\n",
    "    \n",
    "    for spw, ax in enumerate(axes.ravel()):\n",
    "        if spw == len(uvp.spw_array):\n",
    "            break\n",
    "        to_hist = []\n",
    "        dlys = uvp_tavg.get_dlys(spw) * 1e9\n",
    "        dlys_to_use = (low_dly <= np.abs(dlys)) & (np.abs(dlys) <= high_dly)\n",
    "        for key in uvp_tavg.get_all_keys():\n",
    "            _spw, blp, pp = key\n",
    "            if _spw != spw: \n",
    "                continue\n",
    "            if pp[0] != 'pI':\n",
    "                continue\n",
    "            SNRs = uvp_tavg.get_data(key)[0, dlys_to_use] / uvp_tavg.get_stats('P_N', key)[0, dlys_to_use].real\n",
    "            to_hist.extend(SNRs)\n",
    "    \n",
    "        bins = np.linspace(-10,10,101)        \n",
    "        \n",
    "        for func, c, zorder in zip([np.real, np.imag], ['C0', 'C1'], [2, 1]):\n",
    "            ax.hist(func(to_hist), bins=bins, density=True, color=c, edgecolor='k', linewidth=.1, alpha=.5, zorder=zorder,\n",
    "                    label=f'{\"Re\" if func == np.real else \"Im\"}[$P(k)$] / $P_N$ ({low_dly}—{high_dly} ns)')\n",
    "        ax.set_yscale('log')\n",
    "        ax.set_ylim([2e-5, 1e0])\n",
    "        \n",
    "        gauss = np.exp(-bins**2/2) / np.sqrt(2*np.pi)\n",
    "        ax.plot(bins, gauss, 'k--', label='Gaussian Distribution')\n",
    "    \n",
    "        ax.text(ax.get_xlim()[0] + .6, ax.get_ylim()[-1] / 1.8, f'Band {spw + 1}\\nz = {zs[spw]:.1f}', ha='left', va='top',\n",
    "                bbox=dict(facecolor='w', edgecolor='black', alpha=.75, boxstyle='round', ls='-'))\n",
    "        if spw % 2 == 0:\n",
    "            ax.set_ylabel('Probability Density')\n",
    "        if spw >= int(np.ceil(uvp.Nspws / 2)) - 2:\n",
    "            ax.set_xlabel('SNR')\n",
    "    \n",
    "        if spw == 0:\n",
    "            fig.legend(loc='upper center', ncol=3, fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    fig.subplots_adjust(top=0.95)\n",
    "    \n",
    "    plt.savefig(os.path.join(RESULTS_FOLDER, 'SNR_histogram_all_bands.pdf'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045db541-798c-4174-b898-6d28d21ed43e",
   "metadata": {},
   "source": [
    "# Figure 3: Histograms of Time-Averaged High-Delay SNRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10e741a-fa6e-44dc-9d4c-b2b604735a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_SNR_histograms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f502ef7a-184d-4b50-a481-9b019af1f78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cylindrical_Pk(SNR=False):\n",
    "    # TODO: update to show k_|| and k_perp\n",
    "    # TODO: update to show wedge buffer\n",
    "    fig, axes = plt.subplots(int(np.ceil(uvp.Nspws / 2)),2, figsize=(12, 12), \n",
    "                             sharex=True, sharey=True, gridspec_kw={'wspace': 0, 'hspace':0})\n",
    "\n",
    "    uvp_here = copy.deepcopy(uvp_tavg)\n",
    "    \n",
    "    for spw, ax in enumerate(axes.ravel()):\n",
    "        if SNR:\n",
    "            uvp_here.data_array[spw] /= uvp_here.stats_array['P_N'][spw].real\n",
    "            extra_kwargs = dict(cmap='bwr', vmin=-2, vmax=2)\n",
    "        else:\n",
    "            extra_kwargs = dict(cmap='turbo', error_weights='P_N')\n",
    "        \n",
    "        hp.plot.delay_wedge(uvp_here, spw, ('pI', 'pI'), rotate=True, ax=ax, fold=True, log10=False, \n",
    "                            component='real',  horizon_lines=True, **extra_kwargs)\n",
    "\n",
    "        cax = ax.collections[0]\n",
    "        if not SNR:\n",
    "            cax.set_norm(matplotlib.colors.LogNorm(vmin=1e2, vmax=1e14))\n",
    "\n",
    "        if spw % 2 == 0:\n",
    "            ax.set_ylabel('$\\\\tau$ (ns)', fontsize=10)\n",
    "        else:\n",
    "            ax.set_ylabel('')\n",
    "        if uvp.Nspws - spw <= 2:\n",
    "            ax.set_xlabel('$|\\\\vec{b}|$ (m)', fontsize=10)\n",
    "        else:\n",
    "            ax.set_xlabel('')\n",
    "        ax.text(ax.get_xlim()[0] + 7, ax.get_ylim()[-1] - 200, f'Band {spw + 1}\\nz = {zs[spw]:.1f}', ha='left', va='top',\n",
    "                     bbox=dict(facecolor='w', edgecolor='black', alpha=.75, boxstyle='round', ls='-'))\n",
    "        ax.tick_params(axis='both', direction='in')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    cbar = fig.colorbar(cax, ax=axes, pad=.02, aspect=40, extend='both', location='top',\n",
    "                        label=(r\"$\\text{Re}[P(k_\\parallel, k_\\perp)]\\text{ }/\\text{ }P_N(k_\\parallel, k_\\perp)$\" if SNR\n",
    "                               else r\"$\\text{Re}[P(k_\\parallel, k_\\perp)]\\ [{\\rm mK}^2\\ h^{-3}\\ {\\rm Mpc}^3]$\"))\n",
    "    plt.savefig(os.path.join(RESULTS_FOLDER, (f'cylindrical_{\"SNR\" if SNR else \"Pk\"}_all_bands.pdf')), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1727342-717e-4acc-b0c0-c156b757bc0a",
   "metadata": {},
   "source": [
    "# Figure 4: Time-Averaged Cylindrical P(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e53ea0-51d0-4fbd-82ff-190383ee57d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cylindrical_Pk()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64786842-f69f-48cc-888d-ef78e76d3e01",
   "metadata": {},
   "source": [
    "# Figure 5: Time-Averaged Cylindrical SNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ab2cde-766c-4a80-9a51-dc90f100c886",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cylindrical_Pk(SNR=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b477cde3-f92a-48b5-84c3-71b64a870baf",
   "metadata": {},
   "source": [
    "## Produce $\\Delta^2$ Limits\n",
    "\n",
    "**Major TODOs:** \n",
    "* add window functions\n",
    "* proper noise covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e846df60-8e23-4b53-9b0d-154d75d58c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spherical Binning Parameters\n",
    "dk_multiplier = 2.0 # the size of each spherical k bin (Delta k) when multiplied by the natural k_para spacing\n",
    "k_start_multiplier = .75 # the center of first spherical k bin in units of Delta k\n",
    "\n",
    "# make a copy of the time-averaged UVPspec object, where we can by-hand remove the baseline dependence of the time/lst arrays\n",
    "uvp_tavg_copy = copy.deepcopy(uvp_tavg)\n",
    "for time_array in [uvp_tavg_copy.time_avg_array, uvp_tavg_copy.time_1_array, uvp_tavg_copy.time_2_array, \n",
    "                   uvp_tavg_copy.lst_avg_array, uvp_tavg_copy.lst_1_array, uvp_tavg_copy.lst_2_array]:\n",
    "    time_array[:] = np.median(time_array)\n",
    "uvp_tavg_copy.average_spectra(time_avg=True, error_weights='P_N', error_field=['P_SN'], inplace=True)\n",
    "uvp_tavg_copy.set_stats_slice('P_N', 1e9 / constants.c, WEDGE_BUFFER_NS, above=False, val=np.inf)\n",
    "\n",
    "# Perform spherical averaging\n",
    "sph_avgs = []\n",
    "for spw in tqdm(uvp.spw_array):\n",
    "    dk = dk_multiplier * np.median(np.diff(uvp_tavg_copy.get_kparas(spw)))\n",
    "    kbins = np.arange(k_start_multiplier * dk, 2.5, dk) # even spacing \n",
    "    uvp_tavg_this_spw = uvp_tavg_copy.select(spws=[spw], inplace=False)\n",
    "    sph_avgs.append(hp.grouping.spherical_average(uvp_tavg_this_spw, kbins, dk, error_weights='P_N'))\n",
    "\n",
    "delta_sqs = [sph_avg.convert_to_deltasq(inplace=False) for sph_avg in sph_avgs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5289bf-4aa4-4e2d-b703-200183c8ff24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_deltasq_limits():\n",
    "    fig, axes = plt.subplots(2, int(np.ceil(uvp.Nspws / 2)), figsize=(12, 8), sharey='row', sharex=True, gridspec_kw={'wspace': 0, 'hspace': 0})\n",
    "    \n",
    "    # Adjust layout to make room for the legend\n",
    "    plt.subplots_adjust(top=0.95)\n",
    "    \n",
    "    # Loop over each subplot\n",
    "    for spw, ax in enumerate(axes.ravel()):\n",
    "    \n",
    "        if spw >= uvp.Nspws / 2:\n",
    "            ax.set_xlabel('$k$ ($h$Mpc$^{-1}$)')\n",
    "        if spw % int(np.ceil(uvp.Nspws / 2)) == 0:\n",
    "            ax.set_ylabel('$\\\\Delta^2$ (mK$^2$)')\n",
    "        \n",
    "        if spw == len(sph_avgs):\n",
    "            break\n",
    "    \n",
    "        # get Delta^2 and error bars\n",
    "        z = zs[spw]\n",
    "        k = delta_sqs[spw].get_kparas(0)\n",
    "        key = delta_sqs[spw].get_all_keys()[0]\n",
    "        Dsq = np.squeeze(delta_sqs[spw].get_data(key)).real\n",
    "        Dsq[Dsq < 0] = 0\n",
    "        pn_error = np.squeeze(delta_sqs[spw].get_stats('P_N', key))\n",
    "        psn_error = np.squeeze(delta_sqs[spw].get_stats('P_SN', key))\n",
    "        pn_error = np.where(pn_error > 1e20, np.inf, pn_error)\n",
    "        psn_error = np.where(psn_error > 1e20, np.inf, psn_error)\n",
    "        \n",
    "        # Plot with error bars and lines\n",
    "        ax.errorbar(k, Dsq, yerr=(2 * psn_error), marker='o', ms=6, ls='', c='deeppink', label='1$\\\\sigma$ Noise Level')\n",
    "        ax.plot(k, pn_error, c='k', ls='--', lw=3, label='PRELIMINARY Power Spectrum with 2$\\\\sigma$ Error Bars')\n",
    "        ax.set_yscale('log')\n",
    "        ax.set_ylim([1, ax.get_ylim()[1]])\n",
    "        ax.grid()\n",
    "        ax.text(ax.get_xlim()[-1] - .1, ax.get_ylim()[0]*3, f'Band {spw+1}\\nz = {z:.1f}', ha='right', va='bottom',\n",
    "                bbox=dict(facecolor='w', edgecolor='black', alpha=.75, boxstyle='round', ls='-'))\n",
    "    \n",
    "        # Add Phase I's two best limits at the most-comparable redshift\n",
    "        if zs[np.argmin(np.abs(zs - 7.9))] == z:\n",
    "            ax.errorbar([.34], [44], yerr=[2 * 206], marker='*', ms=10, lw=2, ls='', c='g', label='HERA Phase I Best Limits at Similar z')\n",
    "            # Create a custom legend with labels\n",
    "            handles, labels = axes.ravel()[spw].get_legend_handles_labels()\n",
    "            fig.legend(handles, [labels[1], labels[0], labels[2]] , loc='upper center', ncol=3, fontsize=10)\n",
    "        if zs[np.argmin(np.abs(zs - 10.4))] == z:\n",
    "            ax.errorbar([.36], [0], yerr=[2 * 1748], marker='*', ms=10, lw=2, ls='', c='g')\n",
    "    \n",
    "    plt.savefig(os.path.join(RESULTS_FOLDER, 'delta_sq_all_bands.pdf'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039ff996-0299-4ac1-a8be-d99d237f3a45",
   "metadata": {},
   "source": [
    "# Figure 6: Spherically-Averaged $\\Delta^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fb1d0e-6ef3-4de7-8adc-80f0cff54774",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_deltasq_limits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1a6522-87d5-4974-af39-aedf218a7e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_upper_limits_table():\n",
    "    for spw in uvp.spw_array:\n",
    "        # get Delta^2 and error bars\n",
    "        z = zs[spw]\n",
    "        k = delta_sqs[spw].get_kparas(0)\n",
    "        key = delta_sqs[spw].get_all_keys()[0]\n",
    "        deltasq = np.squeeze(delta_sqs[spw].get_data(key)).real\n",
    "        deltasq_err = np.squeeze(delta_sqs[spw].get_stats('P_SN', key)).real # TODO: this isn't quite what we did in H1C\n",
    "        deltasq_ul = np.array([dsq if dsq > 0 else 0 for dsq in deltasq]) + 2 * deltasq_err\n",
    "        to_use = (deltasq_err > 0) & (deltasq_err < 1e20)\n",
    "        if len(deltasq_ul[to_use]) == 0:\n",
    "            continue\n",
    "    \n",
    "        table = {'k': k[to_use],\n",
    "                 '$\\Delta^{2}(k)$ (mK$^2$)': deltasq[to_use],\n",
    "                 '$1\\sigma$ (mK$^2$)': deltasq_err[to_use],\n",
    "                 '$\\Delta^{2}_{UL}$ (mK$^2$)': deltasq_ul[to_use]}\n",
    "        df = pd.DataFrame(table)\n",
    "    \n",
    "        def css_border(x):\n",
    "            return [\"border-left: 1px solid black\" if (i%3==1) else \"border: 0px\" for i, col in enumerate(x)]\n",
    "        def scientific_html_formatter(x):\n",
    "            formatted = \"{:.2e}\".format(x)  # Convert to scientific format like 8.58e+05\n",
    "            base, exponent = formatted.split(\"e\")  # Split into base and exponent\n",
    "            exponent = int(exponent)  # Convert exponent to an integer\n",
    "            return f\"{base} × 10<sup>{exponent}</sup>\"  # Format with HTML superscript\n",
    "    \n",
    "        display(HTML(f'<h3>Band {spw+1} (z = {zs[spw]:.1f}):</h3>'))\n",
    "        \n",
    "        to_display = df.style.format({'k': \"{:,.2f}\"} | {col: scientific_html_formatter for col in df.columns[1:]}) \\\n",
    "                              .apply(css_border, axis=1) \\\n",
    "                              .set_properties(width='100px') \\\n",
    "                              .hide(axis='index')\n",
    "        \n",
    "        display(to_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360f9f08-ff2c-4fb8-b1d4-c29df0c79a0a",
   "metadata": {},
   "source": [
    "# Table 1: Power Spectra, Error Bars, and Upper Limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c7c581-3f7c-4254-a56f-8626d6505b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_upper_limits_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b884e497-5eca-4c8c-8e2a-0b9f19ce5aa4",
   "metadata": {},
   "source": [
    "## Export Limits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c29ca5-2a6b-4f07-89e9-c33d64bf3925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results are saved to h5 files per-band and per-field\n",
    "for spw, delta_sq in enumerate(delta_sqs):\n",
    "    outfilename = os.path.join(RESULTS_FOLDER, f'Deltasq_Band_{spw+1}.h5')\n",
    "    delta_sq.write_hdf5(outfilename, overwrite=True)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010a9fc7-a899-4486-9936-eed8eebd9c7b",
   "metadata": {},
   "source": [
    "## Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2017137a-4f4e-469e-b4b7-69f911aa9e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "for repo in ['numpy', 'scipy', 'astropy', 'hera_cal', 'hera_qm', 'pandas',\n",
    "             'hera_filters', 'hera_pspec', 'hera_notebook_templates', 'pyuvdata']:\n",
    "    exec(f'from {repo} import __version__')\n",
    "    print(f'{repo}: {__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db6e118-9045-4619-9c72-bc4ed13c4939",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Finished execution in {(time.time() - tstart) / 60:.2f} minutes.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34b9dc8-117f-49b9-8814-fa4e8424aff5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
