{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "525f6877",
   "metadata": {},
   "source": [
    "# Single File Post-Processing\n",
    "\n",
    "**by Josh Dillon**, last updated August 5, 2025\n",
    "\n",
    "This notebook, the final step in a pre-LST-binning analysis, applies calibration solutions and flags to both sum and diff data, producing a variety of data products. These currently may include:\n",
    "\n",
    "* Abs-calibrated, redundantly-averaged visibility sums\n",
    "* Abs-calibrated, redundantly-averaged visibility diffs\n",
    "* Smooth-calibrated, redundantly-averaged visibility sums\n",
    "* Smooth-calibrated, redundantly-averaged visibility diffs\n",
    "* Abs-calibrated, redundantly-averaged, delay-filtered visibility sums\n",
    "* Abs-calibrated, redundantly-averaged, delay-filtered visibility diffs\n",
    "* Smooth-calibrated, redundantly-averaged, delay-filtered visibility sums\n",
    "* Smooth-calibrated, redundantly-averaged, delay-filtered visibility diffs\n",
    "\n",
    "* Smooth-calibrated, coherently redundantly-averaged, incoherently array-averaged visibility magnitudes\n",
    "    * This is done for all visibilities, just autos, and just cross-correlations and is designed to be lightweight data product for transient searches\n",
    "\n",
    "Some of these data products are experimental and may be removed at a later date to save compute and disk.  Some may be turned off using environment variables.\n",
    "\n",
    "Here's a set of links to skip to particular figures and tables:\n",
    "# [• Figure 1: Redundant Averaging of a Single Baseline Group After `smooth_cal`](#Figure-1:-Redundant-Averaging-of-a-Single-Baseline-Group-After-smooth_cal)\n",
    "# [• Figure 2: Number of Redundantly-Averaged Samples as a Function of Baseline](#Figure-2:-Number-of-Redundantly-Averaged-Samples-as-a-Function-of-Baseline)\n",
    "# [• Figure 3: Delay-Filtered Visibility Delay Spectra after `smooth_cal` and Redundant-Averaging](#Figure-3:-Delay-Filtered-Visibility-Delay-Spectra-after-smooth_cal-and-Redundant-Averaging)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4882c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "tstart = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612b6de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HDF5_USE_FILE_LOCKING'] = 'FALSE'\n",
    "import h5py\n",
    "import hdf5plugin  # REQUIRED to have the compression plugins available\n",
    "import numpy as np\n",
    "import copy\n",
    "import pickle\n",
    "import glob\n",
    "import re\n",
    "from pyuvdata import UVFlag, UVData\n",
    "from hera_cal import io, utils, redcal, apply_cal, datacontainer, vis_clean\n",
    "from hera_filters import dspec\n",
    "from hera_qm.metrics_io import read_a_priori_ant_flags\n",
    "from hera_qm.time_series_metrics import true_stretches\n",
    "from scipy import constants, signal\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5a0b7f",
   "metadata": {},
   "source": [
    "## Load data and calibration solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02b9a03",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# figure out whether to save results (and which ones)\n",
    "SAVE_RESULTS = os.environ.get(\"SAVE_RESULTS\", \"TRUE\").upper() == \"TRUE\"\n",
    "SAVE_DIFF_RED_AVG = os.environ.get(\"SAVE_DIFF_RED_AVG\", \"TRUE\").upper() == \"TRUE\"\n",
    "SAVE_ABS_CAL_RED_AVG = os.environ.get(\"SAVE_ABS_CAL_RED_AVG\", \"TRUE\").upper() == \"TRUE\"\n",
    "SAVE_DLY_FILT_RED_AVG = os.environ.get(\"SAVE_DLY_FILT_RED_AVG\", \"TRUE\").upper() == \"TRUE\"\n",
    "SAVE_CROSS_POLS = os.environ.get(\"SAVE_CROSS_POLS\", \"FALSE\").upper() == \"TRUE\"\n",
    "# TODO: in theory, if some of these are turned off, some memory and/or processing could be saved \n",
    "# by doing only a subset of what this notebook does. That's left for future work.\n",
    "\n",
    "# Output a subset of calibrated and flagged but non-redundantly averaged baselines \n",
    "DO_BL_SELECT = os.environ.get(\"DO_BL_SELECT\", 'FALSE').upper() == 'TRUE'\n",
    "BL_SELECT_DECIMATION_FACTOR = int(os.environ.get(\"BL_SELECT_DECIMATION_FACTOR\", 30))\n",
    "BL_SELECT_NGROUP = int(os.environ.get(\"BL_SELECT_NGROUP\", 30))\n",
    "\n",
    "add_to_history = 'Produced by file_prostprocessing notebook with the following environment:\\n' + '=' * 65 + '\\n' + os.popen('conda env export').read() + '=' * 65\n",
    "for setting in ['SAVE_RESULTS', 'SAVE_DIFF_RED_AVG', 'SAVE_ABS_CAL_RED_AVG', 'SAVE_DLY_FILT_RED_AVG', 'SAVE_CROSS_POLS',\n",
    "                'DO_BL_SELECT', 'BL_SELECT_DECIMATION_FACTOR',  'BL_SELECT_NGROUP']:\n",
    "    print(f'{setting} = {eval(setting)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3265dc3a",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# get input data file names\n",
    "SUM_FILE = os.environ.get(\"SUM_FILE\", None)\n",
    "# SUM_FILE = '/lustre/aoc/projects/hera/h6c-analysis/IDR3/2459861/zen.2459861.46123.sum.uvh5'\n",
    "SUM_SUFFIX = os.environ.get(\"SUM_SUFFIX\", 'sum.uvh5')\n",
    "DIFF_SUFFIX = os.environ.get(\"DIFF_SUFFIX\", 'diff.uvh5')\n",
    "\n",
    "# get input calibration files and flags\n",
    "ABS_CAL_SUFFIX = os.environ.get(\"ABS_CAL_SUFFIX\", 'sum.omni.calfits')\n",
    "SMOOTH_CAL_SUFFIX = os.environ.get(\"CAL_SUFFIX\", 'sum.smooth.calfits')\n",
    "APOSTERIORI_YAML_SUFFIX = os.environ.get(\"APOSTERIORI_YAML_SUFFIX\", '_aposteriori_flags.yaml')\n",
    "aposteriori_yaml_file = os.path.join(os.path.dirname(SUM_FILE), SUM_FILE.split('.')[-4] + APOSTERIORI_YAML_SUFFIX)\n",
    "\n",
    "# Get filter cache name\n",
    "FILTER_CACHE = os.environ.get(\"FILTER_CACHE\", \"filter_cache\")\n",
    "filter_cache_dir = os.path.join(os.path.dirname(SUM_FILE), FILTER_CACHE)\n",
    "\n",
    "# Get output reds pickle name for storing the actual reds used in the redundant average\n",
    "REDS_PICKLE_SUFFIX = os.environ.get(\"REDS_PICKLE_SUFFIX\", 'reds_used.p')\n",
    "\n",
    "# output abs-calibrated, redundantly-averaged files\n",
    "SUM_ABS_CAL_RED_AVG_SUFFIX = os.environ.get(\"SUM_ABS_CAL_RED_AVG_SUFFIX\", 'sum.abs_calibrated.red_avg.uvh5')\n",
    "DIFF_ABS_CAL_RED_AVG_SUFFIX = os.environ.get(\"DIFF_ABS_CAL_RED_AVG_SUFFIX\", 'diff.abs_calibrated.red_avg.uvh5')\n",
    "\n",
    "# output smooth-calibrated, redundantly-averaged files\n",
    "SUM_SMOOTH_CAL_RED_AVG_SUFFIX = os.environ.get(\"SUM_SMOOTH_CAL_RED_AVG_SUFFIX\", 'sum.smooth_calibrated.red_avg.uvh5')\n",
    "DIFF_SMOOTH_CAL_RED_AVG_SUFFIX = os.environ.get(\"DIFF_SMOOTH_CAL_RED_AVG_SUFFIX\", 'diff.smooth_calibrated.red_avg.uvh5')\n",
    "\n",
    "# output abs-calibrated, redundantly-averaged, delay-filtered files\n",
    "SUM_ABS_CAL_RED_AVG_DLY_FILT_SUFFIX = os.environ.get(\"SUM_ABS_CAL_RED_AVG_DLY_FILT_SUFFIX\", 'sum.abs_calibrated.red_avg.dly_filt.uvh5')\n",
    "DIFF_ABS_CAL_RED_AVG_DLY_FILT_SUFFIX = os.environ.get(\"DIFF_ABS_CAL_RED_AVG_DLY_FILT_SUFFIX\", 'diff.abs_calibrated.red_avg.dly_filt.uvh5')\n",
    "\n",
    "# output smooth-calibrated, redundantly-averaged, delay-filtered files\n",
    "SUM_SMOOTH_CAL_RED_AVG_DLY_FILT_SUFFIX = os.environ.get(\"SUM_SMOOTH_CAL_RED_AVG_DLY_FILT_SUFFIX\", 'sum.smooth_calibrated.red_avg.dly_filt.uvh5')\n",
    "DIFF_SMOOTH_CAL_RED_AVG_DLY_FILT_SUFFIX = os.environ.get(\"DIFF_SMOOTH_CAL_RED_AVG_DLY_FILT_SUFFIX\", 'diff.smooth_calibrated.red_avg.dly_filt.uvh5')\n",
    "\n",
    "# output smooth-calibrated, averaged absolute value of visibilities\n",
    "AVG_ABS_ALL_SUFFIX = os.environ.get(\"AVG_ABS_SUFFIX\", 'sum.smooth_calibrated.avg_abs_all.uvh5')\n",
    "AVG_ABS_AUTO_SUFFIX = os.environ.get(\"AVG_ABS_AUTO_SUFFIX\", 'sum.smooth_calibrated.avg_abs_auto.uvh5')\n",
    "AVG_ABS_CROSS_SUFFIX = os.environ.get(\"AVG_ABS_CROSS_SUFFIX\", 'sum.smooth_calibrated.avg_abs_cross.uvh5')\n",
    "\n",
    "# output a subset of non-redundantly averaged baselines \n",
    "BL_SELECT_SUFFIX = os.environ.get(\"BL_SELECT_SUFFIX\", 'sum.smooth_calibrated.bl_select.uvh5')\n",
    "\n",
    "for suffix in ['DIFF_SUFFIX', 'ABS_CAL_SUFFIX', 'SMOOTH_CAL_SUFFIX', 'REDS_PICKLE_SUFFIX',\n",
    "               'SUM_ABS_CAL_RED_AVG_SUFFIX', 'DIFF_ABS_CAL_RED_AVG_SUFFIX',\n",
    "               'SUM_SMOOTH_CAL_RED_AVG_SUFFIX', 'DIFF_SMOOTH_CAL_RED_AVG_SUFFIX',\n",
    "               'SUM_ABS_CAL_RED_AVG_DLY_FILT_SUFFIX', 'DIFF_ABS_CAL_RED_AVG_DLY_FILT_SUFFIX', \n",
    "               'SUM_SMOOTH_CAL_RED_AVG_DLY_FILT_SUFFIX', 'DIFF_SMOOTH_CAL_RED_AVG_DLY_FILT_SUFFIX',\n",
    "               'AVG_ABS_ALL_SUFFIX', 'AVG_ABS_AUTO_SUFFIX', 'AVG_ABS_CROSS_SUFFIX', \"BL_SELECT_SUFFIX\"]:\n",
    "    file_var = suffix.replace('SUFFIX', 'FILE')\n",
    "    exec(f'{file_var} = SUM_FILE.replace(SUM_SUFFIX, {suffix})')\n",
    "    print(f\"{file_var} = '{eval(file_var)}'\")\n",
    "print(f'aposteriori_yaml_file = {aposteriori_yaml_file}')\n",
    "print(f'filter_cache = {filter_cache_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9ecd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure out if this file will produced baseline-selected calibrated outfile\n",
    "if DO_BL_SELECT:\n",
    "    all_sum_files = sorted(glob.glob(os.path.join(os.path.dirname(SUM_FILE), re.sub(r'\\d+', '*', os.path.basename(SUM_FILE)))))\n",
    "    valid_files = all_sum_files[::BL_SELECT_DECIMATION_FACTOR]\n",
    "    if SUM_FILE not in valid_files:\n",
    "        DO_BL_SELECT = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7489bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse delay filtering settings\n",
    "DLY_FILT_HORIZON = float(os.environ.get(\"DLY_FILT_HORIZON\", 1.0))\n",
    "DLY_FILT_STANDOFF = float(os.environ.get(\"DLY_FILT_STANDOFF\", 0.0)) # in ns\n",
    "DLY_FILT_MIN_DLY = float(os.environ.get(\"DLY_FILT_MIN_DLY\", 150.0)) # in ns\n",
    "DLY_FILT_EIGENVAL_CUTOFF = float(os.environ.get(\"DLY_FILT_EIGENVAL_CUTOFF\", 1e-12))\n",
    "FM_LOW_FREQ = float(os.environ.get(\"FM_LOW_FREQ\", 87.5)) # in MHz\n",
    "FM_HIGH_FREQ = float(os.environ.get(\"FM_HIGH_FREQ\", 108.0)) # in MHz\n",
    "for setting in ['DLY_FILT_HORIZON', 'DLY_FILT_STANDOFF', 'DLY_FILT_MIN_DLY', 'DLY_FILT_EIGENVAL_CUTOFF', 'FM_LOW_FREQ', 'FM_HIGH_FREQ']:\n",
    "        print(f'{setting} = {eval(setting)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa4e3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all cached matrices within the filter cache\n",
    "cache_files = glob.glob(os.path.join(filter_cache_dir, \"*.filter_cache.p\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfa06e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_FLAGGED = False\n",
    "pols = ['nn', 'ee', 'ne', 'en'] if SAVE_CROSS_POLS else ['ee', 'nn']\n",
    "\n",
    "def _empty_data(hd):\n",
    "    bls_in_data = [bl for bl in hd.bls if bl[2] in pols]\n",
    "    data = datacontainer.DataContainer({bl: np.zeros((len(hd.times), len(hd.freqs)), dtype=complex) for bl in bls_in_data})\n",
    "    flags = datacontainer.DataContainer({bl: np.ones((len(hd.times), len(hd.freqs)), dtype=bool) for bl in bls_in_data})\n",
    "    nsamples = datacontainer.DataContainer({bl: np.ones((len(hd.times), len(hd.freqs)), dtype=float) for bl in bls_in_data})\n",
    "    return data, flags, nsamples\n",
    "\n",
    "# Load sum and diff data\n",
    "hd = io.HERADataFastReader(SUM_FILE)\n",
    "try:\n",
    "    data, flags, nsamples = hd.read(pols=pols)\n",
    "except KeyError:\n",
    "    # there's a problem with one of the data/flags/nsamples fields\n",
    "    ALL_FLAGGED = True\n",
    "    data, flags, nsamples = _empty_data(hd)\n",
    "\n",
    "if SAVE_DIFF_RED_AVG:\n",
    "    hd_diff = io.HERADataFastReader(DIFF_FILE)\n",
    "    try:\n",
    "        diff_data, diff_flags, diff_nsamples = hd_diff.read(pols=pols)\n",
    "    except KeyError:\n",
    "        ALL_FLAGGED = True\n",
    "        diff_data, diff_flags, diff_nsamples = _empty_data(hd_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54cf5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick all reds that might be in the data, using the same set across polarizations for easier output\n",
    "reds = redcal.get_reds(hd.data_antpos, pols=pols, include_autos=True, bl_error_tol=2.0)\n",
    "ex_ants = set(read_a_priori_ant_flags(aposteriori_yaml_file))\n",
    "possibly_unflagged_bls = [bl for bl in data if utils.split_bl(bl)[0] not in ex_ants and utils.split_bl(bl)[1] not in ex_ants]\n",
    "possibly_unflagged_antpairs = set([ap for bl in possibly_unflagged_bls for ap in [bl[:2], bl[-2::-1]]])\n",
    "reds = [red for red in reds if np.any([bl[0:2] in possibly_unflagged_antpairs for bl in red])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371f0733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load calibration solutions and gain flags\n",
    "hc_smooth = io.HERACal(SMOOTH_CAL_FILE)\n",
    "smooth_gains, cal_flags, _, _ = hc_smooth.read()\n",
    "pol_convention = hc_smooth.pol_convention\n",
    "gain_scale = hc_smooth.gain_scale\n",
    "\n",
    "hc_abs = io.HERACal(ABS_CAL_FILE)\n",
    "abs_gains, abs_cal_flags, _, _ = hc_abs.read()\n",
    "assert pol_convention == hc_abs.pol_convention, f'{pol_convention} != {hc_abs.pol_convention}'\n",
    "assert gain_scale == hc_abs.gain_scale, f'{gain_scale} != {hc_abs.gain_scale}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022fdd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle the the case where the smooth_cal flags are all True, trying to maintain consistent data shapes\n",
    "if ALL_FLAGGED or np.all([flag for flag in cal_flags.values()]):\n",
    "    print('This file is entirely flagged. Proceeding with averaging and filtering using earlier flags, '\n",
    "          'but the output data products will still be fully flagged.')\n",
    "    ALL_FLAGGED = True\n",
    "    \n",
    "    # Likely the file was fully flagged for broadband RFI, so instead use the original flags from file_calibration\n",
    "    cal_flags = abs_cal_flags\n",
    "    # And if that didn't work, just make the flags all False for now (though ex_ants will still be exluded via reds)\n",
    "    if np.all([flag for flag in cal_flags.values()]):\n",
    "        cal_flags = {ant: np.zeros_like(cal_flags[ant]) for ant in cal_flags}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7dfa62",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def red_average(reds, data, nsamples, gains, flags={}, cal_flags={}):    \n",
    "    # Redundantly average data\n",
    "    wgts = datacontainer.DataContainer({bl: nsamples[bl] * ~(flags.get(bl, False) | cal_flags.get(utils.split_bl(bl)[0], False) \\\n",
    "                                                             | cal_flags.get(utils.split_bl(bl)[1], False)) for bl in nsamples})\n",
    "    sol = redcal.RedSol(reds, gains=gains)\n",
    "    sol.update_vis_from_data(data, wgts=wgts)\n",
    "    \n",
    "    # Figure out redundantly averaged flags and nsamples\n",
    "    red_avg_flags = {}\n",
    "    red_avg_nsamples = {}\n",
    "    for red in reds:\n",
    "        if red[0] in sol.vis:\n",
    "            red_avg_flags[red[0]] = np.all([wgts[bl] == 0 for bl in red], axis=0) | ~np.isfinite(sol.vis[red[0]])\n",
    "            red_avg_nsamples[red[0]] = np.sum([nsamples[bl] for bl in red if not np.all(wgts[bl] == 0)], axis=0)\n",
    "        else:\n",
    "            # empty placeholders to make sure every file has the same shape for the whole day\n",
    "            sol.vis[red[0]] = np.zeros_like(next(iter(data.values())))\n",
    "            red_avg_flags[red[0]] = np.ones_like(next(iter(flags.values())))\n",
    "            red_avg_nsamples[red[0]] = np.zeros_like(next(iter(nsamples.values())))\n",
    "    sol.make_sol_finite()\n",
    "    \n",
    "    # Build output RedDataContainers \n",
    "    red_avg_data = datacontainer.RedDataContainer(sol.vis, reds)\n",
    "    red_avg_flags = datacontainer.RedDataContainer(red_avg_flags, reds)\n",
    "    red_avg_nsamples = datacontainer.RedDataContainer(red_avg_nsamples, reds)\n",
    "    return red_avg_data, red_avg_flags, red_avg_nsamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d289b4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform redundant averaging\n",
    "red_avg_smooth_cal_sum_data, red_avg_flags, red_avg_nsamples = red_average(reds, data, nsamples, smooth_gains, flags=flags, cal_flags=cal_flags)\n",
    "red_avg_abs_cal_sum_data, _, _ = red_average(reds, data, nsamples, abs_gains, flags=flags, cal_flags=cal_flags)\n",
    "if SAVE_DIFF_RED_AVG:\n",
    "    red_avg_smooth_cal_diff_data, _, _ = red_average(reds, diff_data, diff_nsamples, smooth_gains, flags=diff_flags, cal_flags=cal_flags)\n",
    "    red_avg_abs_cal_diff_data, _, _ = red_average(reds, diff_data, nsamples, abs_gains, flags=flags, cal_flags=cal_flags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bc8cf5-7527-4b5e-8463-cfc3294c723a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the actual reds used for this redundant average\n",
    "flagged_bls = [bl for bl in flags if np.all(flags[bl])]\n",
    "flagged_ants = [ant for ant in cal_flags if np.all(cal_flags[ant])]\n",
    "reds_used_here = redcal.filter_reds(reds, ex_bls=flagged_bls, ex_ants=flagged_ants)\n",
    "if SAVE_RESULTS:\n",
    "    with open(REDS_PICKLE_FILE, 'wb') as reds_pickle_file:\n",
    "        pickle.dump(reds_used_here, reds_pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52db6d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not ALL_FLAGGED:\n",
    "    integration_flags = np.all([red_avg_flags[bl] for bl in red_avg_flags], axis=(0, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b7c8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_red_avg_vis(pols=['ee', 'nn']):\n",
    "    if ALL_FLAGGED:\n",
    "        print('All integrations are flagged. Nothing to plot.')\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 6), dpi=100, sharex='col', sharey='row', gridspec_kw={'hspace': 0, 'wspace': 0})\n",
    "    for i, pol in enumerate(pols):\n",
    "\n",
    "        reds_here = redcal.filter_reds(reds, pols=[pol], antpos=hd.antpos, min_bl_cut=1)\n",
    "        sol = redcal.RedSol(reds_here, gains=smooth_gains)\n",
    "        red = sorted(reds_here, key=lambda red: np.median(red_avg_nsamples.get(red[0], 0)), reverse=True)[0]\n",
    "        \n",
    "        if np.any([bl[2] == pol for bl in red_avg_flags]):\n",
    "            for tind in range(red_avg_flags[red[0]].shape[0]):\n",
    "                if not np.all(red_avg_flags[red[0]][tind]):\n",
    "                    break\n",
    "\n",
    "            calibrated = {bl: np.where(flags[bl] | cal_flags[utils.split_bl(bl)[0]] | cal_flags[utils.split_bl(bl)[1]], \n",
    "                                       np.nan, sol.calibrate_bl(bl, data[bl])) for bl in red if bl in data}\n",
    "            for bl in red:\n",
    "                axes[0, i].plot(hd.freqs/1e6, np.angle(calibrated[bl][tind]), alpha=.5, lw=.5)\n",
    "                axes[1, i].semilogy(hd.freqs/1e6, np.abs(calibrated[bl][tind]), alpha=.5, lw=.5)\n",
    "\n",
    "            to_plot = np.where(red_avg_flags[red[0]][tind], np.nan, red_avg_smooth_cal_sum_data[red[0]][tind])\n",
    "            axes[0, i].plot(hd.freqs / 1e6, np.angle(to_plot), lw=1, c='k')\n",
    "            axes[1, i].semilogy(hd.freqs / 1e6, np.abs(to_plot), lw=1, c='k', label=f'Baseline Group:\\n{(int(red[0][0]), int(red[0][1]), red[0][2])}')\n",
    "            axes[1, i].set_xlabel('Frequency (MHz)')\n",
    "            axes[1, i].legend(loc='upper right')\n",
    "    axes[0, 0].set_ylabel('Visibility Phase (radians)')\n",
    "    axes[1, 0].set_ylabel('Visibility Amplitude (Jy)')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0aa39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_red_avg_nsamples():\n",
    "    if ALL_FLAGGED:\n",
    "        print('All integrations are flagged. Nothing to plot.')\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 1, figsize=(14,7), dpi=100, sharex=True, gridspec_kw={'hspace': 0})\n",
    "    max_nsamples = np.max([np.median(ns) for ns in red_avg_nsamples.values()])\n",
    "    for ax, pol in zip(axes, ['ee', 'nn']):\n",
    "        if np.any([bl[2] == pol for bl in red_avg_nsamples]):\n",
    "            blvecs = np.array([hd.antpos[red[0][1]] - hd.antpos[red[0][0]] for red in reds if red[0] in red_avg_nsamples and red[0][2] == pol])\n",
    "            med_nsamples = np.array([np.median(red_avg_nsamples[red[0]][~integration_flags, :]) \n",
    "                                     for red in reds if red[0] in red_avg_nsamples and red[0][2] == pol])\n",
    "            sca = ax.scatter(blvecs[:, 0], blvecs[:, 1], s=0)\n",
    "            sca = ax.scatter(blvecs[:, 0], blvecs[:, 1], s=(100 * (600 / np.diff(ax.get_xlim())[0])**2), ec='k', linewidths=.5,\n",
    "                             c=med_nsamples, norm=matplotlib.colors.LogNorm(vmin=1, vmax=max_nsamples), cmap='turbo')\n",
    "            ax.axis('equal')\n",
    "            ax.set_xlabel('EW Baseline Vector (m)')\n",
    "            ax.set_ylabel('NS Baseline Vector (m)')\n",
    "        ax.text(.98, .94, f'{pol}-polarized', transform=ax.transAxes, va='top', ha='right', bbox=dict(facecolor='w', alpha=0.5))           \n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig.colorbar(sca, ax=axes, pad=.02, label='Number of Samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8a08d0",
   "metadata": {},
   "source": [
    "# *Figure 1: Redundant Averaging of a Single Baseline Group After `smooth_cal`*\n",
    "\n",
    "The results of calibration and redundant averaging the baseline group with the highest redundancy in each polarization. Visibilities with flagged antennas are excluded. The black line is the redundantly-averaged visibility. Each thin colored line is a different baseline group. Phases are shown in the top row, amplitudes in the bottom, ee-polarized visibilities in the left column, and nn-polarized visibilities in the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d39056",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_red_avg_vis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6103f2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_red_avg_vis(['en', 'ne'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c3f26e",
   "metadata": {},
   "source": [
    "# *Figure 2: Number of Redundantly-Averaged Samples as a Function of Baseline*\n",
    "\n",
    "The number of visibilities averaged together in each baseline group. Note that the split of the HERA core produces more highly-sampled intra-sector baselines that are interspersed with the less-highly-sampled inter-sector baselines off the main grid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ca7d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_red_avg_nsamples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25675fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save memory\n",
    "ALL_CAL_FLAGS = np.all([np.all(~cal_flags[ant]) for ant in cal_flags])\n",
    "del hd, data, flags, nsamples\n",
    "if SAVE_DIFF_RED_AVG:\n",
    "    del hd_diff, diff_data, diff_flags, diff_nsamples\n",
    "del hc_smooth, hc_abs, abs_gains, abs_cal_flags\n",
    "if not DO_BL_SELECT:\n",
    "    del smooth_gains, cal_flags\n",
    "hd = io.HERADataFastReader(SUM_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14864a4",
   "metadata": {},
   "source": [
    "## Perform Delay Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7d4f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_filter_cache_scratch(cache_files):\n",
    "    \"\"\"\n",
    "    Load files from a cache specified by cache_dir.\n",
    "    \"\"\"\n",
    "    # Load up the cache file with the most keys (precomputed filter matrices).\n",
    "    cache = {}\n",
    "    # loop through cache files, load them.\n",
    "    # If there are new keys, add them to internal cache.\n",
    "    # If not, delete the reference matrices from memory.\n",
    "    for cache_file in cache_files:        \n",
    "        try:\n",
    "            cfile = open(cache_file, 'rb')\n",
    "            try:\n",
    "                cache_t = pickle.load(cfile)\n",
    "            finally:\n",
    "                cfile.close()\n",
    "        except:\n",
    "            continue\n",
    "        for key in cache_t:\n",
    "            cache[key] = cache_t[key]\n",
    "                \n",
    "    return cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0b627c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peform delay filter separately above and below FM\n",
    "low_band = slice(0, np.argwhere(hd.freqs > FM_LOW_FREQ * 1e6)[0][0])\n",
    "high_band = slice(np.argwhere(hd.freqs > FM_HIGH_FREQ * 1e6)[0][0], len(hd.freqs))\n",
    "to_filter = np.outer(np.ones(len(hd.times), dtype=bool), (hd.freqs <= FM_LOW_FREQ * 1e6) | (hd.freqs > FM_HIGH_FREQ * 1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b244aef-9d56-4573-b597-aee6bd2ac344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up datacontainers for delay-filtered data products\n",
    "dly_filt_red_avg_smooth_cal_sum_data = copy.deepcopy(red_avg_smooth_cal_sum_data)\n",
    "dly_filt_red_avg_abs_cal_sum_data = copy.deepcopy(red_avg_abs_cal_sum_data)\n",
    "if SAVE_DIFF_RED_AVG:\n",
    "    dly_filt_red_avg_smooth_cal_diff_data = copy.deepcopy(red_avg_smooth_cal_diff_data)\n",
    "    dly_filt_red_avg_abs_cal_diff_data = copy.deepcopy(red_avg_abs_cal_diff_data)\n",
    "\n",
    "auto_bls = [bl for bl in dly_filt_red_avg_smooth_cal_sum_data if utils.split_bl(bl)[0] == utils.split_bl(bl)[1]]\n",
    "gap_flag_warned = set()\n",
    "\n",
    "new_cache_keys = []\n",
    "if ALL_FLAGGED and ALL_CAL_FLAGS:\n",
    "    print('All integrations flagged after the file_calibration notebook... skipping delay filtering.')\n",
    "else:    \n",
    "    # Load files in cache\n",
    "    cache = read_filter_cache_scratch(cache_files)\n",
    "    cache_keys = list(cache.keys())\n",
    "    \n",
    "    for bl in sorted(red_avg_flags.keys(), key=lambda bl: np.sum(red_avg_flags[bl]), reverse=True):\n",
    "        # inverse variance weight using smooth_calibrated autocorrealtions (which are proprotional to std of noise)\n",
    "        auto_bl1 = [abl for abl in auto_bls if abl[2] == utils.join_pol(utils.split_pol(bl[2])[0], utils.split_pol(bl[2])[0])][0]\n",
    "        auto_bl2 = [abl for abl in auto_bls if abl[2] == utils.join_pol(utils.split_pol(bl[2])[1], utils.split_pol(bl[2])[1])][0]\n",
    "        wgts = np.where(red_avg_flags[bl], 0, np.abs(red_avg_smooth_cal_sum_data[auto_bl1] * red_avg_smooth_cal_sum_data[auto_bl2])**-1)\n",
    "        wgts /= np.nanmean(np.where(red_avg_flags[bl], np.nan, wgts))  # avoid dynamic range issues\n",
    "        wgts[~np.isfinite(wgts)] = 0        \n",
    "\n",
    "        # calculate filter properties\n",
    "        bl_vec = (hd.antpos[bl[1]] - hd.antpos[bl[0]])\n",
    "        bl_len = np.linalg.norm(bl_vec[:2]) / constants.c\n",
    "        filter_centers, filter_half_widths = vis_clean.gen_filter_properties(ax='freq', min_dly=DLY_FILT_MIN_DLY, horizon=DLY_FILT_HORIZON,\n",
    "                                                                             standoff=DLY_FILT_STANDOFF, bl_len=bl_len)\n",
    "        \n",
    "        for dc in [dly_filt_red_avg_smooth_cal_sum_data, dly_filt_red_avg_abs_cal_sum_data] + \\\n",
    "                  ([dly_filt_red_avg_smooth_cal_diff_data, dly_filt_red_avg_abs_cal_diff_data] if SAVE_DIFF_RED_AVG else []):\n",
    "            if np.all(dc[bl] == 0.0) and np.all(red_avg_flags[bl]):\n",
    "                # don't bother delay filtering all 0s.\n",
    "                continue\n",
    "\n",
    "            # run delay filter for each band individually\n",
    "            d_mdl = np.zeros_like(dc[bl])\n",
    "            for band in [low_band, high_band]:\n",
    "                d_mdl[:, band], _, info = dspec.fourier_filter(hd.freqs[band], dc[bl][:, band], wgts=wgts[:, band], \n",
    "                                                               filter_centers=filter_centers, filter_half_widths=filter_half_widths,\n",
    "                                                               mode='dpss_solve', ridge_alpha=0, fit_intercept=False,\n",
    "                                                               eigenval_cutoff=[DLY_FILT_EIGENVAL_CUTOFF], \n",
    "                                                               suppression_factors=[DLY_FILT_EIGENVAL_CUTOFF], \n",
    "                                                               max_contiguous_edge_flags=len(hd.freqs), cache=cache)\n",
    "\n",
    "                # Track new DPSS-filters entering the cache\n",
    "                fourier_filter_key = dspec._fourier_filter_hash(\n",
    "                    filter_centers=filter_centers, filter_half_widths=filter_half_widths, filter_factors=[0.], x=hd.freqs[band], w=None,\n",
    "                    crit_name='eigenval_cutoff', label='dpss_operator', crit_val=tuple([DLY_FILT_EIGENVAL_CUTOFF])\n",
    "                )\n",
    "                if fourier_filter_key not in cache_keys:\n",
    "                    new_cache_keys.append(fourier_filter_key)                    \n",
    "                    \n",
    "            if (bl[0] == bl[1]):\n",
    "                # inpaint autos at delay-filter delay\n",
    "                dc[bl] = np.where(red_avg_flags[bl], d_mdl, dc[bl])\n",
    "            else:\n",
    "                # for all other baselines, filter foregrounds\n",
    "                dc[bl] = np.where(red_avg_flags[bl], 0, dc[bl] - d_mdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef5036e-97e0-4eca-b16b-597151b3ade4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_delay_spectra():\n",
    "    if ALL_FLAGGED:\n",
    "        print('All integrations are flagged. Nothing to plot.')\n",
    "        return\n",
    "    \n",
    "    # loop over bands\n",
    "    red_avg_flag_waterfall = np.all(list(red_avg_flags.values()), axis=0)\n",
    "    unflagged_inds = np.squeeze(np.argwhere(~np.any(red_avg_flag_waterfall[~np.all(red_avg_flag_waterfall, axis=1), :], axis=0)))\n",
    "    low_band = slice(unflagged_inds[0], np.max(unflagged_inds[hd.freqs[unflagged_inds] < FM_LOW_FREQ * 1e6]) + 1)\n",
    "    high_band = slice(np.min(unflagged_inds[hd.freqs[unflagged_inds] > FM_HIGH_FREQ * 1e6]), unflagged_inds[-1] + 1)\n",
    "    for band in [low_band, high_band]:\n",
    "\n",
    "        display(HTML(f'<h2>Redundantly-Averaged, Smooth-Calibrated Delay Spectra: {hd.freqs[band][0] / 1e6:.2f} — {hd.freqs[band][-1] / 1e6:.2f} MHz</h2>'))\n",
    "        fig, axes = plt.subplots(5, 2, figsize=(14,10), sharex=True, sharey='row', gridspec_kw={'hspace': 0, 'wspace': 0})\n",
    "\n",
    "        mins, maxes = {}, {}\n",
    "        for bl in red_avg_smooth_cal_sum_data:\n",
    "            \n",
    "            # figure out whether to plot this baseline\n",
    "            bl_vec = (hd.antpos[bl[1]] - hd.antpos[bl[0]])\n",
    "            if not ((np.abs(bl_vec[1]) < 1) and int(np.round(bl_vec[0] / 14.6)) in [0, 1, 2, 4, 8]):\n",
    "                continue\n",
    "            if bl[2] not in ['ee', 'nn']:\n",
    "                continue            \n",
    "            \n",
    "            # compute filter size\n",
    "            bl_len = np.linalg.norm(bl_vec[:2]) / constants.c\n",
    "            _, dly_filter_half_widths = vis_clean.gen_filter_properties(ax='freq', horizon=DLY_FILT_HORIZON, standoff=DLY_FILT_STANDOFF, \n",
    "                                                                        min_dly=DLY_FILT_MIN_DLY, bl_len=bl_len)   \n",
    "            \n",
    "            # figure out where to put the plot\n",
    "            row = {0: 0, 1: 1, 2: 2, 4: 3, 8: 4}[int(np.round(bl_vec[0] / 14.6))]\n",
    "            col = int(bl[2] == 'nn')\n",
    "            ax = axes[row, col]\n",
    "            \n",
    "            # FFT delay-filtered sums\n",
    "            dfft_df, delays = vis_clean.fft_data(dly_filt_red_avg_smooth_cal_sum_data[bl][:, band], delta_bin=(hd.freqs[1] - hd.freqs[0]), window='bh')\n",
    "            ax.semilogy(delays*1e9, np.mean(np.abs(dfft_df[~integration_flags, :]), axis=0), ls='--', label=f'Delay-Filtered Sum Data')\n",
    "\n",
    "            # FFT diffs\n",
    "            if SAVE_DIFF_RED_AVG:\n",
    "                dfft_ddf, _ = vis_clean.fft_data(np.where(red_avg_flags[bl], 0, red_avg_smooth_cal_diff_data[bl])[:, band], \n",
    "                                                 delta_bin=(hd.freqs[1] - hd.freqs[0]), window='bh')\n",
    "                ax.semilogy(delays*1e9, np.mean(np.abs(dfft_ddf[~integration_flags, :]), axis=0), ls=':', label=f'Diff Data')\n",
    "    \n",
    "                mins[bl] = np.median(np.abs(dfft_ddf[~integration_flags, :])) / 5\n",
    "            else:\n",
    "                mins[bl] = np.median(np.abs(dfft_df[~integration_flags, :])) / 5\n",
    "            try:\n",
    "                maxes[bl] = np.max(np.abs(dfft_ip[~integration_flags, :])) * 2\n",
    "            except:\n",
    "                maxes[bl] = np.max(np.abs(dfft_df[~integration_flags, :])) * 2\n",
    "\n",
    "            ax.axvline(1e9 * dly_filter_half_widths[0], c='k', ls='--', lw=1, alpha=.5, label=f'Delay Filter Delay')\n",
    "            ax.axvline(-1e9 * dly_filter_half_widths[0], c='k', ls='--', lw=1, alpha=.5)            \n",
    "\n",
    "            if row == col == 0:\n",
    "                ax.legend(loc='upper left')\n",
    "            ax.set_xlim([-2250, 2250])\n",
    "\n",
    "            vec = f'{bl[2]}-polarized\\n{bl_vec[0]:.1f} m East\\n{bl_vec[1]:.1f} m North'\n",
    "            ax.text(.97, .9, vec, transform=ax.transAxes, va='top', ha='right', bbox=dict(facecolor='w', alpha=0.5))\n",
    "            if col == 0:\n",
    "                ax.set_ylabel('$|\\\\widetilde{V}|$')\n",
    "            ax.set_xlabel('Delay (ns)')\n",
    "\n",
    "        for ax in axes[1:, :].flatten():\n",
    "            ax.set_ylim(np.min([mins[bl] for bl in mins if bl[0] != bl[1]]), \n",
    "                        np.max([maxes[bl] for bl in mins if bl[0] != bl[1]]))\n",
    "        for ax in axes[0, :]:\n",
    "            ax.set_ylim(np.min([mins[bl] for bl in mins if bl[0] == bl[1]]), \n",
    "                        np.max([maxes[bl] for bl in mins if bl[0] == bl[1]]))\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bbf0fe",
   "metadata": {},
   "source": [
    "# *Figure 3: Delay-Filtered Visibility Delay Spectra after `smooth_cal`* and Redundant Averaging\n",
    "\n",
    "This figure plots delay spectra for autocorrelations and 1, 2, 4, and 8 unit East-west baselines. The delay-spectra are computed by FFTing the visibilities with a Blackman-Harris taper and then taking the absolute value. The delay spectra of visibility sums are delay-filtered. The delay spectra of visibility diffs are generated by simply setting their flagged channels to 0. For autocorrelations, only inpainting is performed. These delay spectra are plotted for both polarizations and in both the high (above FM) and low (below FM) bands. Note that the frequency ranges over which the delay-filtering was performed might be slightly larger than bands shown here since flagged channels at band edges were excluded from the FFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f36801c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_delay_spectra()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d997b192",
   "metadata": {},
   "source": [
    "## Save redundantly averaged visbilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7157163",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _empty_hd(hd, antpairs, pols):\n",
    "    x_orientation = hd.telescope.get_x_orientation_from_feeds()\n",
    "    new_uvd = UVData.new(freq_array=hd.freq_array,\n",
    "                         polarization_array=[utils.polstr2num(p, x_orientation=x_orientation) for p in pols],\n",
    "                         times=hd.times,\n",
    "                         telescope=hd.telescope,\n",
    "                         antpairs=antpairs,\n",
    "                         empty=True)\n",
    "    return io.to_HERAData(new_uvd)\n",
    "\n",
    "if SAVE_RESULTS:\n",
    "    hd_out = io.HERAData(SUM_FILE)\n",
    "    try:\n",
    "        hd_out.read(bls=list(set([bl[0:2] for bl in red_avg_flags])), polarizations=list(red_avg_flags.pols()))\n",
    "    except KeyError:\n",
    "        # there's a problem with one of the data/flags/nsamples fields\n",
    "        hd_out = _empty_hd(hd_out, antpairs=list(set([bl[0:2] for bl in red_avg_flags])), pols=pols)\n",
    "    hd_out.empty_arrays()\n",
    "    hd_out.history += add_to_history\n",
    "    hd_out.update(flags=red_avg_flags, nsamples=red_avg_nsamples)\n",
    "    hd_out.pol_convention = pol_convention\n",
    "    hd_out.vis_units = gain_scale\n",
    "    if ALL_FLAGGED:\n",
    "        # put back in all the flags that we had been ignoring up to this point\n",
    "        hd_out.flag_array = np.ones_like(hd_out.flag_array)\n",
    "\n",
    "    def _write(dc, outfile, *args):\n",
    "        if not np.all(args): \n",
    "           return \n",
    "        hd_out.update(data=dc)\n",
    "        print(f'Now writing redundantly-averaged calibrated visibilities to {outfile}')\n",
    "        hd_out.write_uvh5(outfile, clobber=True, fix_autos=True)\n",
    "\n",
    "    # write delay-filtered file(s)\n",
    "    _write(red_avg_smooth_cal_sum_data, SUM_SMOOTH_CAL_RED_AVG_FILE)\n",
    "    _write(red_avg_abs_cal_sum_data, SUM_ABS_CAL_RED_AVG_FILE, SAVE_ABS_CAL_RED_AVG)\n",
    "    _write(dly_filt_red_avg_smooth_cal_sum_data, SUM_SMOOTH_CAL_RED_AVG_DLY_FILT_FILE, SAVE_DLY_FILT_RED_AVG)\n",
    "    _write(dly_filt_red_avg_abs_cal_sum_data, SUM_ABS_CAL_RED_AVG_DLY_FILT_FILE, SAVE_DLY_FILT_RED_AVG, SAVE_ABS_CAL_RED_AVG)\n",
    "    if SAVE_DIFF_RED_AVG:\n",
    "        _write(red_avg_smooth_cal_diff_data, DIFF_SMOOTH_CAL_RED_AVG_FILE, SAVE_DIFF_RED_AVG)\n",
    "        _write(red_avg_abs_cal_diff_data, DIFF_ABS_CAL_RED_AVG_FILE, SAVE_ABS_CAL_RED_AVG, SAVE_DIFF_RED_AVG)\n",
    "        _write(dly_filt_red_avg_smooth_cal_diff_data, DIFF_SMOOTH_CAL_RED_AVG_DLY_FILT_FILE, SAVE_DLY_FILT_RED_AVG, SAVE_DIFF_RED_AVG)    \n",
    "        _write(dly_filt_red_avg_abs_cal_diff_data, DIFF_ABS_CAL_RED_AVG_DLY_FILT_FILE, SAVE_DLY_FILT_RED_AVG, SAVE_ABS_CAL_RED_AVG, SAVE_DIFF_RED_AVG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2135abb",
   "metadata": {},
   "source": [
    "## Save incoherent average magnitude waterfalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9613f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for mode in ['autos', 'crosses', 'all visibiltiies']:\n",
    "    avg_abs_data, avg_flags, avg_nsamples = {}, {}, {}\n",
    "    for pol in ['ee', 'nn']:\n",
    "        bls_to_avg = []\n",
    "        if mode == 'autos' or mode == 'all visibiltiies':\n",
    "            key = (hd.data_ants[0], hd.data_ants[0], pol)\n",
    "            bls_to_avg += [bl for bl in red_avg_smooth_cal_sum_data if bl[0] == bl[1] and bl[2] == pol]\n",
    "        if mode == 'crosses' or mode == 'all visibiltiies':\n",
    "            key = (hd.data_ants[0], hd.data_ants[1], pol)\n",
    "            bls_to_avg += [bl for bl in red_avg_smooth_cal_sum_data if bl[0] != bl[1] and bl[2] == pol]\n",
    "        \n",
    "        if len(bls_to_avg) > 0:\n",
    "            weights = np.array([red_avg_nsamples[bl] * (~red_avg_flags[bl]) for bl in bls_to_avg])\n",
    "            avg_flags[key] = np.all(weights == 0, axis=0)\n",
    "            weights = np.array([np.where(avg_flags[key], red_avg_nsamples[bl], \n",
    "                                         red_avg_nsamples[bl] * (~red_avg_flags[bl])) for bl in bls_to_avg])\n",
    "            if np.all(weights == 0):\n",
    "                weights = np.ones_like(weights)\n",
    "            avg_nsamples[key] = np.sum([red_avg_nsamples[bl] for bl in bls_to_avg], axis=0)\n",
    "            avg_abs_data[key] = np.average([np.abs(red_avg_smooth_cal_sum_data[bl]) for bl in bls_to_avg], weights=weights, axis=0)\n",
    "    \n",
    "    if SAVE_RESULTS:\n",
    "        hd_out = io.HERAData(SUM_FILE)\n",
    "        try:\n",
    "            hd_out.read(bls=set([bl[0:2] for bl in avg_abs_data]), polarizations=['ee', 'nn'])\n",
    "        except KeyError:\n",
    "            # there's a problem with one of the data/flags/nsamples fields\n",
    "            hd_out = _empty_hd(hd_out, antpairs=list(set([bl[0:2] for bl in avg_abs_data])), pols=['ee', 'nn'])\n",
    "        hd_out.empty_arrays()\n",
    "        hd_out.update(data=avg_abs_data, flags=avg_flags, nsamples=avg_nsamples)\n",
    "        hd_out.pol_convention = pol_convention\n",
    "        hd_out.vis_units = gain_scale\n",
    "        if ALL_FLAGGED:\n",
    "            # put back in all the flags that we had been ignoring up to this point\n",
    "            hd_out.flag_array = np.ones_like(hd_out.flag_array)\n",
    "        hd_out.history += add_to_history\n",
    "        outfile = {'autos': AVG_ABS_AUTO_FILE, 'crosses': AVG_ABS_CROSS_FILE, 'all visibiltiies': AVG_ABS_ALL_FILE}[mode]\n",
    "        print(f'Writing averaged absolute of {mode} to {outfile}')\n",
    "        hd_out.write_uvh5(outfile, clobber=True, fix_autos=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6cf577",
   "metadata": {},
   "source": [
    "## Save baseline-selected files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21106594",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE_RESULTS and DO_BL_SELECT:\n",
    "    # Get the largest groups, including autos\n",
    "    reds = redcal.get_pos_reds(hd.antpos, include_autos=True, bl_error_tol=2.0)\n",
    "    sorted_reds = sorted(reds, key=lambda item: -len(item)) # Minus sign so it sorts high to low\n",
    "    best_reds = sorted_reds[:BL_SELECT_NGROUP]\n",
    "    all_bls_to_load = set([tuple([int(ant) for ant in bl]) for red in best_reds for bl in red])\n",
    "    hd_out = io.HERAData(SUM_FILE)\n",
    "    all_bls_to_load = sorted(all_bls_to_load & set([tuple([int(ant) for ant in bl[0:2]]) for bl in hd_out.bls]))\n",
    "    hd_out.read(bls=all_bls_to_load)\n",
    "        \n",
    "    # Apply cals\n",
    "    blgroups_data, blgroups_flags, _ = hd_out.build_datacontainers() \n",
    "    apply_cal.calibrate_in_place(blgroups_data, smooth_gains, data_flags=blgroups_flags, cal_flags=cal_flags)\n",
    "    hd_out.update(data=blgroups_data, flags=blgroups_flags)\n",
    "    if ALL_FLAGGED:\n",
    "        hd_out.flag_array[:] = True \n",
    "\n",
    "    # Some housekeeping, and write\n",
    "    hd_out.history += add_to_history\n",
    "    hd_out.pol_convention = pol_convention\n",
    "    hd_out.vis_units = gain_scale\n",
    "    hd_out.write_uvh5(BL_SELECT_FILE, clobber=True, fix_autos=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4d2551",
   "metadata": {},
   "source": [
    "## Save filter cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1011f4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE_RESULTS:\n",
    "    # Create cache directory if it doesn't already exist\n",
    "    if not os.path.exists(filter_cache_dir):\n",
    "        os.mkdir(filter_cache_dir)\n",
    "        \n",
    "    # For each new cache key save a new cache file in the cache directory\n",
    "    for key in new_cache_keys:\n",
    "        cache_file_basename = key[0] + '.filter_cache.p'\n",
    "        if not os.path.exists(os.path.join(filter_cache_dir, cache_file_basename)):\n",
    "            try:\n",
    "                cfile = open(os.path.join(filter_cache_dir, cache_file_basename), 'wb')\n",
    "                try:\n",
    "                    pickle.dump({key: cache[key]}, cfile)\n",
    "                finally:\n",
    "                    cfile.close()\n",
    "            except (OSError, IOError):\n",
    "                # File could not be written to, move on\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530f8747",
   "metadata": {},
   "source": [
    "## Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bef5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "for repo in ['pyuvdata', 'hera_cal', 'hera_qm', 'hera_filters', 'hera_notebook_templates']:\n",
    "    exec(f'from {repo} import __version__')\n",
    "    print(f'{repo}: {__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a206d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Finished execution in {(time.time() - tstart) / 60:.2f} minutes.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
