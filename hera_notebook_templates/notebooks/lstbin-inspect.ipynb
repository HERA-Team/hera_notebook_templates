{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H6C LST-Binning Inspection Notebook\n",
    "\n",
    "**Steven Murray & Josh Dillon**\n",
    "\n",
    "This notebook provides a sense-check for H6C LST-binned data. It can operate in two different modes: either on redundantly-averaged or non-redundantly-averaged data. Some plots will be included/omitted dependending on which of these modes is being used (the mode is auto-detected)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Figures\n",
    "\n",
    "* [Auto-Correlation Plot](#Figure:-Auto-Correlation-Plot)\n",
    "* [Mean Excess Variance as a Function of Frequency](#Figure:-Mean-Excess-Variance-as-Function-of-Frequency)\n",
    "* Distribution of Excess Variance:\n",
    "  * [As function of Days Binned](#Figure:-Distribution-of-Excess-Variance-as-function-of-Days-Binned)\n",
    "  * [Across Baseline Subsets and LSTs for Low- and High-Band](#Figure:-Distribution-of-Excess-Variance-Across-Baseline-Subsets-and-LSTs-for-Low--and-High-Band)\n",
    "  * [Across LSTs and Bands for All Baselines](#Figure:-Distribution-of-Excess-Variance-across-LSTs-and-Bands-for-All-Baselines)\n",
    "  * [As Function of Baseline Lenth and LST at 160 MHz](#Figure:-Distribution-of-Excess-Variance-with-Baseline-Length-and-LST-at-160-MHz)\n",
    "  * [Between NS and EW baselines and N/E pols](#Figure:-Distribution-of-Excess-Variance-Between-NS-and-EW-baselines-and-pols)\n",
    "  * [Across Redundant Group Size](#Figure:-Distribution-of-Excess-Variance-Across-Redundant-Group-Size)\n",
    "* [Raw Visibilities over Nights for the Worst Cases of Excess Variance](#Figure:-Visibilities-Over-Nights)\n",
    "* Distribution of Predicted Z-Scores:\n",
    "  * [For a single Frequency/LST/Night](#Figure:-Histogram-of-Baseline-Z-Scores-at-single-Frequency-/-LST-/-Night)\n",
    "  * [Per-night at 138 MHz](#Figure:-Box-Plot-of-Z-Scores-at-138-MHz)\n",
    "  * [Per-night at 169 MHz](#Figure:-Box-Plot-of-Z-Scores-at-169-MHz)\n",
    "* Sigma-Clipping\n",
    "  * [Sigma-Clipped Fraction as a Function of Threshold, LST and Night](#Figure:-Sigma-Clipped-Fraction-As-Function-of-Threshold,-LST-and-Night)\n",
    "  * [Sigma-Clipped Fraction as a Function of Frequency](#Figure:-Sigma-Clipped-Fraction-as-Function-of-Frequency)\n",
    "  * [List of Most Sigma-Clipped LSTs, Nights and Baselines](#List-of-most-sigma-clipped-LSTs,-Nights-and-Baselines)\n",
    "  * [Counts of Contiguous Flagged Region Sizes](#Figure:-Counts-of-Contiguous-Flagged-Region-Sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-16T19:32:37.266279Z",
     "start_time": "2021-02-16T19:32:33.932962Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "from hera_cal.io import HERAData\n",
    "from hera_cal.datacontainer import DataContainer\n",
    "import glob\n",
    "from hera_cal import utils, noise, redcal, lstbin\n",
    "from hera_cal.lstbin_simple import lst_average\n",
    "from hera_cal.abscal import match_times\n",
    "from copy import deepcopy\n",
    "import os\n",
    "from IPython.display import display, HTML\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from astropy.time import Time\n",
    "from astropy import units as un\n",
    "import matplotlib as mpl\n",
    "from hera_cal import io, apply_cal\n",
    "import toml\n",
    "from hera_cal.io import HERADataFastReader\n",
    "from collections import defaultdict\n",
    "from matplotlib import patches\n",
    "from hera_opm.mf_tools import get_lstbin_datafiles\n",
    "from hera_cal.red_groups import RedundantGroups\n",
    "from hera_cal.datacontainer import RedDataContainer\n",
    "import yaml\n",
    "from pyuvdata.uvdata import FastUVH5Meta\n",
    "import re\n",
    "import json\n",
    "from functools import partial\n",
    "import copy\n",
    "import attrs\n",
    "from functools import cached_property\n",
    "from scipy.stats import gamma, norm\n",
    "from collections import Counter\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the path below if running this notebook interactively\n",
    "lstbin_path = Path(os.environ.get(\n",
    "    \"LSTBIN_PATH\", \n",
    "    \"/lustre/aoc/projects/hera/h6c-analysis/IDR2/lstbin-outputs/redavg-no-sigma-clip/\"\n",
    "))\n",
    "\n",
    "if not lstbin_path.exists():\n",
    "    raise IOError(f\"{lstbin_path} does not exist!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the baseline groups we want to look at\n",
    "bl_groups_to_view = os.environ.get(\n",
    "    \"BLGROUPS\",  # should be in given in form '[[1, 2,\"ee\"], [0,0,\"nn\"]]'\n",
    "    [\n",
    "        (4, 7, 'ee'),\n",
    "        (4, 7, 'nn'),\n",
    "        (10, 22, 'nn'), \n",
    "        (20, 22, 'nn'), \n",
    "        (10, 47, 'nn'), \n",
    "        (81, 155, 'ee'), \n",
    "        (8, 61, 'ee'),\n",
    "        (4, 6, 'ee'),\n",
    "    ]\n",
    ")\n",
    "\n",
    "if isinstance(bl_groups_to_view, str):\n",
    "    bl_groups_to_view=[tuple(x) for x in json.loads(bl_groups_to_view)]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = toml.load(lstbin_path / \"lstbin-config.toml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(lstbin_path / 'file-config.yaml', 'r') as fl:\n",
    "    lstbin_file_config = yaml.safe_load(fl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlst = lstbin_file_config['config_params']['dlst']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of every file that goes into the LST-bin products\n",
    "all_data_files = sum(sum(lstbin_file_config['matched_files'], start=[]), start=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the JDs of the input data files from their filenames (i.e. the first time in each file)\n",
    "data_jds = [float(re.findall(lstbin_file_config['config_params']['jd_regex'], fl)[0]) for fl in all_data_files]\n",
    "data_jd_ints = sorted({int(jd) for jd in data_jds})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a simple increasing list of jd-ints that cover the full observation (including missing days)\n",
    "JDs = np.arange(int(min(data_jds)), int(max(data_jds)) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The dataset spans a range of days {JDs.min()} -- {JDs.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of output files containing the full dataset at a given LST, for ease of comparison.\n",
    "GOLDEN_LSTs = [float(x) for x in config['LSTBIN_OPTS']['golden_lsts'].split(\",\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "golden_files = sorted(lstbin_path.glob('*.GOLDEN.*'))\n",
    "golden_hds = [io.HERAData(fl) for fl in golden_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = FastUVH5Meta(all_data_files[0], blts_are_rectangular=True)\n",
    "dt = (meta.times[1] - meta.times[0])*3600*24\n",
    "df = meta.freq_array[1] - meta.freq_array[0]\n",
    "dlst = (meta.lsts[1] - meta.lsts[0])%(2*np.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that for each golden file, all the LSTs are really aligned in a single bin\n",
    "for ghd in golden_hds:\n",
    "    assert ghd.lsts.min() + dlst >= (ghd.lsts.max() - 1e-7), f\"Got range of {ghd.lsts.max()  - ghd.lsts.min()} > {dlst}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the index of the LST-bin output file that matches the \"Golden\" LST, so we can do more easy comparison.\n",
    "# Get the file index and time index that the golden LST corresponds to\n",
    "lst_grid = lstbin_file_config['lst_grid']\n",
    "lst_grid_flat = np.array(sum(lst_grid, start=[]))\n",
    "dlst = lst_grid_flat[1] - lst_grid_flat[0]\n",
    "\n",
    "lst_edges = [(lst - dlst/2, lst + dlst/2) for lst in lst_grid_flat]\n",
    "\n",
    "# Note that the lst_edges might not all be contiguous, because some LSTs\n",
    "# might not be observed in a dataset\n",
    "            \n",
    "print(f\"Golden LSTs investigated in this notebook come from the file indices (and time indices within that file):\")\n",
    "\n",
    "golden_file_indices = {}\n",
    "golden_time_indices = {}\n",
    "new_golden_lsts = []\n",
    "for j, glst in enumerate(GOLDEN_LSTs):\n",
    "    for k, edges in enumerate(lst_edges):\n",
    "        if glst < edges[0]:\n",
    "            glst += 2*np.pi\n",
    "            \n",
    "        if glst < edges[1]:\n",
    "            break\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "    lstbin_index = k\n",
    "    glst = lst_grid_flat[lstbin_index]  # this is more exact than the golden LST gotten from file name\n",
    "\n",
    "    for i, lsts in enumerate(lst_grid):\n",
    "        if glst in lsts:\n",
    "            idx = lsts.index(glst)\n",
    "            golden_file_indices[glst] = i\n",
    "            golden_time_indices[glst] = idx\n",
    "            print(f\"LST {glst*12/np.pi:6.3f} hr: file={i:>04}, time-idx={idx}\")\n",
    "            break\n",
    "\n",
    "    new_golden_lsts.append(glst)\n",
    "GOLDEN_LSTs = new_golden_lsts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put our files and hds into the same kind of dicts\n",
    "new_golden_files = {}\n",
    "new_golden_hds = {}\n",
    "for fl, hd in zip(golden_files, golden_hds):\n",
    "    lstmean = np.mean(hd.lsts)\n",
    "    idx = np.argmin(np.abs(np.array(GOLDEN_LSTs) - lstmean))\n",
    "    new_golden_files[GOLDEN_LSTs[idx]] = fl\n",
    "    new_golden_hds[GOLDEN_LSTs[idx]] = hd\n",
    "    \n",
    "golden_files = new_golden_files\n",
    "golden_hds = new_golden_hds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "golden_meta =next(iter(golden_hds.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reds = RedundantGroups.from_antpos(golden_meta.antpos, pols=golden_meta.pols, include_autos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine if this is a redundant dataset or not\n",
    "blgroups = {reds.get_ubl_key(bl) for bl in golden_meta.bls}\n",
    "RED_DATA = len(blgroups) == len(golden_meta.bls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RED_DATA:\n",
    "    print(\"This dataset is redundantly averaged\")\n",
    "else:\n",
    "    print(\"This dataset is NOT redundantly averaged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the baselines required to view the *groups* requested by the user\n",
    "keyed_reds = reds.keyed_on_bls(golden_meta.bls)\n",
    "\n",
    "if RED_DATA:\n",
    "    bls_to_read = list({keyed_reds.get_ubl_key(bl) for bl in bl_groups_to_view + [(0,0,'ee'), (0,0,'nn')]})\n",
    "else:\n",
    "    bls_to_read = set(sum([reds[bl] for bl in bl_groups_to_view + [(0,0,'ee'), (0,0,'nn')]], []))\n",
    "    bls_to_read = [bl for bl in bls_to_read if bl in golden_hds[0].bls or utils.reverse_bl(bl) in golden_hds[0].bls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_redundant(hd, reds=None, bls=None, **kw):\n",
    "    if reds is None:\n",
    "        reds = RedundantGroups.from_antpos(hd.antpos, hd.pols, include_autos=True)\n",
    "    keyed = reds.keyed_on_bls(hd.bls)\n",
    "    if bls is not None:\n",
    "        bls = [keyed.get_ubl_key(bl) for bl in bls]\n",
    "    d, f , n = hd.read(bls=bls, **kw)\n",
    "    d = RedDataContainer(d, reds=reds)\n",
    "    f = RedDataContainer(f, reds=reds)\n",
    "    n = RedDataContainer(n, reds=reds)\n",
    "    return d, f, n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "golden_data = {}\n",
    "golden_flags = {}\n",
    "golden_nsamples = {}\n",
    "\n",
    "# We read all the baselines, because we need the golden data to get the expected variance for each baseline.\n",
    "for glst, hd in golden_hds.items():\n",
    "    if RED_DATA:\n",
    "        golden_data[glst], golden_flags[glst], golden_nsamples[glst] = read_redundant(hd, reds=reds)\n",
    "    else:\n",
    "        golden_data[glst], golden_flags[glst], golden_nsamples[glst] = hd.read(bls=bls_to_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-16T19:33:20.255535Z",
     "start_time": "2021-02-16T19:32:37.293023Z"
    }
   },
   "outputs": [],
   "source": [
    "# load LST-binned data for the bins that match the \"GOLDEN\" LSTs\n",
    "lst_bin_files = {}\n",
    "lstbin_hds = {}\n",
    "for glst, fl_idx in golden_file_indices.items():\n",
    "    lst_edge = lst_grid[fl_idx][0] - dlst/2\n",
    "    fname = lstbin_path / \"zen.{kind}.{lst:7.5f}.sum.uvh5\".format(kind='LST', lst=lst_edge)\n",
    "    lst_bin_files[glst] = fname\n",
    "    lstbin_hds[glst] = HERAData(fname)\n",
    "\n",
    "# We read all the baselines for the lst-binned data, because we want to do averages.\n",
    "lstbin_data = {}\n",
    "lstbin_flags = {}\n",
    "lstbin_nsamples = {}\n",
    "for glst in GOLDEN_LSTs:\n",
    "    hd = lstbin_hds[glst]\n",
    "    idx = golden_time_indices[glst]\n",
    "    if RED_DATA:\n",
    "        lstbin_data[glst], lstbin_flags[glst], lstbin_nsamples[glst] = read_redundant(hd, reds=reds, times=[hd.times[idx]])\n",
    "    else:\n",
    "        lstbin_data[glst], lstbin_flags[glst], lstbin_nsamples[glst] = hd.read(times=[hd.times[idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load night-to-night standard deviations\n",
    "std_files = {glst: fl.parent / fl.name.replace('.LST.', '.STD.') for glst, fl, in lst_bin_files.items()}\n",
    "std_hds = {glst: HERAData(fl) for glst, fl in std_files.items()}\n",
    "\n",
    "std_data = {}\n",
    "std_flags = {}\n",
    "std_nsamples = {}\n",
    "for glst, hd in std_hds.items():\n",
    "    idx = golden_time_indices[glst]\n",
    "    if RED_DATA:\n",
    "        std_data[glst], std_flags[glst], std_nsamples[glst] = read_redundant(hd, reds=reds, times=[hd.times[idx]])\n",
    "    else:\n",
    "        std_data[glst], std_flags[glst], std_nsamples[glst] = hd.read(times=[hd.times[idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we just make double-sure that data/flags/nsamples are consistent\n",
    "for glst in GOLDEN_LSTs:\n",
    "    for bl in lstbin_data[glst].bls():\n",
    "        lstf = lstbin_flags[glst][bl]\n",
    "        lstn = lstbin_nsamples[glst][bl]\n",
    "        lstd = lstbin_data[glst][bl]\n",
    "        \n",
    "        lstf |= (lstn==0)\n",
    "        lstn[lstf] == 0\n",
    "        lstd[lstf] *= np.nan  # multiply by nan instead of setting to nan, to get the imaginary cmp.\n",
    "        std_data[glst][bl][lstf] *= np.nan\n",
    "        std_nsamples[glst][bl][lstf] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we re-sort the \"golden lsts\" such that we get them in one contiguous chunk\n",
    "if len(GOLDEN_LSTs) < 24:\n",
    "    firstidx = np.argmax(np.diff(sorted(GOLDEN_LSTs))) + 1\n",
    "    GOLDEN_LSTs = np.roll(GOLDEN_LSTs, len(GOLDEN_LSTs) - firstidx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign colors / line styles for days for the entire notebook, so we have consistency.\n",
    "styles = {}\n",
    "for i, jdint in enumerate(data_jd_ints):\n",
    "    styles[jdint] = {'color': f\"C{i%10}\", 'ls': ['-', '--', ':', '-.'][i//10]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Baseline Subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define some functions for obtaining different subsets of baselines (eg. long vs. short, EW vs. NS, different pols, intra- vs. inter-sector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_antenna_sectors():\n",
    "    antpos = next(iter(lstbin_data.values())).antpos\n",
    "    zero_pos = np.mean([antpos[165], antpos[166], antpos[145]], axis=0)\n",
    "    \n",
    "    sectors = {}\n",
    "    for ant, pos in antpos.items():\n",
    "        rec = pos - zero_pos\n",
    "        theta = np.arctan2(rec[1], rec[0])\n",
    "        bllen = np.sqrt(rec[0]**2 + rec[1]**2)\n",
    "        if bllen > 200:\n",
    "            sectors[ant] = 4  # outrigger\n",
    "        elif -np.pi / 3 <= theta < np.pi / 3:\n",
    "            sectors[ant] = 1\n",
    "        elif np.pi / 3 <= theta < np.pi:\n",
    "            sectors[ant] = 2\n",
    "        elif -np.pi <= theta < -np.pi/3:\n",
    "            sectors[ant] = 3\n",
    "    return sectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sectors = get_all_antenna_sectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getbllen(a,b):\n",
    "    return np.sqrt(np.sum(np.square(golden_meta.antpos[a] - golden_meta.antpos[b])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ee = lambda bl: bl[2] == 'ee'\n",
    "all_nn = lambda bl: bl[2] == 'nn'\n",
    "short_bls = lambda bl: getbllen(bl[0], bl[1])<=60.0\n",
    "long_bls = lambda bl: getbllen(bl[0], bl[1])>60.0\n",
    "intersector_bls = lambda bl: sectors[bl[0]] != sectors[bl[1]]\n",
    "intrasector_bls = lambda bl: sectors[bl[0]] == sectors[bl[1]]\n",
    "\n",
    "subsets = {\n",
    "    'all': lambda bl: True,\n",
    "    'ee-only': all_ee,\n",
    "    'nn-only': all_nn,\n",
    "    'Short (<60 m) baselines': short_bls,\n",
    "    'Long (>60 m) baselines': long_bls,\n",
    "    'Inter-sector baselines': intersector_bls,\n",
    "    \"Intra-sector baselines\": intrasector_bls,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_selected_bls(bls, days_binned, selectors=None, min_days: int=7):\n",
    "    mindaysel = lambda bl: bl in days_binned and (np.median(days_binned[bl]) >= min_days)\n",
    "    crossbl = lambda bl: bl[0] != bl[1] and bl[2][0] == bl[2][1]\n",
    "    \n",
    "    if selectors is None:\n",
    "        selectors = [mindaysel, crossbl]\n",
    "    elif callable(selectors):\n",
    "        selectors = [mindaysel, crossbl, selectors]\n",
    "    else:\n",
    "        selectors.extend([crossbl, mindaysel])\n",
    "        \n",
    "    select = lambda bl: all(sel(bl) for sel in selectors)\n",
    "    \n",
    "    return [bl for bl in bls if select(bl)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autocorrelations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_autos(\n",
    "    data: DataContainer, flags: DataContainer, tidx=slice(None), fig=None, ax=None, \n",
    "    xlabel: bool=True, ylabel: bool = True, title: bool=True, legend=True, color=None,\n",
    "    freq_step: int=1\n",
    "):\n",
    "    if fig is None:\n",
    "        fig, ax = plt.subplots(\n",
    "            1, 2, sharex=True, sharey=True, figsize=(15,7), \n",
    "            gridspec_kw={'wspace': 0.0}, constrained_layout=True\n",
    "        )\n",
    "    else:\n",
    "        assert len(ax) == 2\n",
    "    \n",
    "    handles = []\n",
    "    for i, pol in enumerate(data.pols()):\n",
    "        if pol[0] != pol[1]:\n",
    "            continue  # skip non-autos\n",
    "        \n",
    "        plt.sca(ax[i])    \n",
    "        \n",
    "        bls = [bl for bl in reds[(0,0,pol)] if bl in data]\n",
    "\n",
    "        for j, bl in enumerate(bls):\n",
    "            thisd = np.where(flags[bl][tidx], np.nan, np.abs(data[bl][tidx]))\n",
    "                \n",
    "            for k, spec in enumerate(thisd):\n",
    "                jdint = int(data.times[tidx][k])\n",
    "                \n",
    "                if i==0 and j==0:\n",
    "                    handles.append(mpl.lines.Line2D([0], [0], label=str(jdint), **styles[jdint]))\n",
    "                    \n",
    "                if np.all(np.isnan(spec)):\n",
    "                    continue \n",
    "                    \n",
    "                if color:\n",
    "                    plt.plot(\n",
    "                        data.freqs[::freq_step] / 1e6, \n",
    "                        spec[::freq_step], \n",
    "                        color=color\n",
    "                    )\n",
    "                else:\n",
    "                    plt.plot(\n",
    "                        data.freqs[::freq_step] / 1e6, \n",
    "                        spec[::freq_step], \n",
    "                        **styles[jdint]\n",
    "                    )\n",
    "                \n",
    "        plt.yscale('log')\n",
    "        if title:\n",
    "            plt.title(f\"{pol} Pol\")\n",
    "\n",
    "        if xlabel:\n",
    "            plt.xlabel(\"Frequency [MHz]\")\n",
    "    if ylabel:\n",
    "        ax[0].set_ylabel(\"|V| [Jy]\")\n",
    "    if legend:\n",
    "        ax[0].legend(loc='lower left', ncols=3, handles=handles)\n",
    "\n",
    "            \n",
    "def plot_autos_multi_lst(datas: dict[float, DataContainer], flags: dict[float, DataContainer], \n",
    "                         tidx: int=slice(None), freq_step: int = 1):\n",
    "    fig, ax = plt.subplots(\n",
    "        len(datas), 2, figsize=(10, max(1.5*len(datas), 6)), \n",
    "        sharex=True, sharey=True, \n",
    "        gridspec_kw={'hspace': 0, 'wspace': 0}, constrained_layout=True\n",
    "    )\n",
    "    \n",
    "    fig.suptitle(\"All Auto-Correlations\")\n",
    "\n",
    "    handles = []\n",
    "    for jdint in data_jd_ints:\n",
    "        handles.append(mpl.lines.Line2D([0], [0], label=str(jdint), **styles[jdint]))\n",
    "    handles.append(mpl.lines.Line2D([0], [0], label=\"LST-average\", color='k'))\n",
    "    ax[0,0].legend(handles=handles, ncols=3)\n",
    "    for i , key in enumerate(GOLDEN_LSTs):\n",
    "        plot_autos(\n",
    "            datas[key], flags[key], tidx=tidx, fig=fig, ax=ax[i], \n",
    "            xlabel=i==(len(GOLDEN_LSTs)-1), \n",
    "            title=i==0,\n",
    "            legend=False,\n",
    "            freq_step=freq_step\n",
    "        )\n",
    "        plot_autos(\n",
    "            lstbin_data[key], \n",
    "            lstbin_flags[key], fig=fig, ax=ax[i], \n",
    "            xlabel=False, title=False, \n",
    "            color='k',\n",
    "            freq_step=freq_step,\n",
    "            legend=False\n",
    "        )\n",
    "        ax[i, 1].text(\n",
    "            0.8, 0.8, \n",
    "            f\"{key*12/np.pi:5.2f} hr\", size=14, transform=ax[i,1].transAxes\n",
    "        )\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure: Auto-Correlation Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_autos_multi_lst(golden_data, golden_flags, freq_step=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure 1:** All autocorrelations in the night-to-night data going into the LST averages explored in this notebook. Each row is an LST bin, columns are polarizations. Colored lines represent different nights, and the black line represents the LST average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributions of Excess Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we explore the distribution of the night-to-night variance and variance of LST-averaged data. We derive theoretical distributions for these quantities in Memo #XXXX. We look at how these quantities vary with frequency, LST, various properties of the baselines, and things like autocorrelation magnitude and baseline group size (where applicable). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first set up some classes to deal with the theoretical and observed variance distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import rv_continuous\n",
    "\n",
    "class MixtureModel(rv_continuous):\n",
    "    \"\"\"A distribution model from mixing multiple models.\n",
    "    \n",
    "    Taken from https://stackoverflow.com/a/72315113/1467820\n",
    "    \"\"\"\n",
    "    def __init__(self, submodels, *args, weights = None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.submodels = submodels\n",
    "        if weights is None:\n",
    "            weights = [1 for _ in submodels]\n",
    "        if len(weights) != len(submodels):\n",
    "            raise(ValueError(f'There are {len(submodels)} submodels and {len(weights)} weights, but they must be equal.'))\n",
    "        self.weights = [w / sum(weights) for w in weights]\n",
    "        \n",
    "    def _pdf(self, x):\n",
    "        pdf = self.submodels[0].pdf(x) * self.weights[0]\n",
    "        for submodel, weight in zip(self.submodels[1:], self.weights[1:]):\n",
    "            pdf += submodel.pdf(x)  * weight\n",
    "        return pdf\n",
    "            \n",
    "    def _sf(self, x):\n",
    "        sf = self.submodels[0].sf(x) * self.weights[0]\n",
    "        for submodel, weight in zip(self.submodels[1:], self.weights[1:]):\n",
    "            sf += submodel.sf(x)  * weight\n",
    "        return sf\n",
    "\n",
    "    def _cdf(self, x):\n",
    "        cdf = self.submodels[0].cdf(x) * self.weights[0]\n",
    "        for submodel, weight in zip(self.submodels[1:], self.weights[1:]):\n",
    "            cdf += submodel.cdf(x)  * weight\n",
    "        return cdf\n",
    "\n",
    "    def rvs(self, size):\n",
    "        submodel_choices = np.random.choice(len(self.submodels), size=size, p = self.weights)\n",
    "        submodel_samples = [submodel.rvs(size=size) for submodel in self.submodels]\n",
    "        rvs = np.choose(submodel_choices, submodel_samples)\n",
    "        return rvs\n",
    "\n",
    "@attrs.define(slots=False)\n",
    "class LSTBinStats:\n",
    "    days_binned: DataContainer  = attrs.field()\n",
    "    n2n_var_obs: DataContainer = attrs.field()\n",
    "    lstavg_var_obs: DataContainer = attrs.field()\n",
    "    lstavg_var_pred: DataContainer = attrs.field()\n",
    "    per_night_var_pred: DataContainer = attrs.field()\n",
    "        \n",
    "    @classmethod\n",
    "    def from_data(cls, \n",
    "        data: DataContainer, nsamples: DataContainer, flags: DataContainer, \n",
    "        lstbin_data=None, lstbin_nsamples=None, lstbin_flags=None, std_data=None, \n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"Get the observed and predicted variance metrics from observations in a particular LST bin.\"\"\"\n",
    "        days_binned = {}\n",
    "        all_obs_var = {}\n",
    "        all_predicted_var = {}\n",
    "        all_interleaved_var = {}\n",
    "        all_predicted_binned_var = {}\n",
    "        excess_binned_var = {}\n",
    "        excess_interleaved_var = {}\n",
    "        per_night_var_pred = {}\n",
    "        \n",
    "        # Make sure we output correct types\n",
    "        dcls = data.__class__ # Either DataContainer or RedDataContainer\n",
    "        if dcls == RedDataContainer:\n",
    "            dcls = partial(dcls, reds=data.reds)\n",
    "            \n",
    "        if lstbin_data is None:\n",
    "            lstbind = {}\n",
    "            lstbinf = {}\n",
    "            std = {}\n",
    "            lstbin_nsamps = {}\n",
    "            for bl in data.bls():\n",
    "                lstbind[bl], lstbinf[bl], std[bl], lstbin_nsamps[bl] = lst_average(\n",
    "                    data[bl], nsamples[bl], flags[bl], **kwargs\n",
    "                )\n",
    "            \n",
    "            lstbin_data = dcls(lstbind)\n",
    "            lstbin_nsamples = dcls(lstbin_nsamps)\n",
    "            lstbin_flags = dcls(lstbinf)\n",
    "            std_data = dcls(std)\n",
    "            \n",
    "        for bl in data.bls():\n",
    "\n",
    "            gd = data[bl]\n",
    "            gn = nsamples[bl]\n",
    "            gf = flags[bl]\n",
    "\n",
    "            lstd = lstbin_data[bl][0]\n",
    "            lstn = lstbin_nsamples[bl][0]\n",
    "            lstf = lstbin_flags[bl][0]\n",
    "            stdd = std_data[bl][0]\n",
    "            \n",
    "            if np.all(gf):\n",
    "                continue\n",
    "\n",
    "            splbl = utils.split_bl(bl)\n",
    "            if splbl[0] == splbl[1]:  # don't use autos\n",
    "                continue\n",
    "\n",
    "            # Ensure flagged data has zero samples\n",
    "            gn[gf] = 0\n",
    "            \n",
    "            per_day_expected_var = noise.predict_noise_variance_from_autos(\n",
    "                bl, data, dt=dt, df=df, nsamples=nsamples\n",
    "            )\n",
    "            per_day_expected_var[gf] = np.inf\n",
    "            per_night_var_pred[bl] = per_day_expected_var\n",
    "            \n",
    "            wgts_arr = np.where(gf, 0, per_day_expected_var**-1) \n",
    "\n",
    "            # compute ancillary statistics, see math above\n",
    "            days_binned[bl] = np.sum(gn > 0, axis=0)\n",
    "            all_obs_var[bl] = np.abs(np.where(lstf, np.nan, stdd**2))\n",
    "            all_interleaved_var[bl] = noise.interleaved_noise_variance_estimate(\n",
    "                np.atleast_2d(np.where(lstf, np.nan, lstd)), kernel=[[1, -2, 1]]\n",
    "            )[0]\n",
    "            # Set first and last frequency to NaN\n",
    "            all_interleaved_var[bl][[0,-1]] = np.nan\n",
    "\n",
    "            all_predicted_binned_var[bl] = np.sum(wgts_arr, axis=0)**-1\n",
    "            all_predicted_var[bl] = (days_binned[bl] - 1) * all_predicted_binned_var[bl]\n",
    "\n",
    "            excess_binned_var[bl] = all_obs_var[bl] / all_predicted_var[bl]\n",
    "            excess_interleaved_var[bl] = all_interleaved_var[bl] / all_predicted_binned_var[bl]\n",
    "\n",
    "        days_binned = dcls(days_binned)\n",
    "        all_obs_var = dcls(all_obs_var)\n",
    "        all_predicted_var = dcls(all_predicted_var)\n",
    "        all_interleaved_var = dcls(all_interleaved_var)\n",
    "        all_predicted_binned_var = dcls(all_predicted_binned_var)\n",
    "        excess_binned_var = dcls(excess_binned_var)\n",
    "        excess_interleaved_var = dcls(excess_interleaved_var)\n",
    "        \n",
    "        return cls(\n",
    "            days_binned=dcls(days_binned),\n",
    "            n2n_var_obs = dcls(all_obs_var),\n",
    "            lstavg_var_obs= dcls(all_interleaved_var),\n",
    "            lstavg_var_pred= dcls(all_predicted_binned_var),\n",
    "            per_night_var_pred = dcls(per_night_var_pred),\n",
    "        )\n",
    "    \n",
    "    @cached_property\n",
    "    def _cls(self):\n",
    "        if isinstance(self.days_binned, RedDataContainer):\n",
    "            return partial(RedDataContainer, reds=self.days_binned.reds)\n",
    "        else:\n",
    "            return DataContainer\n",
    "        \n",
    "    @cached_property\n",
    "    def n2n_var_pred(self) -> DataContainer:\n",
    "        return self._cls({bl: self.lstavg_var_pred[bl] * (self.days_binned[bl] - 1) for bl in self.bls()})\n",
    "    \n",
    "    @cached_property\n",
    "    def n2n_excess_var(self) -> DataContainer:\n",
    "        return self._cls({bl: self.n2n_var_obs[bl] / self.n2n_var_pred[bl] for bl in self.bls()})\n",
    "\n",
    "    @cached_property\n",
    "    def lstavg_excess_var(self) -> DataContainer:\n",
    "        return self._cls({bl: self.lstavg_var_obs[bl] / self.lstavg_var_pred[bl] for bl in self.bls()})\n",
    "    \n",
    "    @classmethod\n",
    "    def n2n_excess_var_distribution(cls, ndays_binned: int):\n",
    "        return gamma(a=(ndays_binned-1)/2, scale=2/(ndays_binned-1))\n",
    "    \n",
    "    def n2n_excess_var_pred_dist(self, bls, freq_inds=slice(None), min_n: int = 1) -> rv_continuous:\n",
    "        \"\"\"Get a scipy distribution representing the theoretical distribution of excess variance.\n",
    "        \n",
    "        This will return a MixtureModel -- i.e. it will be the expected distribution of all frequencies\n",
    "        and baselines asked for (not their average).\n",
    "        \n",
    "        \"\"\"\n",
    "        if not hasattr(bls[0], \"__len__\"):\n",
    "            bls = [bls]\n",
    "            \n",
    "        all_ns = np.concatenate(tuple(self.days_binned[bl][freq_inds]for bl in bls))\n",
    "        unique_days_binned, counts = np.unique(all_ns, return_counts=True)\n",
    "        indx = np.argwhere(unique_days_binned >= min_n)[:, 0]\n",
    "        unique_days_binned = unique_days_binned[indx]\n",
    "        counts= counts[indx]\n",
    "        \n",
    "        return MixtureModel([self.n2n_excess_var_distribution(nn) for nn in unique_days_binned], weights=counts)\n",
    "\n",
    "    def n2n_excess_var_avg_pred_dist(self, bls, freq_inds=slice(None), min_n: int = 1):\n",
    "        \"\"\"Get a scipy distribution representing the theoretical distribution of averaged excess variance.\n",
    "        \n",
    "        This will return the expected distribution of the averaged excess variance for the\n",
    "        requested baselines and frequencies. Note this is NOT the excess averaged variance (i.e.\n",
    "        we're averaging the mean-one excess over the baselines/frequencies, rather than averaging\n",
    "        the observed variance and dividing by the averaged expected variance).\n",
    "        \n",
    "        This is exact for non-redundantly averaged data, and an approximation for red-avg data.\n",
    "        Gotten from https://stats.stackexchange.com/a/191912/81338\n",
    "        \"\"\"\n",
    "        if not hasattr(bls[0], \"__len__\"):\n",
    "            bls = [bls]\n",
    "\n",
    "        ndays_binned = (np.concatenate(tuple(self.days_binned[bl][freq_inds]for bl in bls)))\n",
    "        ndays_binned = ndays_binned[ndays_binned >= min_n]\n",
    "        \n",
    "        M = len(ndays_binned)\n",
    "        ksum = np.sum(M**2 / 2 / np.sum(1/(ndays_binned - 1)))\n",
    "        thetasum = 1 / ksum\n",
    "        \n",
    "        return gamma(a=ksum, scale=thetasum)\n",
    "        \n",
    "    def bls(self):\n",
    "        return self.days_binned.bls()\n",
    "    \n",
    "    def getmean(\n",
    "        self,\n",
    "        rdc: str | RedDataContainer | DataContainer, \n",
    "        bls = None,\n",
    "        min_days: int = 7\n",
    "    ):\n",
    "        if isinstance(rdc, str):\n",
    "            rdc = getattr(self, rdc)\n",
    "        if bls is None:\n",
    "            bls = self.bls()\n",
    "            \n",
    "        return np.nanmean([np.where(self.days_binned[bl] >= min_days, rdc[bl], np.nan) for bl in bls], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = {}\n",
    "for glst in GOLDEN_LSTs:\n",
    "    stats[glst] = LSTBinStats.from_data(\n",
    "        golden_data[glst], golden_nsamples[glst], golden_flags[glst], \n",
    "        lstbin_data[glst], lstbin_nsamples[glst], lstbin_flags[glst], std_data[glst]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-16T19:36:18.068992Z",
     "start_time": "2021-02-16T19:36:18.041429Z"
    }
   },
   "outputs": [],
   "source": [
    "def noise_comparison(glst, subsets: dict[str, callable], mean_of_ratios: bool = False, log: bool=False, min_days: int = 7):\n",
    "    \n",
    "    lstbin_hd = lstbin_hds[glst]\n",
    "    \n",
    "    stat = stats[glst]\n",
    "    meanvar = stat.getmean('n2n_var_pred', min_days=min_days)\n",
    "    \n",
    "    if np.all(np.isnan(meanvar)):\n",
    "        return\n",
    "    \n",
    "    fig, ax = plt.subplots(2,2, figsize=(16,8), sharex='col', gridspec_kw={'height_ratios': [2, 1]})\n",
    "    plt.subplots_adjust(hspace=.0)\n",
    "    ax=ax.flatten()\n",
    "\n",
    "    ax[0].plot(\n",
    "        lstbin_hd.freqs/1e6, \n",
    "        meanvar, \n",
    "        lw=2, \n",
    "        label='Predicted Variance from LST-Binned\\nAutocorrelations', \n",
    "        color='k'\n",
    "    )\n",
    "    ax[0].set_ylabel('Nightly Visibility Variance (Jy$^2$) ')\n",
    "    ax[0].set_title(\n",
    "        f'Visibility Variance Across Nights at {glst*12/np.pi:5.3f} Hours LST'\n",
    "        '\\n(Mean Over Unflagged Times and Baselines)'\n",
    "    )\n",
    "    if log:\n",
    "        ax[0].set_yscale('log')\n",
    "    else:\n",
    "        ax[0].set_ylim(-100, 6000)\n",
    "\n",
    "    ax[1].plot(\n",
    "        lstbin_hd.freqs/1e6, \n",
    "        stat.getmean('lstavg_var_pred', min_days=min_days), \n",
    "        lw=2,\n",
    "        label='Predicted Variance from LST-Binned\\nAutocorrelations and N$_{samples}$',\n",
    "        color='k'\n",
    "    )\n",
    "    ax[1].set_ylabel('LST-Binned Visibility Variance (Jy$^2$)')\n",
    "    ax[1].set_title(\n",
    "        f'Variance of LST-Binned Visibilities at {glst*12/np.pi:5.3f} Hours LST'\n",
    "        '\\n(Mean Over Unflagged Times and Baselines, measured by frequency-interleaving)'\n",
    "    )\n",
    "    \n",
    "    if log:\n",
    "        ax[1].set_yscale('log')\n",
    "    else:\n",
    "        ax[1].set_ylim(-10, 200)\n",
    "    \n",
    "    for i, (name, subset) in enumerate(subsets.items()):\n",
    "        bls = get_selected_bls(stat.bls(), days_binned=stat.days_binned, selectors=subset, min_days=min_days)\n",
    "        if not bls:\n",
    "            continue\n",
    "            \n",
    "        mean_obs_var = stat.getmean('n2n_var_obs', bls, min_days=min_days)\n",
    "        mean_interleaved_var = stat.getmean('lstavg_var_obs', bls, min_days=min_days)\n",
    "        \n",
    "        if mean_of_ratios:\n",
    "            mean_excess_var = stat.getmean('n2n_excess_var', bls, min_days=min_days)\n",
    "            mean_excess_lstavg_var = stat.getmean('lstavg_excess_var', bls, min_days=min_days)\n",
    "        else:\n",
    "            mean_excess_var = mean_obs_var / stat.getmean(\"n2n_var_pred\", bls, min_days=min_days)\n",
    "            mean_excess_lstavg_var = mean_interleaved_var / stat.getmean(\"lstavg_var_pred\", bls, min_days=min_days)\n",
    "                        \n",
    "        if i == 0:\n",
    "            ax[0].plot(lstbin_hd.freqs/1e6, mean_obs_var, lw=1, label=name, color=f'C{i}')\n",
    "            ax[1].plot(lstbin_hd.freqs/1e6, mean_interleaved_var, lw=1, color=f'C{i}')\n",
    "        else:\n",
    "            # Dummy plot to get a legend\n",
    "            ax[0].plot(lstbin_hd.freqs/1e6, np.nan*np.ones(len(mean_obs_var)), lw=1, label=name, color=f'C{i}')\n",
    "            \n",
    "        ax[2].plot(lstbin_hd.freqs/1e6, mean_excess_var, color=f'C{i}', lw=1)\n",
    "        favg_rat = np.nanmean(mean_excess_var)\n",
    "        ax[2].plot(lstbin_hd.freqs/1e6, np.ones_like(lstbin_hd.freqs) * favg_rat, '--', color=f'C{i}', label=f'{favg_rat:.3f}')\n",
    "    \n",
    "        ax[3].plot(lstbin_hd.freqs/1e6, mean_excess_lstavg_var, color=f'C{i}', lw=1)\n",
    "        favg_rat = np.nanmean(mean_excess_lstavg_var)\n",
    "        ax[3].plot(lstbin_hd.freqs/1e6, np.ones_like(lstbin_hd.freqs) * favg_rat, '--', color=f'C{i}', label=f'{favg_rat:.3f}')\n",
    "\n",
    "    ax[0].legend()\n",
    "    ax[1].legend()\n",
    "    ax[2].set_xlabel('Frequency (MHz)')\n",
    "    ax[2].set_xlim([40,250])\n",
    "    ax[2].set_ylim([.9, 1.5 * favg_rat])\n",
    "    ax[2].set_ylabel('Observed / Predicted')\n",
    "    ax[2].legend(loc='upper right', title='Freq-Mean Ratios', ncols=3)\n",
    "\n",
    "    ax[3].set_xlabel('Frequency (MHz)')\n",
    "    ax[3].set_ylim([.9, 1.5 * favg_rat])\n",
    "    ax[3].set_xlim([40,250])\n",
    "    ax[3].set_ylabel('Observed / Predicted')\n",
    "    ax[3].legend(loc='upper right', title=\"Freq-Mean Ratios\", ncols=3)\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure: Mean Excess Variance as Function of Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison of noise predicted from autocorrelations (and $N_{samples}$) to the noise measured either from the standard deviation across nights or from frequency-interleaving.\n",
    "\n",
    "Based on [Validation Test 4.0.0b](https://github.com/HERA-Team/hera-validation/blob/master/test-series/4/test-4.0.0b.ipynb) and [Aguirre et al. (2021) Figure 12](https://www.overleaf.com/project/5e7cdde364f7d40001749218) (the H1C IDR2 Validation paper)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-16T19:36:19.665547Z",
     "start_time": "2021-02-16T19:36:18.071612Z"
    }
   },
   "outputs": [],
   "source": [
    "for glst in GOLDEN_LSTs:\n",
    "    noise_comparison(glst, subsets, log=True, min_days=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure: Distribution of Excess Variance as function of Days Binned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In principle, the excess variance has a distribution that is dependent only on the number of days being binned and no other variable (eg. variance of any particular night). The following plot shows these distributions vs. the theoretical prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat = stats[GOLDEN_LSTs[5]]\n",
    "all_nds = np.concatenate([stat.days_binned[bl] for bl in stat.bls()])\n",
    "unique_nds = np.sort(np.unique(all_nds))\n",
    "unique_nds = unique_nds[unique_nds > 2]\n",
    "\n",
    "fig, ax = plt.subplots(len(unique_nds), 2, sharex=True, sharey=True,\n",
    "                       gridspec_kw={\"hspace\": 0.0, \"wspace\": 0}, figsize=(15, len(unique_nds)*2))\n",
    "\n",
    "x = np.linspace(0, 5, 200)\n",
    "for i, nd in enumerate(unique_nds):\n",
    "    for j, (name, select) in enumerate(subsets.items()):\n",
    "        bls = get_selected_bls(list(stat.bls()), stat.days_binned, selectors=select, min_days=0)\n",
    "\n",
    "        excess_low = np.concatenate([stat.n2n_excess_var[bl][(stat.days_binned[bl] == nd) & (golden_meta.freqs < 90e6)] for bl in bls])\n",
    "        excess_high = np.concatenate([stat.n2n_excess_var[bl][(stat.days_binned[bl] == nd) & (golden_meta.freqs > 110e6)] for bl in bls])\n",
    "\n",
    "        ax[i, 0].hist(excess_low, bins=np.linspace(0, 5, 101), histtype='step', density=True, color=f'C{j}', label=name)\n",
    "        ax[i, 1].hist(excess_high, bins=np.linspace(0, 5, 101), histtype='step', density=True, color=f'C{j}', label=name)\n",
    "        \n",
    "    ax[i, 0].plot(x, gamma(a=(nd-1)/2, scale=2/(nd-1)).pdf(x), color='k')\n",
    "    ax[i, 1].plot(x, gamma(a=(nd-1)/2, scale=2/(nd-1)).pdf(x), color='k')\n",
    "    \n",
    "    ax[i, 0].set_ylabel(f\"{int(nd)} days binned\")\n",
    "ax[0,0].legend(ncols=3)\n",
    "ax[0,0].set_title(\"Low Band (<90 MHz)\")\n",
    "ax[0,1].set_title(\"High Band (>110 MHz)\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_violin_plot(stat, bl_lists, coords, freq_masks=(slice(None),), min_days=7, xlabel=None, fig=None, ax=None, ylabel=True, topticks=False):\n",
    "    if fig is None:\n",
    "        fig, ax = plt.sublots(1, 1, figsize=(12,8))\n",
    "      \n",
    "    plt.sca(ax)\n",
    "    \n",
    "    # Expand freq_masks out to the length of bl_lists\n",
    "    if len(freq_masks) == 1 and len(bl_lists) > 1:\n",
    "        freq_masks = freq_masks * len(bl_lists)\n",
    "    elif len(bl_lists) == 1 and len(freq_masks) > 1:\n",
    "        bl_lists = bl_lists * len(freq_masks)\n",
    "    elif len(bl_lists) != len(freq_masks):\n",
    "        raise ValueError(\"bl_lists and freq_masks must be of the same length\")\n",
    "        \n",
    "    evs = []\n",
    "    dists = []\n",
    "    for bls, freq in zip(bl_lists, freq_masks):\n",
    "        if not bls:\n",
    "            evs.append(None)\n",
    "            dists.append(None)\n",
    "            continue\n",
    "        evs.append(np.concatenate([stat.n2n_excess_var[bl][stat.days_binned[bl] >= min_days][freq] for bl in bls]))\n",
    "        try:\n",
    "            dists.append(stat.n2n_excess_var_pred_dist(bls=bls, freq_inds=freq, min_n=min_days).rvs(size=5000))\n",
    "        except ValueError:\n",
    "            # If you get no samples, dist won't work here.\n",
    "            dists.append(None)\n",
    "            evs[-1] = None\n",
    "            \n",
    "    # Create labels for categorical-type coords based on the original number of\n",
    "    # coords, even if they end up not being shown because they have no baselines.\n",
    "    labels = None\n",
    "    if isinstance(coords[0], str):\n",
    "        labels = coords\n",
    "        coords = np.arange(len(coords))\n",
    "        plt.gca().set_xticks(np.arange(len(coords)), labels=labels)\n",
    "\n",
    "        \n",
    "    if topticks:\n",
    "        ax.xaxis.set_tick_params(labeltop=True)\n",
    "        \n",
    "    # remove all the coordinates that have no values at all\n",
    "    coords = [c for c, ev in zip(coords, evs) if ev is not None and not np.all(np.isnan(ev))]\n",
    "    dists = [d for d, ev in zip(dists, evs) if d is not None and not np.all(np.isnan(ev))]\n",
    "    evs = [ev for ev in evs if ev is not None and not np.all(np.isnan(ev))]\n",
    "    \n",
    "    if not evs:\n",
    "        return evs\n",
    "    \n",
    "    # Remove huge outliers from evs because otherwise the KDE struggles...\n",
    "    evs = [ev[ev < 10] for ev in evs]\n",
    "        \n",
    "    widths=[0.5 * (y-x) for x,y in zip(coords, coords[1:])] + [0.5 * (coords[-1] - coords[-2])]\n",
    "    parts = plt.violinplot(\n",
    "        dists, coords, showextrema=False, \n",
    "        widths=widths\n",
    "    )\n",
    "    plt.axhline(1, color='k', ls='--')\n",
    "    \n",
    "    for pc in parts['bodies']:\n",
    "        pc.set_facecolor('black')\n",
    "        pc.set_alpha(0.5)\n",
    "        \n",
    "    plt.violinplot(evs, coords, showextrema=False, showmeans=True, widths=widths)\n",
    "    \n",
    "        \n",
    "    plt.ylim(0, 5)\n",
    "    if ylabel:\n",
    "        plt.ylabel(\"Excess Variance\")\n",
    "    \n",
    "    if xlabel and labels is not None:\n",
    "        plt.xlabel(xlabel)\n",
    "        \n",
    "    return evs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_full_violin_plot(selectors, fig=None, ax=None, min_days=7, suptitle=None, ylabel=True):\n",
    "    if fig is None:\n",
    "        fig, ax = plt.subplots(len(stats), 1, sharex=True, sharey=True, figsize=(15, 2*len(stats)), constrained_layout=True)\n",
    "\n",
    "    for i, (glst, stat) in enumerate(stats.items()):\n",
    "        bls = [\n",
    "            get_selected_bls(list(stat.bls()), stat.days_binned, selectors=selector[0], min_days=min_days) for selector in selectors.values()\n",
    "        ]\n",
    "        freqmask = [selector[1] for selector in selectors.values()]\n",
    "        make_violin_plot(\n",
    "            stat, bls, \n",
    "            list(selectors.keys()), \n",
    "            fig=fig, ax=ax[i], \n",
    "            freq_masks=freqmask, \n",
    "            ylabel=False, \n",
    "            topticks=i==0\n",
    "        )\n",
    "        if ylabel:\n",
    "            ax[i].set_ylabel(f\"{glst*12/np.pi:.2f} hr\")\n",
    "\n",
    "    if suptitle:\n",
    "        fig.suptitle(suptitle)\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure: Distribution of Excess Variance Across Baseline Subsets and LSTs for Low- and High-Band"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following plot, the orange violins represent the observed distribution of excess variance (and the horizontal orange line is the mean of each), while the gray represents the theoretical distribution for that category. The data in each violin come from all baselines within the baseline subset, and all frequencies within the specified band (there is no averaging being done, we're just taking each Baseline/LST/freq as its own datum). Only data that has at least 7 contributing days in the LST-average are counted. Low/High band refer to below and above FM respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(len(stats), 2, sharex=True, sharey=True, figsize=(15, 2*len(stats)), constrained_layout=True)\n",
    "\n",
    "make_full_violin_plot(\n",
    "    selectors = {name.replace(\"baselines\", \"\"): (sel, slice(None, 850)) for name, sel in subsets.items()},\n",
    "    fig=fig, ax = ax[:, 0],\n",
    "    suptitle=\"Distribution of Excess Variance across Subsets, LSTs and Bands\"\n",
    ")\n",
    "make_full_violin_plot(\n",
    "    selectors = {name: (sel, slice(850, None)) for name, sel in subsets.items()},\n",
    "    fig=fig, ax = ax[:, 1], ylabel=False\n",
    ")\n",
    "\n",
    "ax[0,0].set_title(\"Low Band (< 90 MHz)\")\n",
    "ax[0,1].set_title(\"High Band (> 110 MHz)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure: Distribution of Excess Variance across LSTs and Bands for All Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following plot, the orange violins represent the observed distribution of excess variance (and the horizontal orange line is the mean of each), while the gray represents the theoretical distribution for that category. The data in each violin come from all baselines, and all frequencies within the specified band of 200 channels each (there is no averaging being done, we're just taking each Baseline/LST/freq as its own datum). Only data that has at least 7 contributing days in the LST-average are counted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = make_full_violin_plot(\n",
    "    selectors = {golden_meta.freqs[ind] / 1e6: (lambda bl: True, slice(ind-100, ind+100)) for ind in range(100, 1535, 200)},\n",
    "    suptitle=\"Distribution of Excess Variance across LSTs and Bands\"\n",
    ")\n",
    "ax[-1].set_xlabel(\"Freq [MHz]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure: Distribution of Excess Variance with Baseline Length and LST at 160 MHz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following plot, the orange violins represent the observed distribution of excess variance (and the horizontal orange line is the mean of each), while the gray represents the theoretical distribution for that category. The data in each violin come from baselines within a given range of lengths (each bin is 14.6 m wide), and all frequencies within a 200-channel frequency band centered around 160 MHz. The choice of frequency range is intended to capture the best quality data in the spectrum. No averaging is done, we're just taking each Baseline/LST/freq as its own datum. Only data that has at least 7 contributing days in the LST-average are counted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bllen_grid = [(start, start + 14.6) for start in np.arange(7.0, 180.0, 14.6)]\n",
    "\n",
    "fig, ax = make_full_violin_plot(\n",
    "    selectors = {(edge[0] + edge[1])/2: (lambda bl, edge=edge: edge[0] <= getbllen(bl[0], bl[1]) < edge[1], slice(800, 1000)) for edge in bllen_grid},\n",
    "    suptitle=\"Distribution of Excess Variance across Baseline Lengths at ~160 MHz\"\n",
    ")\n",
    "ax[-1].set_xlabel(\"Baseline Length [m]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure: Distribution of Excess Variance Between NS and EW baselines and pols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following plot, the orange violins represent the observed distribution of excess variance (and the horizontal orange line is the mean of each), while the gray represents the theoretical distribution for that category. \n",
    "The data in each violin come from baselines that are North-South or East-West oriented (within 6 degrees), and further subdivided by their polarization. Data in each category is taken from all frequencies within a 200-channel frequency band centered around 160 MHz. The choice of frequency range is intended to capture the best quality data in the spectrum. No averaging is done, we're just taking each Baseline/LST/freq as its own datum. Only data that has at least 7 contributing days in the LST-average are counted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bllen_grid = [(start, start + 14.6) for start in np.arange(7.0, 180.0, 14.6)]\n",
    "\n",
    "fig, ax = make_full_violin_plot(\n",
    "    selectors = {\n",
    "        \"EW (ee)\": (lambda bl: np.abs(bl[1]/bl[0]) < 1/10. and bl[2]=='ee', slice(800,1000)),\n",
    "        \"EW (nn)\": (lambda bl: np.abs(bl[1]/bl[0]) < 1/10. and bl[2]=='nn', slice(800,1000)),\n",
    "        \"NS (ee)\": (lambda bl: np.abs(bl[0]/bl[1]) < 1/10. and bl[2]=='ee', slice(800,1000)),\n",
    "        \"NS (nn)\": (lambda bl: np.abs(bl[0]/bl[1]) < 1/10. and bl[2]=='nn', slice(800,1000)),\n",
    "    },\n",
    "    suptitle=\"Distribution of Excess Variance for EW vs NS Baselines at ~160 MHz\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure: Distribution of Excess Variance Across Redundant Group Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following plot, the orange violins represent the observed distribution of excess variance (and the horizontal orange line is the mean of each), while the gray represents the theoretical distribution for that category. \n",
    "The data in each violin come from baselines that have redundant groups within the specified size range. Data in each category is taken from all frequencies within a 200-channel frequency band centered around 160 MHz. The choice of frequency range is intended to capture the best quality data in the spectrum. No averaging is done, we're just taking each Baseline/LST/freq as its own datum. Only data that has at least 7 contributing days in the LST-average are counted. **Note:** redgroup size is highly correlated with baseline length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_size_bins = [(1, 10), (10, 20), (20, 40), (40, 80), (80, 200), (200, 500), (500, 1000), (1000, np.inf)]\n",
    "\n",
    "fig, ax = make_full_violin_plot(\n",
    "    selectors = {\n",
    "        f\"{g[0]}-{g[1]}\": (lambda bl, g=g: (g[0] <= len(reds[bl]) < g[1]), slice(800, 1000)) for g in group_size_bins\n",
    "    },\n",
    "    suptitle=\"Distribution of Excess Variance across Redundant Group Size\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Visibilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we plot some raw (calibrated) data in comparison to the LST-binned data, using the \"Golden Data\" output by the LST-binner. We focus on the baselines with the highest excess variance, so that we can more easily identify issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mad(x, axis=0):\n",
    "    med = np.nanmedian(x, axis=axis)\n",
    "    return np.nanmedian(np.abs(x - med), axis=axis)*1.4826"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bl_coords(bl):\n",
    "    sep_unit = np.abs(golden_meta.antpos[1][0] - golden_meta.antpos[0][0])\n",
    "    return (np.abs(golden_meta.antpos[bl[0]] - golden_meta.antpos[bl[1]]) / sep_unit)[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_visibilities_per_type(\n",
    "    types, \n",
    "    glst,\n",
    "    freq_range=None, \n",
    "    label=None, yrange=None,\n",
    "    alpha=0.5,\n",
    "):\n",
    "    all_figs = []\n",
    "    \n",
    "    lstbin_hd = lstbin_hds[glst]\n",
    "    lststyle = dict(color='k', lw=3, alpha=0.2, zorder=10000)\n",
    "    excess = stats[glst].n2n_excess_var\n",
    "    \n",
    "    for bltype in types:\n",
    "        fig, ax = plt.subplots(\n",
    "            4, 2, \n",
    "            sharex=True, figsize=(15, 8), \n",
    "            constrained_layout=True, gridspec_kw={'height_ratios': (2,1,2,1)}\n",
    "        )\n",
    "        \n",
    "        nights_in = set()\n",
    "        if freq_range is not None:\n",
    "            mask = (lstbin_hd.freqs >= freq_range[0]) & (lstbin_hd.freqs < freq_range[1])\n",
    "            freqs=lstbin_hd.freqs[mask]/1e6\n",
    "        else:\n",
    "            mask = slice(None, None, None)\n",
    "            freqs = lstbin_hd.freqs/1e6\n",
    "            \n",
    "        \n",
    "        bls = [bl for bl in reds[bltype] if bl in golden_data[glst].bls()]\n",
    "        \n",
    "        handles = []\n",
    "        for jdint in data_jd_ints:\n",
    "            handles.append(mpl.lines.Line2D([0], [0], label=str(jdint), alpha=alpha, **styles[jdint]))\n",
    "        \n",
    "        \n",
    "        for j, bl in enumerate(bls):\n",
    "            flgs = golden_flags[glst][bl][:, mask]\n",
    "            datas = golden_data[glst][bl][:, mask]\n",
    "            \n",
    "            mag = np.where(flgs, np.nan, np.abs(datas))\n",
    "            phs = np.where(flgs, np.nan, np.angle(datas))\n",
    "            rl = np.where(flgs, np.nan, datas.real)\n",
    "            im = np.where(flgs, np.nan, datas.imag)\n",
    "\n",
    "            lstflg = lstbin_flags[glst][bl][0, mask]\n",
    "            lstdata = lstbin_data[glst][bl][0, mask]\n",
    "\n",
    "            rlmad = mad(rl)\n",
    "            immad = mad(im)\n",
    "\n",
    "            maglstbin = np.where(lstflg, np.nan, np.abs(lstdata))\n",
    "            phslstbin = np.where(lstflg, np.nan, np.angle(lstdata))\n",
    "            rllstbin = np.where(lstflg, np.nan, lstdata.real)\n",
    "            imlstbin = np.where(lstflg, np.nan, lstdata.imag)\n",
    "            \n",
    "            for night in range(len(golden_data[glst].times)):\n",
    "                jdint = int(golden_data[glst].times[night])\n",
    "                style = copy.deepcopy(styles[jdint])\n",
    "                style['alpha'] = alpha\n",
    "\n",
    "                if np.all(flgs[night]):\n",
    "                    continue\n",
    "                    \n",
    "                # Amplitude and Phase\n",
    "                ax[0, 0].plot(freqs, mag[night], **style)\n",
    "                ax[0, 0].plot(freqs, maglstbin, **lststyle)                \n",
    "                ax[1, 0].plot(freqs, mag[night] - maglstbin, **style)\n",
    "                \n",
    "                ax[2, 0].plot(freqs, phs[night], **style)\n",
    "                ax[2, 0].plot(freqs, phslstbin, **lststyle)\n",
    "                phsdiff = phs[night] - phslstbin\n",
    "                phsdiff[phsdiff < -np.pi] += 2*np.pi\n",
    "                phsdiff[phsdiff > np.pi] -= 2*np.pi\n",
    "                ax[3, 0].plot(freqs, phsdiff, **style)\n",
    "                        \n",
    "                # Real / Imag\n",
    "                ax[0, 1].plot(freqs, rl[night], **style)\n",
    "                ax[0, 1].plot(freqs, rllstbin, **lststyle)                \n",
    "                ax[1, 1].plot(freqs, (rl[night] - rllstbin)/rlmad, **style)\n",
    "                \n",
    "                ax[2, 1].plot(freqs, im[night], **style)\n",
    "                ax[2, 1].plot(freqs, imlstbin, **lststyle)\n",
    "                ax[3, 1].plot(freqs, (im[night] - imlstbin)/immad, **style)\n",
    "                \n",
    "                if yrange:\n",
    "                    ax[0, 0].set_ylim(yrange)\n",
    "                    \n",
    "            ax[1,1].axhline(4, color='gray', ls='--')\n",
    "            ax[1,1].axhline(-4, color='gray', ls='--')\n",
    "            \n",
    "            ax[3,1].axhline(4, color='gray', ls='--')\n",
    "            ax[3,1].axhline(-4, color='gray', ls='--')\n",
    "            \n",
    "        bl_coords = get_bl_coords(bltype)\n",
    "        \n",
    "        fig.suptitle(\n",
    "            f\"Baseline Type: {bltype} [{bl_coords[0]:.1f}-EW, {bl_coords[1]:.1f}-NS]. \"\n",
    "            f\"LST = {glst*12/np.pi:5.3} hr. Median Excess Var = {np.nanmedian(excess[bltype][mask]):.2f}\"\n",
    "        )\n",
    "        ax[-1, 0].set_xlabel(\"Frequency [MHz]\")\n",
    "        ax[-1, 1].set_xlabel(\"Frequency [MHz]\")\n",
    "        \n",
    "        ax[0, 0].set_ylabel(\"Magnitude\")\n",
    "        ax[0, 1].set_ylabel(\"Real Part\")\n",
    "        \n",
    "        ax[1, 0].set_ylabel(\"Magnitude Diff\")\n",
    "        ax[1, 1].set_ylabel(\"Real Z-score\")\n",
    "        ax[1, 1].set_ylim(-7, 7)\n",
    "        \n",
    "        ax[2, 0].set_ylabel(\"Phase\")\n",
    "        ax[2, 1].set_ylabel(\"Imag Part\")\n",
    "        \n",
    "        ax[3, 0].set_ylabel(\"Phase Diff\")\n",
    "        ax[3, 1].set_ylabel(\"Imag Z-score\")\n",
    "        ax[3, 1].set_ylim(-7, 7)\n",
    "        ax[0, 0].legend(handles=handles, ncols=5)\n",
    "\n",
    "        all_figs.append(fig)\n",
    "        \n",
    "    return all_figs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure: Visibilities Over Nights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sorted_keys(stat, min_days=7):\n",
    "    excess = []\n",
    "    mask = (golden_meta.freqs>125e6) & (golden_meta.freqs<=230e6)\n",
    "    bls = []\n",
    "    for bl in stat.bls():\n",
    "        if np.mean(stat.days_binned[bl][mask]) < min_days:\n",
    "            continue\n",
    "            \n",
    "        median = np.nanmedian(stat.n2n_excess_var[bl][mask])\n",
    "        if not np.isnan(median):\n",
    "            excess.append(median)\n",
    "            bls.append(bl)\n",
    "    srt = [k for k, v in sorted(zip(bls, excess), key=lambda item: item[1])]\n",
    "    return srt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for glst in GOLDEN_LSTs:\n",
    "    # Sort keys from best to worst\n",
    "    keys = get_sorted_keys(stats[glst])\n",
    "    if keys:\n",
    "        figs = plot_visibilities_per_type(\n",
    "            keys[-1:-5:-1] + keys[:1], glst, freq_range=(125e6, 230e6), alpha=0.75,\n",
    "        );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of Predicted Z-Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_baseline_zscores(freq_index, golden_data, golden_nsamples, golden_flags, lstbin_data):\n",
    "    zscores_pred_real = {}\n",
    "    zscores_pred_imag = {}\n",
    "    \n",
    "\n",
    "    for i, jd in enumerate(golden_data.times):\n",
    "        zscores_pred_real[int(jd)] = []\n",
    "        zscores_pred_imag[int(jd)] = []\n",
    "        \n",
    "        for j, (bl, val) in enumerate(golden_data.items()):\n",
    "            if bl[0] == bl[1] or bl[2][0] != bl[2][1]:\n",
    "                # skip autos\n",
    "                continue\n",
    "                \n",
    "            if golden_flags[bl][i, freq_index]:\n",
    "                continue\n",
    "\n",
    "            pred_var = noise.predict_noise_variance_from_autos(\n",
    "                bl, \n",
    "                golden_data, \n",
    "                dt=dt, \n",
    "                df=df, \n",
    "                nsamples=golden_nsamples\n",
    "            )[i, freq_index]\n",
    "\n",
    "            devreal = (val[i, freq_index].real - lstbin_data[bl][0, freq_index].real)\n",
    "            devimag = (val[i, freq_index].imag - lstbin_data[bl][0, freq_index].imag)\n",
    "            zscores_pred_real[int(jd)].append(devreal*np.sqrt(2)/np.sqrt(pred_var))\n",
    "            zscores_pred_imag[int(jd)].append(devimag*np.sqrt(2)/np.sqrt(pred_var))\n",
    "            \n",
    "    return zscores_pred_real, zscores_pred_imag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_zscore_histogram(freq_index, glst, fig=None, ax=None, xlabel=True, legend=True):\n",
    "    if fig is None:\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(12, 6), sharex=True, sharey=True, constrained_layout=True)\n",
    "    \n",
    "    real, imag = get_baseline_zscores(freq_index, golden_data[glst], golden_nsamples[glst], golden_flags[glst], lstbin_data[glst])\n",
    "\n",
    "    bins=np.linspace(-10, 10, 101)\n",
    "    \n",
    "    \n",
    "    for i, (jd, z) in enumerate(real.items()):\n",
    "        ax[0].hist(z, bins=bins, label=str(int(jd)), histtype='step', density=True, **styles[int(jd)])\n",
    "    for i, (jd, z) in enumerate(imag.items()):\n",
    "        ax[1].hist(z, bins=bins, label=str(int(jd)), histtype='step', density=True, **styles[int(jd)])\n",
    "        \n",
    "    if xlabel:\n",
    "        ax[0].set_xlabel(f\"Real Z-Score at {golden_meta.freqs[freq_index]/1e6:.1f} MHz\")\n",
    "        ax[1].set_xlabel(f\"Imag Z-Score at {golden_meta.freqs[freq_index]/1e6:.1f} MHz\")\n",
    "    \n",
    "    x = np.linspace(-10, 10, 1000)\n",
    "    y = np.exp(-(x**2)/2) / np.sqrt(2*np.pi)\n",
    "    ax[0].plot(x, y, color='k', lw=3)\n",
    "    ax[1].plot(x, y, color='k', lw=3)\n",
    "    \n",
    "    if legend:\n",
    "        ax[0].legend(ncol=3, title='Night (JD)');\n",
    "    ax[0].set_yscale('log')\n",
    "    ax[1].set_yscale('log')\n",
    "    ax[0].set_ylim(8e-4, 1)\n",
    "    ax[1].set_ylim(8e-4, 1)\n",
    "    \n",
    "def plot_all_zscore_histograms(freq_index):\n",
    "    fig, ax = plt.subplots(len(GOLDEN_LSTs), 2, figsize=(15, 2*len(GOLDEN_LSTs)), sharex=True, sharey=True, constrained_layout=True)\n",
    "    fig.suptitle(f\"Z-Scores (from predicted variance) across baselines per-night at {golden_meta.freqs[freq_index]/1e6:.1f} MHz \")\n",
    "    \n",
    "    ax[0,0].set_title(\"Real Part\")\n",
    "    ax[0,1].set_title(\"Imag Part\")\n",
    "    \n",
    "    handles = {}\n",
    "    for i, glst in enumerate(GOLDEN_LSTs):\n",
    "        plot_zscore_histogram(freq_index, glst, fig=fig, ax=ax[i], xlabel=i==len(GOLDEN_LSTs)-1, legend=False)\n",
    "        \n",
    "        # Keep track of legend stuff.\n",
    "        h, l = ax[i, 0].get_legend_handles_labels()\n",
    "        handles.update(dict(zip(l, h)))\n",
    "                \n",
    "        ax[i, 0].text(0.8, 0.8, f\"LST {glst*12/np.pi:5.3f} hr\", transform=ax[i,0].transAxes, fontweight='bold')\n",
    "       \n",
    "    handles = [h for l, h in sorted(handles.items())]\n",
    "    fig.legend(loc='upper left', handles=handles, ncols=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure: Histogram of Baseline Z-Scores at single Frequency / LST / Night"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot is showing the the distribution of Z-scores (with respect to the _predicted_ variance) over baselines for each night. Now, remember that the value for a *particular baseline* over nights is by definition mean-zero here (since the z-score for a baseline is defined as the visibility minus the mean over nights for that vis, divided by the std over nights), so the full distribution of everything in this plot should be mean zero, as we can see it is. However, any particular night is free to have a non-zero mean -- all baselines could have been \"bad\" on that night together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_zscore_histograms(750)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_box_plot(freq_index, glst):\n",
    "    real, imag = get_baseline_zscores(\n",
    "        freq_index, golden_data[glst], golden_nsamples[glst], golden_flags[glst], lstbin_data[glst]\n",
    "    )\n",
    "    \n",
    "    # Only use real here.\n",
    "    zscores = list(real.values())\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.boxplot(\n",
    "        [np.array(zscore)[~np.isnan(zscore)] for zscore in zscores], \n",
    "        positions=[jd - 2459800 for jd in real]\n",
    "    );\n",
    "    mean_var = np.array([np.nanmean(np.array(zscore)**2) for zscore in zscores])\n",
    "    plt.scatter([jd -2459800 for jd in real], mean_var, marker='*', s=75, color='r', zorder=5, \n",
    "                label=r'$\\langle z^2 \\rangle \\approx$  Excess Variance ')\n",
    "    plt.ylim(-12, 12)\n",
    "    plt.xlabel(\"Night (JD)\")\n",
    "    plt.ylabel(\"Predicted Z-Score\")\n",
    "    \n",
    "    # Put on lines where box-plot markers should be if it were Gaussian\n",
    "    plt.axhline(0.6754, ls='--', color='gray')\n",
    "    plt.axhline(-0.6754, ls='--', color='gray')\n",
    "    plt.axhline(2.698, ls='--', color='gray', alpha=0.5)\n",
    "    plt.axhline(-2.698, ls='--', color='gray', alpha=0.5)\n",
    "    plt.axhline(1, ls='-', color='r', lw=1)\n",
    "    \n",
    "    plt.title(f\"Z-Scores Per-Night Across Baselines at {golden_meta.freqs[freq_index]/1e6:.1f} MHz and LST {glst*12/np.pi:5.3f} hr\")\n",
    "    plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure: Box-Plot of Z-Scores at 138 MHz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following plot shows the distribution of Z-scores at 138 MHz, grouped by night, to highlight which nights (if any) are behaving poorly at each LST. Gray lines show the theoretical expectation for the box and whiskers respectively of the box plots. The red line is at unity and red stars indicate the estimate of the contribution to excess variance from that night (determined by the average of the squared z-score over baselines for that night). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for glst in GOLDEN_LSTs:\n",
    "    make_box_plot(750, glst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure: Box-Plot of Z-Scores at 169 MHz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following plot shows the distribution of Z-scores at 169 MHz (nominally a well-behaved frequency), grouped by night, to highlight which nights (if any) are behaving poorly at each LST. Gray lines show the theoretical expectation for the box and whiskers respectively of the box plots. The red line is at unity and red stars indicate the estimate of the contribution to excess variance from that night (determined by the average of the squared z-score over baselines for that night). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for glst in GOLDEN_LSTs:\n",
    "    make_box_plot(1000, glst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration of Sigma-Clipping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we attempt to understand the impact of sigma-clipping on the data. We form robust Z-scores from the GOLDEN data using median absolute deviation, just as in the pipeline code itself, then threshold at different thresholds to inform us of the impact of sigma-clipping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_observed_zscores(glst, min_N: int=4):\n",
    "    gd = golden_data[glst]\n",
    "    gf = golden_flags[glst]\n",
    "    \n",
    "    # We do this in a baseline-loop, which may be slower than it could be\n",
    "    zscores = {}\n",
    "    flags = {}\n",
    "    for bl in gf.bls():\n",
    "        # Ignore autos\n",
    "        if bl[0] == bl[1]:\n",
    "            continue\n",
    "            \n",
    "        flg = gf[bl].copy()\n",
    "        \n",
    "        this = np.zeros(flg.shape, dtype=complex)\n",
    "        for part in ['real', 'imag']:\n",
    "        \n",
    "            d = getattr(gd[bl], part).copy()\n",
    "\n",
    "            flg[np.isnan(d) | np.isinf(d)] = True\n",
    "\n",
    "            d[flg] *= np.nan\n",
    "            location = np.nanmedian(d, axis=0)\n",
    "            mad = np.nanmedian(np.abs(d - location), axis=0) * 1.482579\n",
    "\n",
    "            if part == \"real\":\n",
    "                this += (d - location)/mad\n",
    "            else:\n",
    "                this += 1j * (d - location)/mad\n",
    "            \n",
    "        # Apply min_N criterion\n",
    "        # the point of \"flagging\" here is just to be able to exclude \n",
    "        # these values from showing up in the computed fractions\n",
    "        # that are flagged specifically because of their Z-score.\n",
    "        ndays_binned = np.sum(~flg, axis=0)\n",
    "        flg[:, ndays_binned < min_N] = True\n",
    "\n",
    "        zscores[bl] = this\n",
    "        flags[bl] = flg\n",
    "\n",
    "    if RED_DATA:\n",
    "        return RedDataContainer(zscores, reds=reds), RedDataContainer(flags, reds=reds)\n",
    "    else:\n",
    "        return DataContainer(zscores), DataContainer(flags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zscores_obs = {glst: get_observed_zscores(glst) for glst in GOLDEN_LSTs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma_clip_fraction_plot(zscores_obs):\n",
    "    nglst = len(zscores_obs)\n",
    "    ncols = 3\n",
    "    nrows = nglst // ncols + 1\n",
    "    fig, axx = plt.subplots(nrows, 3, sharey=True, sharex=True, constrained_layout=True, figsize=(15, 2*nrows))\n",
    "\n",
    "    nsig = np.linspace(2, 9, 20)\n",
    "    cdf = 2* norm().cdf(-nsig)\n",
    "\n",
    "    ax = axx.flatten()\n",
    "\n",
    "    # get legend entries\n",
    "    for jdint, style in styles.items():\n",
    "        ax[0].plot([0], [np.nan], label=str(jdint), **style)\n",
    "            \n",
    "    for j, (glst, (zscores, flags)) in enumerate(zscores_obs.items()):\n",
    "        ax[j].plot(nsig, cdf, color='k', label='Gaussian Theory' if j==0 else None)\n",
    "        \n",
    "        ax[j].text(0.5, 0.85, f\"LST {glst*12/np.pi:.2f}\", fontsize=14, transform=ax[j].transAxes)\n",
    "\n",
    "        for i, jd in enumerate(golden_data[glst].times):\n",
    "            for part in ['real', 'imag']:\n",
    "                allz = np.concatenate([getattr(zscores[bl][i][~flags[bl][i]], part) for bl in zscores.bls()])\n",
    "\n",
    "                frac_cut = np.array([np.sum(np.abs(allz) >= sig) for sig in nsig]) / allz.size\n",
    "\n",
    "                intjd = int(jd)\n",
    "                ax[j].plot(nsig, frac_cut, alpha=0.5 if part=='imag' else 1.0, **styles[intjd])\n",
    "                #ax[j].set_yscale('log')\n",
    "    for axxx in ax[j+1:]:\n",
    "        axxx.axis('off')\n",
    "        \n",
    "    fig.supxlabel(\"sigma clip threshold\")\n",
    "\n",
    "    fig.legend(loc='center', ncols=3, bbox_to_anchor=(0.85, 0.18), frameon=False)\n",
    "    \n",
    "    fig.supylabel(\"Fraction of Samples Clipped\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure: Sigma-Clipped Fraction As Function of Threshold, LST and Night"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the underlying data is Gaussian, the fraction sigma-clipped can be predicted by the CDF of the Gaussian function as a function of threshold. In reality, we expect the data to have more outliers than an actual Gaussian. In the plot below, we show the fraction of samples (across baselines and frequencies) that are flagged specifically due to sigma-clipping, as a function of the sigma-clipping threshold. We split the plots between different LST bins and different nights. The real part is shown as the full-colour lines, while the imaginary part is shown as the 50% transparent lines of the same style. The black line is the theoretical expectation, given an underlying Gaussian distribution across nights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_clip_fraction_plot(zscores_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_zscores = []\n",
    "all_preflags = []\n",
    "for j, (glst, (zscores, flags)) in enumerate(zscores_obs.items()):\n",
    "    for i, jd in enumerate(golden_data[glst].times):\n",
    "        for bl in zscores.bls():\n",
    "            all_zscores.append(zscores[bl][i])\n",
    "            all_preflags.append(flags[bl][i])\n",
    "        \n",
    "all_zscores = np.array(all_zscores).reshape((-1, len(golden_meta.freqs)))\n",
    "all_preflags = np.array(all_preflags).reshape((-1, len(golden_meta.freqs)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsig = np.linspace(2, 6, 5)\n",
    "cdf = 2* norm().cdf(-nsig)\n",
    "\n",
    "fracs_sigma_clip = []\n",
    "for sig in nsig:\n",
    "    nsigmaclip_flags = np.sum(((np.abs(all_zscores.real) > sig) | (np.abs(all_zscores.imag) > sig)) & (~all_preflags), axis=0)\n",
    "    pre_flagged = np.sum(all_preflags, axis=0)\n",
    "    \n",
    "    frac = nsigmaclip_flags/(len(all_zscores) - pre_flagged)\n",
    "    fracs_sigma_clip.append(frac)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure: Sigma-Clipped Fraction as Function of Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we show the sigma-clipping fraction (over baselines, LSTs and nights) as a function of frequency. The aim is to understand which frequencies are commonly flagged due to sigma-clipping, and also the rule-of-thumb estimate for the fraction flagged for a given threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sig, frac in zip(nsig,fracs_sigma_clip):\n",
    "    plt.plot(golden_meta.freqs / 1e6, frac, label=f\"{sig}\")\n",
    "plt.legend(title='thresholds', ncols=2, loc='upper center')\n",
    "\n",
    "plt.xlabel(\"Freq [MHz]\")\n",
    "plt.ylabel(\"Fraction of Samples Flagged by Sigma-Clipping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_true_chunk_sizes(x):\n",
    "    return np.diff(np.where(np.concatenate(([x[0]], x[:-1] != x[1:], [True])))[0])[::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = Counter()\n",
    "cnt_nopreflags = Counter()\n",
    "bad_ones = {}\n",
    "for glst, (zscores, flags) in zscores_obs.items():\n",
    "    for bl in zscores.bls():\n",
    "        for i, (z, flg) in enumerate(zip(zscores[bl], flags[bl])):\n",
    "            clip_flags = (np.abs(z.real) > 4) | (np.abs(z.imag) > 4)\n",
    "            cnt.update(get_true_chunk_sizes(clip_flags).tolist())\n",
    "            cnt_nopreflags.update(get_true_chunk_sizes(clip_flags & (~flg)).tolist())\n",
    "            \n",
    "            frac = np.sum(clip_flags & (~flg)) / clip_flags.size\n",
    "            if frac > 0.2:\n",
    "                bad_ones[(glst*12/np.pi, int(golden_data[glst].times[i]), bl)] = frac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of most sigma-clipped LSTs, Nights and Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One concern is that we might be flagging out large fractions of particular antennas with sigma-clipping. In this case, it would be ideal to identify the actual issue in a previous step, rather than arbitrarily sigma-clipping them at the LST-binning step. Here, we find the most sigma-clipped baselines for any night/LST, and print all those that are sigma-clipped more than 30% (at 4-sigma). This is purely _sigma-clipped_ flags, where pre-flagged data is not counted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (glst, jd, bl), frac in reversed(sorted(bad_ones.items(), key=lambda item: item[1])):\n",
    "    print(f\"LST {glst:5.2f} hr on night {jd} for bl {bl} had {frac*100:.1f}% sigma-clip flags\")\n",
    "    if frac < 0.3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure: Counts of Contiguous Flagged Region Sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might also worry about when large contiguous chunks of frequency are flagged for a particular antenna. Here we plot the number of contiguous regions of a given size that are flagged:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(cnt.keys(), cnt.values(), label='Sigma-clip Flags')\n",
    "plt.scatter(cnt_nopreflags.keys(), cnt_nopreflags.values(), label=\"Not counting pre-flagged\", marker='x')\n",
    "\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.xlabel(\"Number of contiguous flags (in frequency)\")\n",
    "plt.ylabel(\"Number of occurences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-16T19:37:29.396496Z",
     "start_time": "2021-02-16T19:37:28.585720Z"
    }
   },
   "outputs": [],
   "source": [
    "import hera_cal\n",
    "import pyuvdata\n",
    "print('hera_cal version: ', hera_cal.__version__)\n",
    "print('pyuvdata version: ', pyuvdata.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "h6c",
   "language": "python",
   "name": "h6c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "338.667px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
